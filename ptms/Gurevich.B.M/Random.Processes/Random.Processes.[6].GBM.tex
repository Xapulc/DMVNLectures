\documentclass[a4paper]{article}
\usepackage{dmvn}
\input rotate

\newbox\rotbox
\def\rotleft#1{\setbox\rotbox\hbox{#1}\@rotl\rotbox}
\newcommand{\indep}{\mathop{\lower.07em\hbox{\rotleft{$\vDash$}}}\nolimits}

\def\mcomp#1{\mskip-10mu#1\mskip-10mu}
\def\scomp#1{\mskip-20mu#1\mskip-20mu}
\def\compr#1#2{\mskip#1#2\mskip#1}

\def\z{,\;}

\newbox\mybigbox
\newbox\mysmallbox
\def\Size{\Huge}
\def\bigsyml#1{\setbox\mysmallbox\hbox{#1}\setbox\mybigbox\hbox{\Size#1}\raise\ht\mysmallbox\hbox{\lower\ht\mybigbox\copy\mybigbox}}

\def\bigsymh#1{\smash{\hbox{\Size#1}}}

\newcommand{\dx}{\,dx}
\newcommand{\dy}{\,dy}
\newcommand{\ds}{\,ds}
\newcommand{\du}{\,du}
\newcommand{\dt}{\,dt}
\newcommand{\dmu}{\,d\mu}
\newcommand{\dze}{\,d\ze}
\newcommand{\dla}{\,d\la}

\tocsubsubsectionparam{3.4em}

\begin{document}
\dmvntitle{Курс лекций по}{случайным процессам}{Лектор\т Борис Маркович Гуревич}
{III курс, 6 семестр, поток математиков}{Москва, 2005 г.} \pagebreak

\pagestyle{plain}
\tableofcontents
\pagebreak

\section*{Введение}

\subsection*{Предисловие}

Убедительная просьба ко~всем читателям: в~случае обнаружения ошибок
немедленно сообщайте авторам на \dmvnmail{} или загляните на \dmvnwebsite{} и~посмотрите, где можно
достать в~настоящее время самих авторов. Все пожелания и~предложения по поводу оформления
и~содержания документа будут обязательно приняты к~сведению. Последнее обновление: \сегодня~года.
В~данной версии уже есть всё, что было в~программе экзамена 2005 года.

\subsection*{Слова благодарности}

Хочется отметить добрым словом деятельность Иры Шитовой, Влада Короткова, Миши Малинина, Коли Рудого,
Вани Вегнера, Ани Черниловской, Паши Наливайко, Руслана Суюндыкова, Юры Кудряшова, Саши Воронцова
и~Пети Митричева за многочисленные замечания и~багоисправление. Порядок имён столь же случаен,
как и~сия загадочная наука, и~не указывает на чьё\д либо превосходство
в~данном неблагодарном деле. Всем спасибо!


\subsection*{Используемые в~тексте обозначения}

\begin{items}{-1}
\item Значком <<$\indep$>> будем обозначать независимость случайных величин. Не следует путать
с~ортогональностью $\bot$, которая всего лишь означает, что величины некоррелированные
(их ковариация равна нулю).
\item Через $\Law$ будем обозначать распределение\footnote{Обозначение заимствовано
из книги \cite{bsh}.} (грубо говоря, этот значок\т сокращение для слов <<закон распределения>>).
\item $x \wg y  := \min\hc{x,y}$.
\item Значком $\Cl$ обозначается замыкание (от слова \emph{closure}).
\end{items}


\begin{thebibliography}{4}
\setlength\itemsep{-.5mm}
\bibitem[БШ]{bsh} Булинский\,А.\,В., Ширяев\,А.\,Н. \emph{Теория случайных процессов.} М.: Физматлит, 2003.
\end{thebibliography}

\newpage

\pagestyle{headings}

\makeatletter
  \renewcommand{\headheight}{11mm}
  \renewcommand{\headsep}{2mm}
  \renewcommand{\sectionmark}[1]{}
  \renewcommand{\subsectionmark}[1]{}
  \renewcommand{\subsubsectionmark}[1]{\markright{\thesubsubsection. #1}}
  \renewcommand{\@oddhead}{\vbox{\hbox to \textwidth{\scriptsize\thepage\hfil\rightmark\strut}\hrule}}
  \renewcommand{\@oddfoot}{\hfil\thepage\hfil}
\makeatother

\section{Общая теория случайных процессов}

\subsection{Первая теорема Колмогорова}

\subsubsection{Определение случайного процесса}

\begin{df}
Пусть $T$\т произвольное множество. Пусть всякому $t \in T$ поставлена в~соответствие
некоторая случайная величина $\xi_t$ на некотором фиксированном вероятностном пространстве
$(\Om, \Fc, \Pf)$. Таким образом, можно считать, что задана функция
$\xi\cln T \times \Om \ra \R$ (или $\Cbb$), причём при всяком фиксированном $t \in T$ отображение $\xi(t, \cdot)$
является случайной величиной. В~этом случае $\xi$ называется \emph{случайной функцией}.
\end{df}

Желая подчеркнуть, каким именно множеством заиндексировано семейство $\xi$, мы будем
использовать запись $\xi_t, t \in T$ для обозначения случайной функции в~целом.

В~самом общем случае можно считать, что задано измеримое пространство $(X, \Ac)$, а~случайная функция
имеет вид $\xi\cln T \times \Om \ra X$ и~при всяком фиксированном $t$ является $(\Fc, \Ac)$\д измеримой функцией.
Определение, данное выше, соответствует $X = \R$ и~$\Ac = \Bs(\R)$\т борелевская $\si$\д алгебра на $\R$.

Часто имеет смысл рассматривать менее абстрактные случайные функции.

\begin{df}
Если $\xi$\т случайная функция, а~$T \subs \R$, то $\xi$ называется \emph{случайным процессом}.
В~этом случае на $T$ появляется естественное упорядочение, поэтому оно часто называется \emph{временем}.
\end{df}

Примеры процессов с~дискретным временем\т это процессы, у~которых $T = \N$ или $T = \Z$.
Примеры процессов с~непрерывным временем\т это случаи $T = \R$, $T = [0,\bes)$.

Другой важный класс случайных функций\т это так называемые \emph{случайные поля}, у~которых $T = \Z^k$.

\begin{df}
Пусть $\Sc$\т некоторое множество функций, называемых основными (как правило, $\Cb_0^\bes$).
Непрерывное линейное отображение, ставящее в~соответствие функции $f \in \Sc$ некоторую случайную
величину $\xi_f$,
%$$L\cln S \ra R, \quad L\cln f \mapsto \xi_f$$
называется \emph{обобщённой случайной функцией}. Линейность означает,
что $\xi_{\la f + \mu g} = \la \xi_f + \mu \xi_g$, а~непрерывность\т что если $f_n\ra f$ в~$\Sc$, то $\xi_{f_n} \ra \xi_f$. Сходимость может пониматься в
разных смыслах\т по вероятности, почти всюду, по распределению, \итд
\end{df}



\subsubsection{Конечномерные распределения случайной функции}

Как это принято в~теории вероятностей, функцию распределения величины $\xi$ будем обозначать $F_\xi$.

Пусть $\xi\cln T \times \Om \ra X$\т случайная функция, а~$(X, \Ac)$\т измеримое пространство.
Зафиксируем $t \in T$. Распределение случайной величины $\xi_t$ будем обозначать $\mu_t$.
Это некоторая мера на пространстве $X$: пусть $A \in \Ac$, тогда
\eqn{\mu_t(A) = \Pf(\xi_t \in A) = \Pf\hc{\om\cln \xi_t(\om) \in A}.}
В~случае, когда $A = (-\bes,a]$, получаем
$F_{\xi_t}(a) = \mu_t(A)$.

Аналогично построим \emph{конечномерные распределения}. Положим
\eqn{\mu_{t_1\sco t_n}(A_1\st A_n) = \Pf\hc{\xi_{t_1} \in A_1\sco \xi_{t_n} \in A_n}.}
Очевидно, что
\eqn{\label{eqn:concordance.first}\mu_{t_1\sco t_n}(A_1\st A_{n-1}\times X) = \mu_{t_1\sco t_{n-1}}(A_1\st A_{n-1}).}
Не менее очевидно, что для произвольной подстановки индексов $\si \in \Sb_n$ имеем
\eqn{\label{eqn:concordance.second}\mu_{t_1\sco t_n}(A_1\st A_n) = \mu_{t_{\si(1)}\sco t_{\si(n)}}(A_{\si(1)}\st A_{\si(n)}),}
поскольку непосредственно из определения следует, что это меры одного и~того же множества.

\begin{df}
Условия~\eqref{eqn:concordance.first} и~\eqref{eqn:concordance.second} называются \emph{условиями согласованности}
конечномерных распределений.
\end{df}

\subsubsection{Теорема Колмогорова о~согласованных конечномерных распределениях}

\begin{df}
Пусть $(X,\rho)$\т метрическое пространство. \emph{Борелевской} $\si$\д алгеброй $\Bs(X,\rho)$
на пространстве~$X$ называется $\si$\д алгебра, порождённая всеми открытыми
множествами (в~метрике $\rho$).
\end{df}


\begin{df}
Измеримое пространство $(X, \Ac)$ называется \emph{борелевским}, если существует полное сепарабельное
метрическое пространство $(Y,\rho,\Bs)$ и~$Y_0 \in \Bs$ такое, что
$(Y_0, \Bs\ev{Y_0}{7pt}) \cong (X,\Ac)$.
\end{df}

\begin{theorem}[Колмогорова]
Пусть имеется борелевское пространство $(X, \Ac)$ и~множество $T$. Пусть также для всякого набора
$t_1\sco t_n$ определена мера $\mu_{t_1\sco t_n}$ на $(X^n,\Ac^n)$, причём все эти меры согласованы.
Тогда существует случайная функция $\xi \cln T \times \Om \ra X$ на некотором
вероятностном пространстве $(\Om, \Fc, \Pf)$, конечномерные распределения которой совпадают с~заданным семейством мер $\hc{\mu}$, то есть для всякого набора $t_1\sco t_n$ имеем
\eqn{\mu_{t_1\sco t_n}(A_1\st A_n) = \Pf\hc{\xi_{t_1} \in A_1\sco \xi_{t_n} \in A_n}.}
\end{theorem}

Мы не будем излагать полностью доказательство теоремы Колмогорова, а~чуть ниже изложим
основную идею доказательства. Перед этим сделаем ряд замечаний.

Эта теорема показывает, что условия согласованности конечномерных распределений являются необходимыми
и~достаточными для существования случайной функции с~заданным набором конечномерных распределений
в~классе борелевских пространств. Вообще говоря, этот класс очень широк. Кроме того, если ограничиться
рассмотрением лишь вещественных (или комплексных) случайных величин, то пространство $(X, \Ac)$ автоматически
становится борелевским, ибо $X = \R^k$, а~$\Ac = \Bs(\R^k)$.

Заметим ещё, что для случая, когда $T$\т конечное множество, эта теорема была фактически доказана в~курсе
теории вероятностей. Пусть $|T| = n$. Положим
\eqn{\Om := X^n, \quad \Fc := \Ac^n,\quad \Pf := \mu_{t_1\sco t_n},
\quad \xi_{t_i}(\om) = \xi_{t_i}(x_1\sco x_n) := x_i.}
Фактически, $i$\д я случайная величина есть $i$\д я координатная проекция. Очевидно, что это
действительно случайная величина, поскольку прообразы множеств при координатных проекциях\т
это цилиндры с~одномерными образующими, а~такие множества, конечно,
принадлежат нашей $\si$\д алгебре.

\begin{problem}
Показать, что $\mu_{t_{i_1}\sco t_{i_k}} = \Law(\xi_{t_{i_1}}\sco \xi_{t_{i_k}})$.
\end{problem}

Напомним, что через $X^T$ обозначается множество всех отображений $f\cln T \ra X$.

\begin{problem}
Показать, что если $|T| = n$, то $X^T \cong X^n$.
\end{problem}

\begin{df}
\emph{Цилиндр}\т это подмножество в~$X^T$ вида
\eqn{C^{A_1\sco A_k}_{t_1\sco t_k} := \hc{f\cln f(t_1) \in A_1\sco f(t_k) \in A_k}.}
Множества $A_1\sco A_k$ называются \emph{образующими} цилиндра.
\end{df}

Наметим теперь доказательство теоремы Колмогорова.

\begin{proof}
Основная идея уже была изложена, когда мы рассмотрели случай конечного~$T$.
Итак, в~общем случае положим $\Om := X^T$. На цилиндрических множествах вероятность определяется
так:
\eqn{\Pf(C) := \mu_{t_1\sco t_k}(A_1\st A_k).}
Если доказать, что на алгебре
объединений цилиндров полученная мера счётно\д аддитивна, то можно применить теорему о~продолжении
меры, и~всё будет доказано\т искомая вероятностная мера будет распространена на $\si$\д алгебру, порождённую
всеми цилиндрами. В~доказательстве этого факта и~содержится основная трудность, и~мы не будем этого делать.

После этого всё совсем просто: положим $\xi_t(f) := f(t)$. Мы здесь используем нестандартное обозначение
для события, чтобы подчеркнуть, что в~нашем случае события\т это функции.
\end{proof}

С~помощью теоремы Колмогорова можно легко доказывать существование независимых одинаково распределённых
случайных величин. Пусть $\mu_t$\т вероятностная мера на $(X, \Ac)$. Пусть $\Law \xi_t = \mu_t$.
Применим теорему Колмогорова и~заметим, что для пространства $(X^n, \Ac^n) $ имеем
$\mu_{t_1\sco t_n} = \mu_{t_1}\sot \mu_{t_n}$. В~самом деле,
$\mu_{t_1\sco t_n}(A_1\st A_n) = \prodl{i=1}{n} \mu_{t_i}(A_i)$,
поэтому по определению величины $\xi_{t_1}\sco \xi_{t_n}$ будут независимыми.

\subsection{Ковариационные функции. Гауссовские процессы}

\subsubsection{Ковариационная функция случайного процесса}

Рассмотрим самый простой пример нетривиального случайного процесса. Пусть $\xi$\т случайная величина,
а~$f\cln \R^+ \ra \R$\т произвольная функция. Рассмотрим процесс $\xi_t(\om) := \xi(\om)f(t)$.
Очевидно, что он удовлетворяет всем аксиомам случайной функции. Однако этот пример неинтересен тем,
что если $f(0) \neq 0$, то, зная $\xi_0$ и~$f$, мы сможем определить его в~любой момент времени.

Мы чаще всего будем рассматривать вещественные или комплексные случайные процессы, поэтому
в~дальнейшем, если не оговорено противное, они будут подразумеваться таковыми.

\begin{df}
Пусть $\xi_t$\т случайный процесс. Положим $m(t) := \Mf \xi_t$. Процесс называется \emph{центрированным}, если $m(t)\equiv 0$.
\end{df}

\begin{df}
\emph{Ковариационной функцией} процесса $\xi_t$ называется величина
\eqn{K_\xi(s,t) := \cov(\xi_s,\xi_t) = \Mf(\xi_s - \Mf \xi_s)(\ol{\xi_t - \Mf \xi_t}).}
\end{df}

\begin{theorem}[Свойства ковариационной функции]
\begin{points}{-2}
\item Если $\eta(t,\om) = \xi(t,\om) + f(t)$, то $K_\eta = K_\xi$.
\item Антисимметричность: $K_\xi(s,t) = \ol{K_\xi(t,s)}$.
\item $K_\xi(t,t) = \Df\xi_t$.
\item Неотрицательная определённость: для $\fa t_1\sco t_n \in T$ и~$\fa z_1\sco z_n \in \Cbb$
\eqn{\sums{i,j} z_i K_\xi(t_i, t_j) \ol z_j \ge 0.}
\end{points}
\end{theorem}
\begin{proof}
Первые три свойства вытекают непосредственно из определения. Докажем последнее свойство.
В~силу первого свойства можно ограничиться рассмотрением случайных процессов,
у~которых $\Mf \xi_t \equiv 0$. А~для таких случайных процессов имеем
\eqn{\sums{i,j}z_i K_\xi(t_i,t_j) \ol z_j  = \sums{i,j}z_i \Mf\br{\xi_{t_i}\ol \xi_{t_j}}\ol z_j =
\Mf\hs{\sum z_i \xi_{t_i} \cdot \ol{\sum z_i \xi_{t_i}}} = \Mf \Bm{\sum z_i \xi_{t_i}}^2 \ge 0,}
что и~требуется.
\end{proof}

\subsubsection{Гауссовские случайные процессы}

\begin{df}
Случайная функция $\xi_t$ называется \emph{гауссовской}, если все её конечномерные распределения гауссовские,
то есть для $\fa t_1\sco t_n$ вектор $(\xi_{t_1}\sco \xi_{t_n})$ имеет многомерное нормальное распределение.
\end{df}

Из курса теории вероятностей (см. например,~\cite[с.~51]{bsh}) известно следующее
\begin{stm}[Эквивалентные определения гауссовского вектора]
Пусть $\vec \xi = (\xi_1\sco \xi_n)$\т случайный вектор. Он является гауссовским, если выполнено любое из
трёх эквивалентных условий:
\begin{points}{-2}
\item Характеристическая функция $\vec \xi$ имеет вид
\eqn{\ph_\xi(\vec t) = \exp\hs{i(\vec a,\vec t)-\frac12(C\vec t,\vec t)},}
где $C$\т некоторая симметрическая неотрицательно определённая матрица (матрица ковариаций) и~$\vec a$\т
некоторый фиксированный вектор (среднее значение).
\item Для любых $c_1\sco c_n\in \R$ величина $\sum с_i \xi_i$ имеет нормальное распределение.
\item Вектор $\vec \xi$ получается из стандартного гауссовского вектора с~независимыми компонентами путём
линейного преобразования, то есть $\vec \xi = A\xi^0$, где $A$\т произвольная матрица, а~$\xi^0= (\xi^0_1\sco \xi^0_n)$,
и~$\xi_i^0\sim \Nc(0,1)$ и~независимы в~совокупности.
\end{points}
\end{stm}

Для краткости ковариационную матрицу вектора $\vec \xi$, то есть матрицу $\br{c_{ij}} = \cov(\xi_i,\xi_j)$,
будем обозначать через $\cov(\xi)$.

\begin{theorem}
Для любых вещественнозначных функций $m(t)$ и~$K(s,t)$, где $K(s,t)$ удовлетворяет всем свойствам
ковариационной функции, существует гауссовская случайная
функция $\xi_t$, у~которой $\Mf\xi_t =m(t)$ и~$K_\xi(s,t) = K(s,t)$.
\end{theorem}
\begin{proof}
Снова нам достаточно рассмотреть случай $m(t)\equiv 0$. Фиксируем произвольные $t_1\sco t_n\in T$ и~рассмотрим матрицу
$C := (c_{ij})$, где $c_{ij} := K(t_i,t_j)$. В~силу свойств функции $K(s,t)$, эта матрица симметрична и~неотрицательно
определена. Значит, она диагонализуема с~помощью ортогональной замены координат, то есть найдётся оператор
$\Oc \in \Ob_n(\R)$, для которого $C = \Oc^{-1}D\Oc$, где $D =\diag(d_1\sco d_n)$ и~все $d_i \ge 0$.
Извлечём корень из матрицы~$C$. Положим $A := \sqrt C := \Oc^{-1}\sqrt D \Oc$. Проверим,
что квадрат матрицы $A$ действительно равен $C$:
\eqn{\br{\sqrt C}^2 = (\Oc^{-1}\sqrt D \Oc)(\Oc^{-1}\sqrt D \Oc) = \Oc^{-1}\sqrt D \sqrt D \Oc = \Oc^{-1}D\Oc= C.}

Положим $\eta := \sqrt C \xi^0$, где $\xi^0$\т стандартный гауссовский вектор. Имеем $\Mf \eta =0$.
Покажем, что ковариационная матрица у~вектора $\eta$ есть в~точности матрица $C$. Действительно,
ковариация есть билинейная функция на соответствующем пространстве. Мы знаем, как меняется значение билинейной
функции при замене координат: если $\eta = A\xi$, то $\cov(\eta) = A^t \cov(\xi) A$.
Значит, в~нашем случае
\eqn{\cov(\eta) = \cov(A \xi) = A^t \cov(\xi)A = \sqrt C E \sqrt C = C,}
так как у~стандартного вектора координаты не коррелируют.
Таким образом, распределение $\eta$ при каждом наборе $t_1\sco t_n$ задаёт конечномерное распределение
искомого гауссовского процесса.

Остаётся показать согласованность конечномерных распределений. Мы знаем, что гауссовские распределения однозначно
определяются матрицей ковариаций и~вектором математических ожиданий координат. Выполнение второго условия
согласованности очевидно\т перестановке $t_i$ и~$t_j$ соответствует перестановка $i$\д го и~$j$\д го
столбца, а~также $i$\д й и~$j$\д й строки в~матрице ковариаций (поэтому <<переставленное>> распределение будет как
раз таким, как нужно). Что касается первого свойства, то оно тоже очевидно, ибо любой подвектор гауссовского
вектора тоже гауссовский (это хорошо видно из третьего эквивалентного определения), и~новая ковариационная матрица
является подматрицей в~исходной матрице.
\end{proof}

\begin{ex}
Пусть $T = [0,\bes)$. Рассмотрим процесс, у~которого $K(s,t) = \min(s,t)$, а~$m(t) \equiv 0$. Симметричность $K$ очевидна.
Докажем её неотрицательную определённость: рассмотрим функции $f_s := \Ibb_{[0,s]}$ (индикатор соответствующего
отрезка). Очевидно, что
\eqn{\intl{0}{\bes} f_s f_t\dx = \min(s,t).}
Осталось заметить, что
\eqn{\sums{i,j}z_i\ol z_jK(t_i,t_j) = \sums{i,j}z_i\ol z_j \intl{0}{\bes}  f_{t_i}f_{t_j}\dx =
\intl{0}{\bes}\Br{\sum z_if_{t_i}(x)}\Br{\ol{\sum z_if_{t_i}(x)}}\dx = \intl{0}{\bes} \Bm{\sum z_i f_{t_i}(x)}^2\dx \ge 0.}
\end{ex}

\begin{dfn}{1}
Пусть $T=[0,\bes)$. Гауссовский процесс $W_t$,  у~которого $m(t) \equiv 0$, $K(s,t) = \min(s,t)$, называется
\emph{винеровским} процессом.
\end{dfn}

\begin{df}
Процесс $\xi_t$ называется \emph{процессом с~независимыми приращениями}, если для любых моментов времени
$0= t_0 < t_1 < \ldots < t_n$ величины
\eqn{\xi_{t_0}, (\xi_{t_1}-\xi_{t_0})\sco (\xi_{t_n} - \xi_{t_{n-1}})}
независимы в~совокупности.
\end{df}

\begin{stm}
Винеровский процесс $W_t$ обладает независимыми приращениями.
\end{stm}
\begin{proof}
Покажем, что приращения винеровского процесса\т это некоррелированные случайные величины.
В~самом деле, пусть $q < r < s < t$. Тогда
\mln{\cov(W_r - W_q, W_t-W_s) = \cov(W_r,W_t) - \cov(W_q,W_t) - \cov(W_r,W_s) + \cov(W_q,W_s) =\\=
\min(r,t) - \min(q,t) - \min(r,s) + \min(q,s) = r  - q - r + q = 0,}
что и~означает некоррелированность. Но для гауссовских случайных величин из некоррелированности
следует\footnote{Некоррелированность означает, что матрица ковариаций диагональна. Значит,
характеристическая функция распадётся в~произведение характеристических функций компонент вектора.
Это условие является необходимым и~достаточным для независимости.}
независимость, а~приращения винеровского процесса, очевидно, гауссовские, поскольку вектор
\eqn{\hr{W_0,W_{t_1}-W_0,W_{t_2}-W_{t_1}\sco W_{t_n}-W_{t_{n-1}}}}
легко получить из  вектора $(W_0,W_{t_1}\sco W_{t_n})$ линейным преобразованием (там будет
транспонированная жорданова клетка $-J^t(-1)$).
\end{proof}

\begin{dfn}{2}
\emph{Винеровским} называется процесс $W_t$ с~независимыми приращениями, у~которого $W_0 \eqas 0$,
и~$(W_t-W_s) \sim \Nc(0,t-s)$ для любых $s < t$.
\end{dfn}

\begin{stm}
Оба определения винеровского процесса эквивалентны.
\end{stm}
\begin{proof}
Покажем, что из первого определения следует второе. Имеем $W_0 \eqas 0$, так как $\Df W_0 = K(0,0)=0$.
Независимость приращений была отдельно проверена в~предыдущем утверждении. Величина $W_t-W_s$ имеет нормальное
распределение как линейная комбинация гауссовских величин. Найдём дисперсию. Так как ${\Mf W_t=0}$, то мы
имеем
\eqn{\Df(W_t-W_s) = K(t,t)-K(s,t)-K(t,s)+K(s,s) \bw = t-2\min(s,t)+s.}
Если $s < t$, то $\Df(W_t-W_s) \bw= t-2s+s=t-s$, что и~требовалось доказать.

\smallskip

Обратно: положим $s=0$. Тогда $W_t-W_s = W_t-W_0 = W_t \sim\Nc(0,t)$. Значит, $\Mf W_t =0$. Гауссовость следует из
того, что можно получить вектор $(W_{t_0}\sco W_{t_n})$ из соответствующего вектора приращений линейным
преобразованием, а~координаты вектора приращений независимы (см. эквивалентные определения гауссовского вектора).
Вычислим ковариационную функцию. При $s = t$ доказывать нечего. Пусть теперь $s < t$. Имеем
\eqn{K(s,t) = \cov(W_s,W_t) = \cov(W_s,W_t-W_s) + \cov(W_s,W_s) = \cov(W_s-W_0,W_t-W_s) + \Df W_s = 0 + s = s.}
Если же $s > t$, то аналогичной выкладкой получаем $K(s,t) = t$. Итак, доказано, что $K(s,t) = \min(s,t)$.
\end{proof}

\subsubsection{Стационарность случайных процессов}

\begin{df}
Случайный процесс $\xi_t$ называется \emph{стационарным в~широком смысле}, если $m(t) \equiv \const$
и~$K_\xi(s+h,t+h)\bw=K_\xi(s,t)$ для всех $h$.
\end{df}

Отметим, что винеровский процесс не является стационарным, но является процессом со~стационарными (в~широком смысле)
приращениями, то есть для $q < r< s< t$ выполняются свойства:

\begin{points}{-2}
\item $\Mf (\xi_t-\xi_s) = f(t-s)$, где $f$\т некоторая функция;
\item $\Mf\hs{(\xi_r-\xi_q)\ol{(\xi_t-\xi_s)}} = \Mf\hs{(\xi_{r+h}-\xi_{q+h})\ol{(\xi_{t+h}-\xi_{s+h})}}$ для всех $h$.
\end{points}


\begin{df}
Процесс $\xi_t$ называется \emph{стационарным в~узком смысле}, если  его конечномерные распределения
инвариантны относительно сдвигов, то есть обладают свойством
\eqn{\mu_{t_1+ h\sco t_n + h} = \mu_{t_1\sco t_n}.}
\end{df}

\begin{ex}
Рассмотрим процесс $\xi_n$, у~которого $\Mf \xi_n = 0$ и~$\Df \xi_n = 1$, а~все $\xi_n$ независимы.
Такой процесс будет стационарен в~широком смысле, ибо $K(n,m) = \Mf (\xi_n\xi_m) = \Mf\xi_n\cdot \Mf\xi_m = 0$
при $n \neq m$ и~$K(n,n) = \Df \xi_n = 1$. Однако такой процесс не обязан быть стационарным
в~узком смысле, потому что из стационарности в~узком смысле следует одинаковость распределений
величин $\xi_n$, а~это может быть и~не так.

Однако, если потребовать, чтобы величины были независимы и~одинаково распределены,
то процесс будет стационарным в~узком смысле.
\end{ex}

Из стационарности в~узком смысле следует стационарность в~широком смысле, при условии, что существуют
математическое ожидание и~дисперсия.

Заметим, что для гауссовского процесса стационарность в~узком и~широком смысле\т это одно и~то же,
поскольку его распределения однозначно определяются математическим ожиданием и~ковариационной функцией.

\begin{df}
Процесс $\xi_t$ имеет стационарные приращения в~узком смысле, если
\eqn{\Law\br{(\xi_{t_2}-\xi_{t_1})\sco (\xi_{t_n}-\xi_{t_{n-1}})} =
\Law\br{(\xi_{t_2+h}-\xi_{t_1+h})\sco (\xi_{t_n+h}-\xi_{t_{n-1}+h})}.}
\end{df}

\subsubsection{Непрерывность случайных процессов}

Будем рассматривать гильбертово пространство $H:= L_2(\Om, \Pf)$ и~случайные функции
$\xi\cln T\ra H$. При этом будем предполагать, что $T \subs \R$.

\begin{df}
Говорят, что $\xi_t \ra \eta$ в~$H$ при $t \ra t_0$ если $\Mf |\xi_t-\eta|^2 \ra 0$ при $t \ra t_0$.
Иначе говоря, это обычная сходимость в~норме пространства~$L_2$. Эту сходимость ещё
называют \emph{сходимостью в~среднем квадратичном}.
\end{df}

\begin{df}
Процесс называется \emph{непрерывным в~среднем квадратичном} в~точке $t_0$, если $\xi_t \ra \xi_{t_0}$ в~$H$.
\end{df}

Рассмотрим центрированный процесс $\wt\xi_t := \xi_t-m(t)$. Ясно, что $\Mf \wt\xi_t = 0$.
Заметим, что сходимость $\xi_t \ra \xi_{t_0}$ равносильна такому свойству:
\eqn{\wt\xi_t \ra \wt\xi_{t_0},\quad m(t) \ra m(t_0) \text{ при } t\ra t_0.}
В~самом деле, векторы $\wt \xi_t - \wt\xi_{t_0}$ и~$m(t) - m(t_0)$ ортогональны в~$H$,
потому что $\Mf \wt\xi_t = 0$ и~$\Mf \wt\xi_{t_0} = 0$. Теперь воспользуемся таким фактом:
длина гипотенузы прямоугольного треугольника стремится к~нулю тогда и~только тогда, когда
длины обоих катетов стремятся к~нулю. А~это как раз то, что мы хотим доказать.

В~следующих двух утверждениях предполагается, что $m(t)\equiv 0$.

\begin{stm}
Если $K_\xi(s,t)$ непрерывна в~точке $(t_0,t_0)$, то
$\xi_t$ непрерывен в~точке $t_0$ в~среднем квадратичном.
\end{stm}
\begin{proof}
В~самом деле,
\eqn{\hn{\xi_t - \xi_{t_0}}^2 = \Mf |\xi_t - \xi_{t_0}|^2 =  K(t,t) - K(t,t_0) - K(t_0,t) + K(t_0,t_0).}
В~силу непрерывности $K$ эта сумма стремится к~нулю.
\end{proof}

\begin{stm}
Пусть $\xi_t$ непрерывен при $t = t_0$ и~$t=s_0$.
Тогда $K$ непрерывна в~точке $(s_0,t_0)$.
\end{stm}
\begin{proof}
В~самом деле,
\eqn{K(s,t) - K(s_0,t_0) = K(s,t) - K(s_0,t) + K(s_0,t) - K(s_0,t_0) =
(\xi_s - \xi_{s_0}, \xi_t) + (\xi_{s_0},\xi_t-\xi_{t_0}).}
Здесь круглые скобки обозначают скалярное произведение в~$H$.
В~силу непрерывности скалярного произведения, оба слагаемых стремятся к~нулю.
\end{proof}

\begin{imp}
Пусть функция $K(s,t)$ непрерывна на множестве $\hc{t=s}$. Тогда $K(s,t)$ непрерывна всюду.
\end{imp}
\begin{proof}
В~самом деле, из непрерывности на диагонали следует, что процесс непрерывен в~каждой точке (в~силу
первого утверждения). Применяя второе утверждение, получаем, что она непрерывна всюду.
\end{proof}

\begin{problem}
\eqn{\exi \liml{t\ra t_0}\xi_t \quad\Lra\quad \case{\exi \liml{s,t\ra t_0} K_\xi(s,t),\\ \exi \liml{t\ra t_0} m(t).}}
\end{problem}
\begin{hint}
Использовать рассуждения, подобные тем, что были использованы при доказательстве двух предыдущих утверждений.
\end{hint}

\begin{df}
Говорят, что процесс $\xi_t$ имеет \emph{производную} в~точке $t_0$, если существует предел в
среднем квадратичном
\eqn{\xi'_{t_0} := \liml{t\ra t_0} \frac{\xi_t-\xi_{t_0}}{t-t_0}.}
\end{df}

\begin{problem}
Существует ли производная у~винеровского процесса?
\end{problem}

\begin{problem}
Доказать, что если $K_{st}''(s,t) \in \Cb\br{(a,b)\times(a,b)}$, то на интервале $(a,b)$ существует непрерывная
производная $\xi'_t$. Верно ли обратное?
\end{problem}

\section{Стохастический интеграл и~спектральное представление процессов}

\subsection{Стохастические интегралы}

\subsubsection{Интеграл от случайного процесса}

В~этом разделе $H:= L_2(\Om, \Pf)$, а~случайные функции предполагаются квадратично\д интегрируемыми.

Нам хотелось бы определить интеграл от случайной функции. При этом возникает та трудность,
что при фиксированном $\om$ функция $\xi(\cdot,\om)$ не обязана быть измеримой. Поэтому мы пойдём
другим путём.

Определим интеграл от случайной функции по отрезку $[a,b] \subs T$ (по аналогии с~интегралом Римана).
Пусть $P = \hc{a = s_0<\ldots< s_n =b}$\т разбиение отрезка~$[a,b]$. Через $\la(P)$ будем обозначать диаметр разбиения $P$.
Пусть $t_i \in [s_{i-1},s_i] =: \De_i$. Рассмотрим интегральную сумму
\eqn{S_P(\om) := \sumiun \xi_{t_i}(\om)|\De_i|.}
Таким образом, интегральная сумма\т это некоторая случайная величина. Тогда величина $\eta$
называется \emph{интегралом} от $\xi_t$ по отрезку $[a,b]$, если в~$H$ существует
предел\footnote{База <<$\la(P)\ra 0$>> означает, что предел не зависит от выбора точек разбиения.}
\eqn{\liml{\la(P) \ra 0} S_P = \eta.}
При этом пишут, что
\eqn{\eta = \intl{a}{b}\xi_t\dt.}

Гильбертово пространство $H$\т это, в~частности, полное метрическое пространство,
в~каком-то смысле мало отличающееся от~$\R^n$.
В~нём тоже есть понятия непрерывной кривой. Так вот, непрерывный в~среднем квадратическом
случайный процесс\т это непрерывная кривая в~этом пространстве, или, что то же самое,
непрерывная функция $\xi\cln [a,b] \ra  H$. Поэтому ничего не стоит дословно перенести
на такие функции следующие три утверждения из математического анализа.
Их доказательства проходят дословно, если заменить модуль на норму в~$H$.

\begin{theorem}[Кантора]
Процесс, непрерывный на отрезке, равномерно непрерывен на нём.
\end{theorem}

\begin{theorem}
Непрерывная на отрезке функция интегрируема на этом отрезке.
\end{theorem}

\begin{theorem}
Кусочно\д непрерывная на отрезке функция интегрируема на нём.
\end{theorem}

\subsubsection{Ортогональная векторная мера}

\begin{df}
Пусть $X$\т множество. \emph{Полукольцом} $\Rc$ множеств на $X$ называется семейство подмножеств~$X$,
замкнутое относительно (конечного) пересечения и~такое, что если $A \in \Rc$, то $X\wo A$ представляется
в~виде дизъюнктного объединения конечного числа множеств из $\Rc$. Кроме того, $\es \in \Rc$.
\end{df}

\begin{ex}
Множество всех полуинтервалов на полуинтервале образует полукольцо.
\end{ex}

\begin{df}
Пусть $(X, \Ac,\mu)$\т измеримое пространство с~мерой. Пусть $\Rc \subs \Ac$\т полукольцо,
порождающее $\si$\д алгебру $\Ac$, то есть $\Ac = \si(\Rc)$. Пусть $H$\т гильбертово пространство,
и~имеется отображение $\ze\cln \Rc\ra H$ такое, что
\eqn{\br{\ze(\De_1), \ze(\De_2)} = \mu(\De_1\cap \De_2),\quad \De_i \in \Rc.}
Тогда $\ze$ называется \emph{ортогональной векторной мерой}.
Мера $\mu$ называется её \emph{структурной мерой}.
\end{df}

Название <<ортогональная мера>> можно объяснить так:
если $\De_1 \cap \De_2 = \es$, то $\ze(\De_1) \bot \ze(\De_2)$. Осталось оправдать сам термин <<мера>>.

\begin{stm}
Функция $\ze$ является счётно\д аддитивной мерой на полукольце $\Rc$.
\end{stm}
\begin{proof}
Пусть $\De = \bigsqcup \De_i$, причём $\De, \De_i \in \Rc$. Докажем, что
\eqn{\ze(\De) = \sum\ze(\De_i).}
В~самом деле, имеем
\mln{\Bn{\ze(\De) - \sumiun\ze(\De_i)}^2 = \br{\ze(\De),\ze(\De)}
- \Br{\ze(\De), \sumiun\ze(\De_i)} -\Br{\sumiun\ze(\De_i), \ze(\De)}
+ \Br{\sumiun \ze(\De_i), \sumiun\ze(\De_i)} =\\=
\mu(\De) - \sumiun\mu(\De_i) - \sumiun\mu(\De_i) + \sumiun\mu(\De_i) = \mu(\De) - \sumiun\mu(\De_i). }
Если сумма исходно была конечной, то всё доказано. А~для счётных объединений остаётся
воспользоваться непрерывностью меры $\mu$.
\end{proof}

Часто в~качестве $H$ рассматривают пространство $L_2(\Om, \Pf)$. В~этом случае мера $\ze$ называется
\emph{ортогональной случайной мерой}.

\begin{df}
Процесс $\eta_t$ называется \emph{процессом с~ортогональными приращениями}, если для любого набора моментов времени
$q<r\le s<t$ выполняется свойство $(\eta_r - \eta_q) \bot (\eta_t - \eta_s)$.
\end{df}

Отметим, что любой процесс с~независимыми приращениями и~постоянным математическим ожиданием будет
иметь ортогональные приращения (например, винеровский процесс обладает этим свойством).

\begin{ex}
Пусть $\eta_t$\т процесс с~ортогональными приращениями. Рассмотрим полукольцо $\Rc$ полуинтервалов на $(a,b]$.
Построим ортогональную случайную меру. Пусть $\De := (s,t]$, тогда положим $\ze(\De) := \eta_t-\eta_s$.
Пусть процесс $\eta_t$ непрерывен справа. Положим $\mu(\De) := \hn{\eta_t-\eta_s}^2$.
Заметим, что полученная мера $\mu$ непрерывна: если $(s,t]\searrow \es$ при $t\ra s$, то
$\mu(\De) = \hn{\eta_t-\eta_s}^2\ra 0$.

Таким образом, мы построили некоторую непрерывную меру на полукольце.
Её можно продолжить на $\si$\д алгебру $\si(\Rc)$.
Мера $\ze$ и~будет ортогональной векторной мерой, построенной по процессу $\eta_t$.
\end{ex}

\begin{problem}
Пусть $\xi_t$\т процесс, у~которого $\xi_t \indep \xi_s$ при $t\neq s$ и~существует $\Mf\xi_t^2$.
Тогда либо этот процесс постоянен, либо разрывен всюду.
\end{problem}

\subsubsection{Интеграл по ортогональной векторной мере}

Рассмотрим пространство $L := L_2(X, \Ac,\mu)$, где $\Ac = \si(\Rc)$, а~$\Rc$\т полукольцо.
Пусть $H$\т гильбертово пространство, и~задана некоторая ортогональная векторная мера $\ze\cln \Rc\ra H$.
Мы хотим построить изометричное отображение $I\cln L\ra H$.

Пусть $\De \in \Rc$. Положим
\eqn{I(\Ibb_\De) := \ze(\De).}
Продолжим это отображение по линейности на все ступенчатые функции вида $f = \sum c_i \Ibb_{\De_i}$:
\eqn{I(f) := \sum c_i \ze(\De_i).}
Проверим изометричность (то есть, попросту, сохранение скалярного произведения):
\eqn{\br{I(f), I(g)} = \Br{\sum c_i \ze(\De_i), \sum d_j\ze(\Sig_j)} =
\sums{i,j}c_i\ol d_j \br{\ze(\De_i),\ze(\Sig_j)} = \sums{i,j}c_i\ol d_j \mu(\De_i \cap \Sig_j)=
\ints{X} f\ol g\dmu = (f,g).}
Поскольку ступенчатые функции плотны в~$L$, наше отображение $I$ можно продолжить по непрерывности
на всё пространство $L$: пусть $f_n \ra f$ в~$L$, тогда положим
$I(f) := \lim I(f_n)$. Этот предел существует в~силу одновременной фундаментальности последовательности
$\hc{f_n}$ и~её образа в~$H$. Корректность очевидна.

Итак, построено изометричное вложение $I\cln L \inj H$.
Заметим, что мы получаем явный вид для продолжения меры $\ze$ на $\si$\д алгебру $\Ac$: пусть $A \in \Ac$, тогда
\eqn{\ze(A) = I(\Ibb_A).}

Пусть $\xi_t$\т процесс с~ортогональными приращениями на множестве $T = (0,C]$.
Мы уже знаем, как по нему строить ортогональную векторную меру:
если $\De = (s,t]$, то $\ze(\De) := \xi_t-\xi_s$.
Используя соотношение $\mu(\De) = \hn{\ze(\De)}^2$, можно очень наглядно представлять себе
связь между структурной и~ортогональной мерами. Векторная мера интервала выдаёт вектор,
соединяющий точку, в~которой процесс был в~начале интервала, и~точку, в~которую он пришёл
к~концу интервала. Структурная же мера $\mu$ возвращает квадрат длины этого вектора. Таким образом,
интеграл от функции $f$ по процессу\т это просто интеграл Лебега по мере $\ze$.
Это будет некоторая случайная величина.
Обозначение таково:
\eqn{\ints{T} f\,d\xi_t := \ints{T} f(t)\,\ze(dt) \equiv \ints{T} f\dze.}

\subsubsection{Ортогональная и~структурная меры винеровского процесса}

Абстрактную чепуху предыдущего раздела проще переварить, если рассмотреть её на примере винеровского процесса $W_t$
на множестве $T = (0,C]$. Как мы уже отмечали, его приращения независимы, а~потому ортогональны.
Пусть $\De = (s,t]$. Поскольку $(W_t-W_s) \sim \Nc(0,t-s)$, а~дисперсия вектора с~нулевым средним\т
это в~точности квадрат его длины, получаем, что
\eqn{\mu(\De) = \hn{\ze(\De)}^2 = \hn{W_t-W_s}^2 = t-s.}
Мы видим, что в~нашем случае структурная мера процесса\т это просто мера Лебега!


Пусть теперь $f$\т некоторая (квадратично интегрируемая) функция. Положим
\eqn{\eta_t :=  \intl{0}{t} f(u)\,dW_u = I\hr{f \cdot\Ibb_{[0,t]}}.}
Покажем, что
\eqn{K_\eta(s,t) = \intl{0}{s \wg t} |f(u)|^2\du.}
Легко видеть, что $\Mf \eta_t\equiv0$ (это следует из того, что $\Mf(W_t-W_s)=0$).
Поэтому имеем
\mln{K_\eta(s,t) = (\eta_s,\eta_t) = \ints{T}f(u)\cdot\Ibb_{[0,s]}\ol{f(u)\cdot \Ibb_{[0,t]}}\,\mu_W(du) =\\=
\ints{T}|f(u)|^2 \cdot\Ibb_{[0,s\wg t]}\,\mu_W(du) = \intl{0}{s\wg t}|f(u)|^2\,\mu_W(du) = \intl{0}{s\wg t}|f(u)|^2\du,}
поскольку $\mu_W$\т это обычная мера Лебега.

Чтобы иметь возможность говорить о~$dW_s$ при $s < 0$, нужно определить ортогональную меру на $\R_-$.
Пусть $W_t^+$\т винеровский процесс, а~$W_t^-$\т ещё один винеровский процесс, не зависящий от $W_t^+$.
Пусть этим двум процессам отвечают ортогональные случайные меры $\ze^+$ и~$\ze^-$ соответственно.
Определим теперь меру $\ze$ на всей прямой, положив
\eqn{\ze(A) :=\case{
\ze^+(A),& A \subs \R_+;\\
0, &A = \hc{0};\\
\ze^-(-A), & A \subs \R_-.}
}
Для всех остальных множеств определим значение меры как сумму мер пересечений множества
с~$\R_+$ и~$\R_-$ соответственно.

\begin{problem}
Проверить, что полученная мера действительно является ортогональной.
\end{problem}

\subsubsection{Процесс Орнштейна\ч Уленбека}

\begin{dfn}{1}
\emph{Процессом Орнштейна\ч Уленбека} называется случайный процесс
\eqn{\xi_t := e^{-\al t}W_{f(t)},\quad f(t) := \frac{e^{2\al t}}{2\al}, \quad \al > 0.}
\end{dfn}

\begin{dfn}{2}
\emph{Процессом Орнштейна\ч Уленбека} называется центрированный гауссовский
стационарный процесс $\xi_t$ с~ковариационной функцией
\eqn{K_\xi(s,t)=\frac{1}{2\al} e^{-\alpha|s-t|},\quad \al > 0.}
\end{dfn}

\begin{stm}
Определения процесса Орнштейна\ч Уленбека эквивалентны.
\end{stm}
\begin{proof}
Покажем, что из первого определения следует второе.
Очевидно, $\Mf\xi_t \equiv 0$, так как $\Mf W_t\equiv 0$. Он будет гауссовским, поскольку вектор
$(\xi_{t_1}\sco \xi_{t_n})$ получается из гауссовского вектора $(W_{s_1}\sco W_{s_n})$, где $s_i = f(t_i)$,
линейным преобразованием, а~значит, тоже является гауссовским.
Найдём ковариационную функцию. Пусть $s< t$. Тогда имеем
\eqn{K_\xi(s,t) = \Mf\hr{e^{-\al s}W_{f(s)},e^{-\al t}W_{f(t)}} = e^{-\al(t+s)} \Mf\hr{W_{f(s)}W_{f(t)}}=
e^{-\al(t+s)} \min\hr{\frac{e^{2\al s}}{2\al}, \frac{e^{2\al t}}{2\al}} = \frac{1}{2\al}e^{-\al(t-s)}.}
Если же $t<s$, получим $K_\xi(s,t) = \frac{1}{2\al}e^{-\al(s-t)}$. Итого получаем
\eqn{K_\xi(s,t) = \frac{1}{2\al}e^{-\al|s-t|}.}
Отсюда же видно, что этот процесс является стационарным (в~широком смысле), ибо ковариационная функция зависит
лишь от разности аргументов.

\smallskip

Обратно, пусть $\xi_t$\т процесс Орнштейна\ч Уленбека в~смысле второго определения. Рассмотрим процесс
\eqn{\eta_t := \sqrt{2\al t}\cdot\xi_{g(t)}, \quad g(t) := \frac{\ln t}{2\al},}
и~положим $\eta_0:=0$.
Покажем, что он является винеровским (для этого проверим все условия первого определения винеровского
процесса). Центрированность очевидна, а~гауссовость проверяется аналогично. Найдём ковариационную функцию.
Пусть $s \le t$, тогда $g(s) \le g(t)$. Имеем
\eqn{\cov(\eta_{g(t)},\eta_{g(s)}) = K_\xi\br{g(t),g(s)} =
\frac{1}{2\al}\exp\hr{-\al \hr{\frac{\ln t}{2\al} - \frac{\ln s}{2\al} }}=
\frac{1}{2\al}\exp\hr{\frac{\ln\frac{s}{t}}{2}} = \frac{1}{2\al}\sqrt{\frac{s}{t}}.}
Отсюда
\eqn{K_\eta(s,t) = \sqrt{2\al t}\cdot \sqrt{2\al s} \cdot\cov(\eta_{g(t)},\eta_{g(s)}) =
2\al \sqrt{s t}\cdot \frac{1}{2\al}\cdot\sqrt{\frac{s}{t}} = s.}
Аналогично можно показать, что при $s > t$ получим $K_\eta(s,t) = t$. Таким образом, мы показали,
что $K_\eta(s,t) = \min(s,t)$, то есть совпадает с~ковариационной функцией винеровского процесса.

Сделаем замену $t = \frac{e^{2\al u}}{2\al}$. Тогда
\eqn{\frac{\ln t}{2\al} = u - \frac{\ln 2\al}{2\al}, \quad \sqrt{2\al t} = e^{\al u}.}
Тогда, обозначая $c := \frac{\ln 2\al}{2\al}$, получаем
\eqn{e^{\al u}\xi_{u-c} = W_{f(u)} \quad \Lra \quad \xi_{u-c} = e^{-\al u}W_{f(u)}, \quad c = g(2\al) = \frac{\ln 2\al}{2\al}.}
Мы видим, что процесс $\xi_t$ с~точностью до сдвига по времени\footnote{Это ни на что не влияет, ибо для гауссовских
процессов стационарность в~узком и~широком смыслах совпадают. Значит, у~него точно такое же распределение, как
и у~<<несдвинутого>> процесса.} совпадает с~процессом Орнштейна\ч Уленбека.
\end{proof}

\begin{problem}
Доказать, что процесс Орнштейна\ч Уленбека можно представить в~виде
\eqn{\xi_t := e^{-\al t} \Br{\xi_0 + \intl{0}{t}e^{\al u}\,dW_u},}
где $\al > 0$, а~величина $\xi_0$ имеет распределение $\Nc\hr{0,\frac{1}{2\al}}$ и~не зависит от $W_t$.
\end{problem}
\begin{solution}
Для краткости положим
\eqn{I(t) := \intl{0}{t}e^{\al u}\,dW_u.}
Вычислим ковариационную функцию процесса $\xi_t$. Пусть, как обычно, $s \le t$.
В~силу независимости $\xi_0$ и~$W_t$ имеем
\eqn{\cov\br{\xi_0 + I(s), \xi_0 + I(t)} = \cov(\xi_0,\xi_0) + \cov\br{I(s),I(t)}=
\frac{1}{2\al} + \intl{0}{s\wg t} e^{2\al u}\du = \frac{1}{2\al} + \frac{1}{2\al}(e^{2\al s} -1) =
\frac1{2\al}e^{2\al s}.}
Отсюда
\eqn{K_\xi(s,t) = e^{-\al s} \cdot e^{-\al t}\cdot \cov\br{\xi_0 + I(s), \xi_0 + I(t)} = \frac1{2\al}
e^{-\al(t -s)}.}
Аналогично, при $s > t$ получаем
\eqn{K_\xi(s,t) = \frac1{2\al}e^{-\al(s -t)}.}
Таким образом, показано, что $K_\xi(s,t) = \frac{1}{2\al}  e^{-\al|s -t|}$.

Осталось объяснить, почему полученный процесс будет гауссовским. Это следует
из общего факта о~том, что предел гауссовских величин тоже является гауссовской
величиной, а~интеграл есть предел частичных сумм (линейных комбинаций
гауссовских приращений винеровского процесса).
\end{solution}

Пользуясь результатами предыдущего раздела (доопределение $dW_s$ при $s<0$) и~этой задачей,
можно доопределить процесс Орнштейна\ч Уленбека на всей прямой:
\eqn{\eta_t := e^{-\al t} \Br{\xi_0 + \intl{-\bes}{t}e^{\al s}\,dW_s} \equiv
e^{-\al t} \Br{\xi_0 + \intl{-\bes}{t}e^{\al s}\,\ze(ds)}.}

\subsection{Стационарные процессы}

\subsubsection{Примеры и~свойства стационарных процессов}

Рассмотрим стационарный процесс $\xi_t$, у~которого $m(t) \equiv \const$ и~$K_\xi(s,t) = \wt K(s-t)$.
Для упрощения обозначений мы не будем писать волну у~функции $K$, потому что сама ковариационная функция нам
здесь не нужна.
В~общем случае получаем, что $K(-t) = \ol{K(t)}$, а~если процесс вещественный, то $K$ будет чётной функцией.
Заметим, что если $K$ непрерывна в~нуле, то процесс непрерывен в~среднем квадратичном, а~потому непрерывна всюду
его ковариационная функция. Но тогда $K$ будет непрерывна всюду.

Рассмотрим несколько примеров стационарных процессов.

\begin{ex}
Если $\xi_n$\т независимые одинаково распределённые случайные величины, то процесс $\xi_n$
стационарен в~узком смысле.
\end{ex}

\begin{ex}
Пусть $\Mf \xi_n = 0$, $\Df \xi_n = \si^2$ и~$\xi_i \bot \xi_j$ при $i \neq j$.
Тогда процесс $\xi_n$ стационарен в~широком смысле.
\end{ex}

\begin{ex}
Пусть $\xi_k$\т вещественные ортогональные случайные величины, и~$\Mf \xi_k = 0$.
Пусть $\la_k \in \R$. Рассмотрим случайные величины
\eqn{\eta_n := \suml{k=1}{N} \xi_k e^{i\la_k n}, \quad n \in \Z.}
Покажем, что процесс $\eta_n$ стационарен в~широком смысле.
Очевидно, что $\Mf \eta_n = 0$, поэтому $K_\eta(n,m) = \Mf(\eta_n\ol\eta_m)$.
Следовательно,
\mln{K_\eta(n,m) = \Mf\Br{\suml{k=1}{N} \xi_k e^{i\la_k n} \cdot
\suml{l=1}{N} \xi_l e^{-i\la_l m}} = \suml{k,l=1}{N}e^{i\la_k n}e^{-i\la_l m}\Mf(\xi_k\ol \xi_l)=\\=
\suml{k=1}{N}e^{i\la_k n}e^{-i\la_k m}\Mf|\xi_k|^2 =
\suml{k=1}{N}e^{i\la_k (n-m)}\Mf|\xi_k|^2 = K(n-m).}
Таким образом, ковариационная функция зависит только от разности своих аргументов.
Это и~означает стационарность в~широком смысле.
\end{ex}

\begin{ex}
Пусть $\xi_n\in L_2(\Om,\Pf)$ независимы и~одинаково распределены, и~$\hc{a_n}\in \ell_2$.
Положим
\eqn{\eta_n := \suml{k=0}{\bes} a_k \xi_{n+k}.}
Можно считать, что $\Mf \xi_n\equiv 0$ (ковариация будет такой же). Покажем, что процесс $\eta_n$
стационарен в~широком смысле. Имеем
\eqn{\cov(\eta_m,\eta_n)=\Br{\sums{k} a_k\xi_{m+k}, \ol{\sums{p} a_p\xi_{n+p}}} =
\sums{k,p}a_k\ol a_p\Mf(\xi_{m+k}\ol\xi_{n+p}).}
В~силу независимости останутся только слагаемые, для которых $m+k=n+p$, то есть $p=k-(n-m)$.
Пусть $\Mf |\xi_n|^2 = M$. Тогда ковариация равна
\eqn{\cov(\eta_m,\eta_n)=\sums{k}a_k\ol a_{k-(n-m)}\Mf|\xi_{n+k}|^2 = M \cdot \sums{k} a_k\ol a_{k-(n-m)}.}
Тем самым показано, что ковариационная функция зависит только от разности $n-m$.
\end{ex}


\subsubsection{Теоремы Бохнера\ч Хинчина и~Герглотца}

Две теоремы, которые далее будут сформулированы, мы доказывать не будем.

\begin{df}
Функция $K(t)$ называется \emph{неотрицательно определённой}, если таковой является
функция $\wh K(s,t) := K(s-t)$ для всех $s,t\in T$.
\end{df}

Будем называть функцию $K(t)$ \emph{симметричной}, если $K(-t)=\ol{K(t)}$.

\begin{theorem}[Бохнера\ч Хинчина]
Пусть функция $K(t)$ непрерывна на $\R$, симметрична и~неотрицательно определена.
Тогда имеет место представление
\eqn{K(t) = \ints{\R}e^{i s t}\,\mu(ds),}
где $\mu$\т некоторая конечная мера на $\R$.
\end{theorem}


\begin{theorem}[Герглотца]
Пусть симметричная функция $K(n)$, где $n\in \Z$, является неотрицательно определённой. Тогда\footnote{На самом
деле, теорема Герглотца утверждает большее: представимость функции $K(n)$
таким интегралом является \emph{необходимым и~достаточным} условием для неотрицательной определённости.}
имеет место представление
\eqn{K(n) = \intl{-\pi}{\pi}e^{i s n}\,\mu(ds),}
где $\mu$\т некоторая конечная мера на $[-\pi,\pi]$.
\end{theorem}

Пусть $\xi_t$\т стационарный процесс, а~$K_\xi(t)$\т его ковариационная функция.
Заметим, что к~функции $K_\xi(t)$ применима теорема Бохнера\ч Хинчина:
\eqn{K_\xi(t) = \ints{\R}e^{i s t}\,\mu_\xi(ds).}
Полученная мера $\mu_\xi$ называется \emph{спектральной мерой} процесса $\xi_t$.
В~случае, если она оказалась абсолютно непрерывной относительно меры Лебега,
её можно по теореме Радона\ч Никодима представить некоторой плотностью,
называемой в~этом случае \emph{спектральной}: $\mu_\xi(ds) = \rho_\xi(s)\ds$.

\subsubsection{Теорема о~спектральном представлении стационарного процесса}

Пусть, как обычно, $H := L_2(\Om,\Pf)$, а~$\xi_t$\т стационарный в~широком смысле центрированный
процесс, и~$K(t)$ непрерывна в~нуле.
Положим
\eqn{H_\xi^0 := \ha{\xi_{t_1}\sco \xi_{t_k}\vl t_i \in T}.}
Это минимальное линейное пространство, натянутое на множество векторов $\hc{\xi_t\vl t \in T}$.
Рассмотрим пространство
\eqn{H_\xi := \Cl H_\xi^0.}
Заметим, что $H_\xi$ сепарабельно, поскольку в~силу непрерывности процесса коэффициенты
линейных комбинаций и~моменты времени можно брать только рациональными.

Заметим, что пространство $L := L_2(\R,\mu_\xi)$ также сепарабельно и~потому изоморфно $H_\xi$.
Построим явно изоморфизм $\Ph\cln L \ra H_\xi$. Обозначим $\ph_t(\la) := e^{i\la t}$.
Положим
\eqn{\Ph(\ph_t) := \xi_t}
и~покажем, что определённое так отображение сохраняет скалярное произведение.
Действительно,
\eqn{(\xi_t,\xi_s) = \Mf (\xi_t\ol\xi_s)= K(t-s) \stackrel!= \ints\R e^{iu(t-s)}\mu_\xi(du) =
\ints\R e^{iut}\cdot e^{-ius}\mu_\xi(du) = \ints\R e^{i ut}\ol{e^{i u s}}\mu_\xi(du) = (\ph_t,\ph_s).}
В~равенстве, отмеченном <<!>>, применяется теорема Бохнера\ч Хинчина. Распространяя отображение
$\Ph$ по линейности и~замечая, что функции $\sum c_k\ph_{t_k}$ плотны в~$L_2$ по любой мере,
получаем искомый изоморфизм. Сюръективность следует из того, что образами функций $\ph_t$ являются
как раз порождающие пространства $H_\xi$.

После того, как изоморфизм $\Ph$ построен, можно сформулировать и~доказать саму теорему
о~спектральном представлении.

\begin{theorem}[О~спектральном представлении стационарного процесса]
Существует ортогональная случайная мера $Z_\xi$ на $\R$ такая, что
\eqn{\label{eqn:specRep}\xi_t = \ints\R e^{ist}Z_\xi(ds).}
\end{theorem}
\begin{proof}
Построим ортогональную векторную меру $\ze\cln \Bs(\R) \ra L_2(\R,\mu_\xi)$.
Пусть $A$\т борелевское множество на прямой. Положим $\ze(A) := \Ibb_A$. Она действительно
будет ортогональной векторной мерой, поскольку
$(\Ibb_A, \Ibb_B) \bw= \mu_\xi(A \cap B)$. Теперь построим ортогональную случайную меру,
<<перетащив>> $\ze$ с~помощью нашего изоморфизма~$\Ph$ на $H_\xi$:
\eqn{Z_\xi(A) := \Ph\br{\ze(A)}.}
Сохранение свойства ортогональности обеспечивается тем, что $\Ph$\т изоморфизм
гильбертовых пространств.
Осталось пояснить, почему верно равенство~\eqref{eqn:specRep}.

Вспомним, как велось построение стохастического интеграла (по ортогональной случайной мере).
На индикаторе множества его значение
полагалось равным ортогональной мере этого множества. В~нашем случае имеем
\eqn{I(\Ibb_A) = Z_\xi(A).}
С~другой стороны, изоморфизм $\Ph$ был устроен так, что
\eqn{\Ph(\Ibb_A) = \Ph\br{\ze(A)} =  Z_\xi(A).}
Но если два линейных отображения совпадают на индикаторах,
то они обязаны совпадать. Итак, $I = \Ph$, а~это и~означает, что
имеет место указанная формула, ибо
\eqn{\xi_t = \Phi(\ph_t(s)) = I\br{\ph_t(s)} = I(e^{ist}) = \ints\R e^{ist}Z_\xi(ds).}
Теорема доказана.
\end{proof}

\subsubsection{Эквивалентное условие дифференцируемости стационарного процесса}

\begin{theorem}
Пусть $\xi_t$\т стационарный центрированный процесс. Производная $\xi_t'$ существует тогда и~только тогда,
когда существует интеграл
\eqn{\ints\R s^2\mu_\xi(ds).}
\end{theorem}
\begin{proof}
Пусть процесс дифференцируем. Тогда существует предел в~среднем квадратичном в~$H_\xi$:
\eqn{\xi'_t = \liml{h\ra 0}\frac{\xi_{t+h}-\xi_t}{h}.}
Поскольку при изоморфизме $\Ph$ вектору $\xi_t$ соответствует вектор $\ph_t$, получаем,
что существует и~соответствующий предел в~$L_2(\R,\mu_\xi)$:
\eqn{\liml{h\ra 0}\frac{\ph_{t+h}-\ph_t}{h} = \frac{d}{dt}\ph_t(s) = \frac{d}{dt}\br{e^{its}} = is\cdot e^{its}.}
Следовательно, функция $is \cdot e^{its}$ должна лежать в~$L_2(\R,\mu_\xi)$, то есть
существует интеграл от функции $\hm{is\cdot e^{its}}^2 \bw= s^2$. Но это и~означает, что
\eqn{\ints\R s^2\mu_\xi(ds) < \bes.}
Для доказательства обратного утверждения нужно лишь заметить, что рассуждения обратимы.
\end{proof}

Заметим, что производная стационарного процесса, если она существует,
автоматически является стационарным процессом, поскольку всякий линейный оператор
с~постоянными коэффициентами, очевидно, не портит стационарности в~широком смысле
(не верите\т напишите ковариационную функцию для произвольной линейной комбинации
стационарных процессов, раскройте всё по линейности и~убедитесь в~том,
что полученное выражение инвариантно относительно сдвигов).

\begin{problem}
Пусть $\xi_t$\т стационарный процесс. Если существует непрерывный
процесс $\xi'_t$ , то его спектральная мера будет такая:
\eqn{\mu_{\xi'}(d\la) = \la^2\mu_\xi(d\la).}
\end{problem}
\begin{solution}
Имеем
\eqn{\xi'_t = \liml{h\ra 0}\frac{\xi_{t+h} - \xi_{t}}{h} = \liml{h\ra 0}\Ph \hr{\frac{e^{i \la (t+h)} - e^{i \la t}}{h}}
= \Ph\hr{i\la e^{i \la t}}.}

Пусть $K_{\xi'}$\т ковариационная функция процесса $\xi'_t$. По теореме Бохнера\ч Хинчина она представляется
интегралом по некоторой мере $\mu_{\xi'}$:
\eqn{K_{\xi'}(\tau) = \ints{\R}e^{i\la \tau}\,\mu_{\xi'}(d\la).}
Теперь явно посчитаем ковариационную функцию $K_{\xi'}$. Имеем
\eqn{K_{\xi'}(s,t) =  \Mf \hr{\xi'_s \ol{\xi_t'}} = \ints{\R}i \la e^{i \la s} \ol{i \la e^{i \la t}}\,\mu_\xi(d\la) = \ints{\R}\la^2 e^{i \la (s-t)} \,\mu_\xi(d\la).}
Положим $\tau = s-t$ :
\eqn{K_{\xi'}(\tau) =  \ints{\R}\la^2 e^{i \la \tau}\,\mu_\xi(d\la) = \ints{\R}e^{i \la \tau} \,\mu_{\xi'}(d\la).}
Теперь мы видим, что интегралы от функций вида $e^{i\la \tau}$ по двум разным мерам совпадают для всех $\tau$.
Эти функции всюду плотны в~$L_2$ по любой мере. Значит, можно сколь угодно точно приблизить индикатор любого
множества линейной комбинацией таких функций, а~отсюда следует, что меры $\mu_{\xi'}$ и~$\la^2\mu_\xi$
совпадают.
\end{solution}


\subsubsection{Эргодическая теорема}


\begin{theorem}[Эргодическая теорема, дискретный случай]
Пусть $\xi_n$\т стационарный центрированный процесс. Тогда
\eqn{\frac{1}{n}\suml{k=1}{n} \xi_k \ra Z_\xi\br{\hc0} \text{ в~} H_\xi, \quad n \ra \bes.}
\end{theorem}

\begin{theorem}[Эргодическая теорема, непрерывный случай]
Пусть $\xi_t$\т непрерывный в~среднем квадратичном центрированный стационарный процесс. Тогда
\eqn{\frac{1}{T} \intl{0}{T}\xi_t\dt \ra Z_\xi\br{\hc0} \text{ в~} H_\xi, \quad T \ra \bes.}
\end{theorem}
\begin{proof}
Нам нужно показать, что наш интеграл сходится в~пространстве $H_\xi$.
С~помощью изоморфизма $\Ph$ перенесём эту сходимость в~пространство $L_2(\R,\mu_\xi)$.
Величине $\xi_t$ при изоморфизме $\Ph$ соответствует функция $\ph_t(s) = e^{its}$,
а~мере множества $Z_\xi(A)$\т индикатор множества $A$.
Итак, теперь нам нужно доказать, что
\eqn{\frac1T\intl0T e^{its}\dt \ra \Ibb_{\hc0} \text{ в~} L_2(\R,\mu_\xi).}
Несложно проверить, что имеет место поточечная сходимость
\eqn{\frac1T\intl{0}{T}e^{its}\dt \ra \Ibb_{\hc0} = \case{1,& s = 0;\\0,&s \neq 0,} \quad T \ra \bes.}
Осталось обосновать предельный переход~${T \ra \bes}$ и~показать, что имеется сходимость в~среднем квадратичном.
Применим теорему Лебега о~мажорированной сходимости. Для нахождения мажоранты
воспользуемся неравенством $|a-b|^2 \le 2\br{|a|^2 + |b|^2}$. Применяя его, получаем
\eqn{\Bm{\frac1T\intl0Te^{its}\dt - \Ibb_{\hc0}}^2 \le 2 \bbs{\frac{1}{T^2}\Bm{\intl{0}{T}e^{its}\dt}^2 + 1}
\le 2(1 + 1) = 4.}
А~поскольку мера $\mu_\xi$ на $\R$ конечна, всякая константа является интегрируемой функцией.
\end{proof}

\begin{note}
Доказательство дискретного случая этой теоремы ничем не отличается от непрерывного,
разве что интеграл от $0$ до $T$ заменится на сумму от $1$ до $n$.
\end{note}

Пусть процесс $\xi_t$ стационарен (в~широком смысле). Тогда его матожидание постоянно
и~равно некоторому числу~$m$.
Как мы знаем, интеграл от случайного процесса по времени\т это предел интегральных сумм.
Математическое ожидание каждой суммы, очевидно, равно сумме длин отрезков разбиения, умноженных на $m = \Mf \xi_{t_k}$,
и~потому оно просто равно $mT$, где $T$\т длина отрезка интегрирования.
Значит,
\eqn{\Mf \frac1T\intl0T\xi_t\dt = \frac1T\cdot m T = m.}
С~другой стороны, если рассмотреть центрированный процесс $\wt\xi_t := \xi_t - m$, то для него
$Z_{\wt\xi} = Z_\xi$ и~справедлива эргодическая теорема, из которой следует, что
\eqn{\frac1T\intl0T\wt\xi_t\dt \ra Z_\xi\br{\hc0}.}
Следовательно,
\eqn{\frac1T\intl0T\xi_t\dt \ra m + Z_\xi\br{\hc0}.}

\begin{imp}
Мера $\mu_\xi$ не имеет атомов в~нуле тогда и~только тогда, когда имеет место закон больших чисел
\eqn{\frac1T\intl0T\xi_t\dt \ra m, \quad T \ra \bes.}
\end{imp}
\begin{proof}
ЗБЧ будет выполнен, если и~только если вектор $Z_\xi\br{\hc0}$ равен нулю в~$H_\xi$.
Но, как мы знаем, структурная мера множества\т это квадрат длины значения соответствующей
векторной меры на этом множестве. Итак, вектор будет нулевым тогда и~только тогда,
когда структурная мера нашего множества нулевая. А~это и~требуется доказать.
\end{proof}

\section{Пуассоновский и~винеровский процессы}

\subsection{Пуассоновский процесс}

\subsubsection{Определение и~свойства пуассоновского процесса}

Напомним, что случайная величина $\xi$ имеет пуассоновское распределение $\pi(\la)$,
если
\eqn{\Pf(\xi = k) = e^{-\la}\cdot \frac{\la^k}{k!}, \quad k \in \Z_+.}

\begin{df}
Пусть $T = [0,+\bes)$, а~$\la$\т фиксированное положительное число.
Процесс $\xi_t$ называется \emph{пуассоновским}, если
\begin{points}{-2}
\item $\xi_0 = 0$;
\item Процесс имеет независимые приращения;
\item $\xi_t-\xi_s \sim \pi\br{\la(t-s)}$ при $s < t$.
\end{points}
\end{df}

\begin{stm}[Свойства пуассоновского процесса]
\begin{points}{-2}
\item Приращения процесса стационарны;
\item $\xi_t \sim \pi(\la t)$;
\item $\Mf \xi_t = \la t$, $\Df \xi_t = \la t$;
\item $\xi_t$ принимает целые неотрицательные значения;
\item $\xi_t$ непрерывен в~среднем квадратическом.
\end{points}
\end{stm}
\begin{proof} В~самом деле,
\begin{points}{-2}
\item Следует непосредственно из определения: распределение приращений зависит только от разности $(t-s)$.
\item Имеем $\xi_0 \eqas 0$, поэтому $\xi_t = \xi_t - \xi_0 \sim \pi(\la t)$.
\item Из теории вероятностей мы знаем, что если $\xi \sim \pi(\la)$, то $\Mf\xi = \la$ и~$\Df\xi = \la$. Теперь всё следует из \pt2.
\item Сразу следует из \pt2 и~определения пуассоновской случайной величины.
\item По определению процесса, $(\xi_{t+h} - \xi_t) \sim \pi(\la h)$. Поэтому
\eqn{\hn{\xi_{t+h} - \xi_t}^2_{L_2(\Pf)} = \Mf |\xi_{t+h}-\xi_t|^2 = \Df(\xi_{t+h}-\xi_t) + \Mf^2|\xi_{t+h}- \xi_t| = \la h + (\la h)^2 \ra 0, \quad h \ra 0.}
\end{points}
Ну вот и~всё, а~вы боялись\ldots
\end{proof}

\subsubsection{Явная конструкция пуассоновского процесса}

Сейчас мы построим некоторый процесс, а~потом докажем, что он является
пуассоновским.
Рассмотрим последовательность $\ze_n$ независимых случайных величин, распределённых
показательно с~параметром $\la > 0$, то есть имеющих плотность
\eqn{p(x) = \Ibb_{\hc{x \ge0}}\la e^{-\la x}.}
Положим
\eqn{S_n := \sumiun \ze_i, \quad S_0 := 0.}
Положим теперь
\eqn{\xi_t := \max\hc{n\cln S_n \le t}, \quad t \ge 0.}

\vbox{\centerline{\epsfbox{pictures.1}}
\centerline{\footnotesize Рис.\,1. Типичная траектория пуассоновского процесса}}

Напомним, что \emph{вариационным рядом} выборки $X_1\sco X_n$ называются случайные величины
$X_{(1)}\sco X_{(n)}$, полученные из исходной выборки упорядочением по возрастанию.

\begin{lemma}
Пусть $\tau_i$\т точки, в~которых происходят скачки траектории процесса $\xi_t$.
Они, очевидно, являются случайными величинами.
Пусть $t > 0$. Тогда совместное распределение величин $\tau_1\sco \tau_n$ при условии,
что на отрезке $\De := [0,t]$ имеется ровно $n$ скачков, совпадает с~распределением
вариационного ряда для независимых случайных величин $X_1\sco X_n$, равномерно распределённых на~$\De$.
\end{lemma}
\begin{proof}
Обозначим через $Y_1\sco Y_n$ вариационный ряд, соответствующий величинам $X_1\sco X_n$.
Рассмотрим непересекающиеся упорядоченные интервалы $\De_1\sco \De_n$ на отрезке~$\De$.
Вычислим вероятность
\eqn{P_1 := \Pf(Y_1\in\De_1\sco Y_n\in\De_n).}
Очевидно, имеет место равенство событий
\eqn{A := \hc{Y_1\in \De_1\sco Y_n\in\De_n}= \cups{\si \in \Sb_n}\hc{X_{\si(1)}\sco X_{\si(n)}} =: \cups{\si}B_\si.}
Так как отрезки $\De_i$ не пересекаются (в~худшем случае имеют общие концы), вероятность события $A$ равна сумме
вероятностей событий в~правой части.

В~силу независимости величин $X_i$ и~их равномерной распределённости вероятность каждого из событий $B_\si$ не
зависит от перестановки $\si$ и~равна
\eqn{\Pf(B_\si) = \frac{|\De_1|}{t}\sd \frac{|\De_n|}{t}.}
Значит, просуммировав по всем перестановкам одинаковые слагаемые, коих $n!$, получим
\eqn{P_1 = n! \cdot \frac{|\De_1|}{t}\sd \frac{|\De_n|}{t} = \frac{n!}{t^n} \cdot \prodl{i=1}{n}|\De_i|.}

Теперь посчитаем условную вероятность:
\eqn{P_2 := \Pf(\ub{\tau_1 \in \De_1\sco \tau_n\in\De_n}_{A} \vl \ub{n \text{ скачков на } \De}_B) = \frac{\Pf(A B)}{\Pf(B)}=
\frac{\Pf(S_1 \in \De_1\sco S_n \in \De_n; \; S_{n+1} > t)}{\Pf(S_n \le t, S_{n+1} > t)}.}
Поскольку $\ze_i$ независимы, их совместная плотность\т это произведение одномерных плотностей:
\eqn{p_{\vec\ze}(x_1\sco x_n) = \Ibb_{\hc{x_i \ge 0}}\cdot \la^n e^{-\la (x_1\spl x_n)}.}
Вычислим плотность распределения сумм $S_n$. Имеем
\eqn{\rbmat{S_1\\ S_2\\ \vdots \\ S_n} =
\rbmat{1 &  &  & \bigsyml0 \\ & 1 &  & \\ & & \ddots & \\ \bigsymh1 & & & 1}\rbmat{\ze_1\\ \ze_2\\ \vdots \\ \ze_n} }
Сделаем замену переменных. Из анализа известно: если имеется гладкая замена переменных $y=y(x)$,
то плотность преобразуется по формуле (это обычная замена переменных в~кратном интеграле)
\eqn{p\br{y(x)} = p(y)\cdot \hm{\pf{x}{y}}.}
В~нашем случае замена координат линейна, а~потому $\pf{x}{y} = С^{-1}$. Легко проверить, что
\eqn{C^{-1} = \rbmat{1 &  &  & \bigsyml0 \\ -1 & 1 &  & \\ & \ddots & \ddots & \\ \bigsymh0 & & -1 & 1}.}
Значит, $C \cdot (y_1\sco y_n)^t = (y_1,y_2-y_1\sco y_n-y_{n-1})^t$.
Тогда плотность в~новых координатах превратится~в
\eqn{p(y) = \Ibb_{\hc{0\le y_1 \le\ldots\le y_n}} \cdot \la^n e^{-\la y_n}.}
Таким образом, искомая вероятность равна
\eqn{P(A B) = \scomp{\ints{\prod \De_i \times [t,\bes)}}\la^{n+1} e^{-\la y_{n+1}} \dy_1\ldots\dy_{n+1} =
\la^n\intl{t}{\bes}\la e^{-\la y_{n+1}}\dy_{n+1}\mcomp{\ints{\prod \De_i}}\dy_1\ldots \dy_n =
\la^n e^{-\la t} \prodl{i=1}{n}|\De_i|.}
Теперь вычислим знаменатель. Обозначим $Y := \hc{0\le y_1\le\ldots y_n\le t}$. Имеем
\eqn{P(B) = \ints{Y\times[t,\bes)}\la^{n+1}e^{-\la y_{n+1}}\dy_1\ldots\dy_{n+1} =
\la ^n  e^{-\la t}\ints{Y}\dy_1\ldots\dy_n.}
Оставшийся интеграл представляет собою объём $n$\д мерного симплекса с~ребром длины $t$, который
равен $\frac{t^n}{n!}$. Последнее тривиально проверяется, например, по индукции. Итак,
\eqn{P_2 = \frac{\Pf(A B)}{\Pf(B)} =
\frac{\la^n e^{-\la t} \prodl{i=1}{n}|\De_i|}{\la^n e^{-\la t} \cdot \frac{t^n}{n!}} =
\frac{n!}{t^n} \cdot \prodl{i=1}{n}|\De_i|.}
Таким образом, $P_1 = P_2$ и~мы доказали, что вероятность попадания двух случайных
векторов во всякий параллелепипед одинакова. Но из совпадения двух вероятностных мер на
порождающих $\si$\д алгебры следует их совпадение на всей $\Bs(\De^n)$. Лемма доказана.
\end{proof}

\begin{theorem}
Построенный процесс $\xi_t$ является пуассоновским с~параметром $\la$.
\end{theorem}
\begin{proof}
Применим лемму и~докажем, что конечномерные распределения у~приращений процесса $\xi_t$ имеют
требуемое распределение.
Пусть $0\le t_1< t_2<\ldots < t_n=t$ и~$k_1\spl k_n= N$, а~$\De_i := |t_i-t_{i-1}|$.
Тогда
\eqn{P := \Pf(\ub{\xi_{t_1} = k_1, \xi_{t_2}-\xi_{t_1}= k_2\sco \xi_{t_n}-\xi_{t_{n-1}}=k_n}_A)= \Pf(A|B)\cdot \Pf(B),}
где $B = \hc{N \text{ скачков на } [0,t]}$.

На отрезок бросают $N$ точек. Пусть событие $A_i$\т <<точка попала на отрезок $(t_{i-1},t_i]$>>.
Тогда по лемме
\mln{P = \Pf(k_i \text{ раз произошло } A_i) \cdot \Pf(B) =
\frac{N!}{k_1!\sd k_n!} \prodl{i=1}{n}\frac{\De_i^{k_i}}{t^{k_i}}\cdot \la^N e^{-\la t} \frac{t^N}{N!} =\\=
 e^{-\la t} \prodl{i=1}{n} \De_i^{k_i} \cdot \frac{\la^N}{k_1!\sd k_n!} =
 \prodl{i=1}{n}e^{-\la \De_i} \frac{(\la \De_i)^{k_i}}{k_i!}.}
Отсюда видно, что распределения приращений пуассоновские. Отсюда же вытекает и~независимость
приращений, так как совместная вероятность распалась в~произведение <<одномерных>> вероятностей.
\end{proof}

\subsubsection{Ортогональная случайная мера пуассоновского процесса}

Построим по пуассоновскому процессу $\xi_t$ ортогональную случайную меру. Пусть $\tau_i$\т
моменты скачков процесса (их ещё называют \emph{пуассоновским потоком}), а~$\tau_i'$\т
такой же поток, не зависящий от первого и~отложенный в~отрицательном направлении оси.
Тогда число~$N_\De$
скачков на отрезке $\De$ равно сумме $N_1+N_2$, где $N_1$\т число скачков
на $\De_1 := \De\cap[0,+\bes)$, а~$N_2$\т число
скачков на $\De_2 = \De \cap (-\bes,0)$. Величины $N_i$ независимы, поэтому
\eqn{\pi(\la |\De_1| + \la |\De_2|) = \pi(\la |\De|).}
Случайную меру зададим так: $\ze\br{(a,b]} := N_\De$. Но так как у~$N_\De$ ненулевое матожидание,
рассмотрим меру
\eqn{\wt \ze := \ze(\De)-\Mf (N_\De) = \ze(\De)-\la|\De|.}
Тогда структурная мера $\mu_\xi(\De) = \Df N_\De = \la |\De|$. Мы видим, что она
пропорциональна лебеговской мере.

\begin{note}
Из всех этих свойств видно, что у~пуассоновского процесса много общего с~винеровским.
\end{note}

\begin{problem}
Найти характеристическую функцию процесса
\eqn{\eta_t := \intl{0}{t} f(s)\dze(ds).}
\end{problem}
\begin{answer}
\eqn{\ph_{\eta_t}(x) = \exp\BS{\la \intl{0}{t}e^{ix f(s)}\ds-\la t}.}
\end{answer}

Построим так называемый \emph{процесс дробового шума}.
Пусть $f \in L_1(\R)\cap L_2(\R)$. Положим $\tau_{-n} := \tau_n'$ и~рассмотрим процесс
\eqn{\xi_t := \sums{\Z\wo\hc0} f(t-\tau_n).}
Можно показать, что
\eqn{\Mf \xi_t = \ints\R f(x)\dx,\quad \Df\xi_t = \la \ints\R f^2(x)\dx.}
Заметим, что это будет стационарный процесс, поскольку его матожидание и~дисперсия постоянны.

\subsection{Вторая теорема Колмогорова}

\subsubsection{Ещё раз непрерывности процессов. Стохастическая эквивалентность}
%\rightline{\emph{Светлой памяти Леночки Корицкой}}

Напомним определение сходимости по вероятности: говорят, что $\xi_n \convp \xi$,
если для всякого $\ep > 0$ имеем
\eqn{\Pf\br{|\xi_n-\xi| > \ep} \ra 0, \quad n \ra \bes.}

\begin{df}
Процесс называется \emph{стохастически непрерывным} в~точке $t_0$, если
\eqn{\xi_t \convp \xi_{t_0}, \quad t\ra t_0.}
\end{df}

\begin{df}
Процессы $\xi_t$ и~$\eta_t$ на одном и~том же множестве $T$ и~на одном и~том же
вероятностном пространстве $(\Om, \Fc, \Pf)$ называются \emph{стохастически эквивалентными},
если $\xi_t \eqas \eta_t$ при всех $t \in T$.
\end{df}

\begin{ex}
Пусть $T = [0,1]$, $\Om = [0,1]$, $\Pf := \la$\т мера Лебега на отрезке~$[0,1]$.
Рассмотрим два процесса: $\xi_t(\om) \equiv 0$, а~второй определим так:
\eqn{\eta_t(\om) := \Ibb_{\hc t}(\om) =\case{1,& \om = t;\\ 0,& \om \neq t.}}
Очевидно, что эти два процесса стохастически эквивалентны, поскольку при всех $t$ функция $\eta_t(\om)$
отличается от тождественного нуля только в~одной точке и~потому почти всюду по мере Лебега равна нулю.
\end{ex}


\subsubsection{Вторая теорема Колмогорова}

Наша цель\т доказать две теоремы о~существовании процесса, стохастически эквивалентного
данному, с~почти наверное непрерывными траекториями (так называемая \emph{непрерывная модификация}).


Нам потребуется несложный факт из анализа, который мы позволим себе не доказывать.

\begin{stm}
Функция, непрерывная на множестве $A$, допускает непрерывное продолжение на замыкание этого множества
тогда и~только тогда, когда она равномерно непрерывна на этом множестве.
\end{stm}

\begin{theorem}\label{thm:contModExistence}
Пусть процесс $\xi_t$ стохастически непрерывен на отрезке $T := [a,b]$ и~равномерно непрерывен почти наверное
на некотором всюду плотном множестве $D \subs T$.
Тогда он обладает непрерывной модификацией.
\end{theorem}
\begin{proof}
Перейдём от $\Om$ к~множеству полной меры $\Om'$, для которого уже верно, что если $\om \in \Om'$,
то $\xi_t(\om)$ равномерно непрерывен на~$D$. Будем строить процесс $\eta_t$, стохастически
эквивалентный процессу~$\xi_t$, у~которого траектории почти наверное непрерывны.
Проведём построение траекторий для каждого $\om \in \Om$. Если $\om \notin \Om'$, то
положим $\eta(t,\om) \stackrel{t}{\equiv} 0$. Если же $\om \in \Om'$, то, пользуясь утверждением,
можно продолжить траекторию $\xi(t,\om)$ с~множества $D$ на~множество~$T$ по непрерывности
до некоторой функции $f(t)$ и~положить $\eta(t,\om) := f(t)$.
Таким образом, у~процесса $\eta_t$ все траектории непрерывны. Осталось доказать, что
он стохастически эквивалентен процессу $\xi_t$, то есть $\xi_t \eqas \eta_t$ при всех $t \in T$.

Если $t \in D$, то доказывать нечего, поскольку в~этом случае имеется даже тождественное равенство.
Пусть теперь $t \notin D$. Рассмотрим последовательность $t_n$ точек множества $D$, сходящуюся к~$t$.
Тогда $\eta_{t_n} \convas \eta_t$ в~силу непрерывности траекторий $\eta_t$. Но из сходимости
почти наверное следует сходимость по вероятности, поэтому $\eta_{t_n} \convp \eta_t$, то есть
процесс $\eta_t$ стохастически непрерывен. С~другой стороны, по условию $\xi_{t_n} \convp \xi_t$,
но $\xi_{t_n} \eqas \eta_{t_n}$, поэтому на самом деле $\eta_t \eqas \xi_t$.
\end{proof}

\begin{theorem}\label{thm:contModExistenceReverse}
Пусть процесс $\xi_t$ на отрезке $T := [a,b]$ обладает непрерывной
модификацией $\eta_t$. Тогда он стохастически непрерывен
и~равномерно непрерывен на любом счётном подмножестве $D$ этого отрезка.
\end{theorem}
\begin{proof}
Стохастическая эквивалентность означает, что при всех $t$ имеем $\xi_t \eqas \eta_t$.
Поскольку траектории $\eta_t$ непрерывны почти наверное, то, если $s \ra t$,
то и~$\eta_s \convas \eta_t$. Докажем, что процесс $\xi_t$ стохастически непрерывен.
Как мы знаем, из сходимости почти наверное следует сходимость по вероятности. Поэтому
при всяком $\ep > 0$ имеем
\eqn{\Pf\br{|\eta_s - \eta_t| \ge \ep} \ra 0, \quad s \ra t.}
Но поскольку при всяком фиксированном $t$ имеем $\xi_t \eqas \eta_t$, при всех фиксированных
$t$ и~$s$ имеет место равенство
\eqn{\Pf\br{|\eta_s - \eta_t| \ge \ep} = \Pf\br{|\xi_s - \xi_t| \ge \ep}.}
Отсюда
\eqn{\Pf\br{|\xi_s - \xi_t| \ge \ep} \ra 0, \quad s \ra t,}
а~это и~означает, что $\xi_s \convp \xi_t$.

Докажем вторую часть теоремы. Пусть $D$\т произвольное счётное подмножество отрезка $T$.
При всяком фиксированном $t$ имеем $\xi_t \eqas \eta_t$, поэтому существует множество
$\Om_t \subs \Om$ полной меры, на котором уже $\xi_t = \eta_t$.
Пусть $\Om_\Cb$\т множество полной меры, на которой все траектории $\eta_t$ непрерывны.
Рассмотрим теперь множество
\eqn{\wt\Om := \Om_\Cb \cap \Br{\caps{t \in D} \Om_t}.}
Поскольку данное пересечение не более чем счётное, множество $\wt \Om$ тоже имеет полную меру.
Следовательно, при всех $t \in D$ и~всех $\om \in \wt\Om$ имеем $\xi_t(\om) = \eta_t(\om)$,
то есть процессы равны тождественно на $D \times \wt\Om$. Но траектории $\eta_t(\om)$ непрерывны
при $\om\in\wt\Om$, поэтому по теореме Кантора они равномерно непрерывны на отрезке $T$.
Но если есть равномерная непрерывность на отрезке, то тем более есть и~равномерная непрерывность
на любом подмножестве, в~том числе на множестве $D$. А~на этом множестве траектории совпадают с
траекториями процесса $\xi_t$, поэтому $\xi_t$ равномерно непрерывен на множестве $D$ почти наверное.
\end{proof}

\begin{theorem}[Колмогорова]
Пусть имеется процесс $\xi_t$ на отрезке $T := [a,b]$ и~существуют такие $C, \al, \be > 0$, что
при всех $s, t \in T$ имеем
\eqn{\Mf |\xi_t - \xi_s|^\al \le C|t-s|^{1+\be}.}
Тогда процесс допускает непрерывную модификацию.
\end{theorem}
\begin{proof}
Покажем, что выполнены все условия предыдущей теоремы. Стохастическая непрерывность следует
из неравенства Чебышёва:
\eqn{\Pf\br{|\xi_t - \xi_s| \ge \ep} = \Pf\br{|\xi_t - \xi_s|^\al \ge \ep^\al} \le
\frac{\Mf|\xi_t - \xi_s|^\al}{\ep^\al} \le  \frac{C|t-s|^{1 + \be}}{\ep^\al} \ra 0, \quad |t - s| \ra 0.}

Проверим теперь второе условие. Для простоты считаем, что $T = [0,1]$, так как к~этому случаю можно
всегда свести ситуацию линейной заменой. Покажем, что в~качестве множества $D$  можно взять двоично\д рациональные
точки отрезка~$T$. Разобьём этот отрезок на $2^n$ равных частей точками $r_{nk} := \frac{k}{2^n}$.
Проведём оценку вероятности при фиксированном $n$.
Пусть $\ga$\т некоторое положительное число, которое мы выберем позже.
Положим
\eqn{\De_{nk} := \hm{\xi_{r_{nk}} - \xi_{r_{n,k-1}}} \text{ и~} A_{nk} := \BC{\De_{nk} \ge \frac{1}{2^{n\ga}}}.}
По неравенству Чебышева имеем
\eqn{
\Pf(A_{nk}) = \Pf\hr{\De_{nk} \ge \frac{1}{2^{n\ga}}} =
\Pf\hr{(\De_{nk})^\al \ge \frac{1}{2^{n\ga\al}}} \le
\Mf(\De_{nk})^\al \cdot 2^{n\ga\al} \le 2^{n\ga\al} \cdot C\cdot \hr{\frac{1}{2^n}}^{1+\be}
= C\cdot 2^{n(\ga\al - (1+\be))}.}
Далее, положим
\eqn{B_n := \cupl{k=1}{2^n}A_{nk}.}
Тогда
\eqn{\Pf(B_n) \le \suml{k=1}{2^n} \Pf(A_{nk}) \le  2^n \cdot C\cdot 2^{n(\ga\al - (1+\be))} = C\cdot 2^{n(\ga\al -\be)}.}
Выберем такое $\ga$, чтобы $\ga \al - \be < 0$. Тогда ряд $\sum \Pf(B_n)$ мажорируется прогрессией
и~потому сходится. По лемме Бореля\ч Кантелли
для почти всех $\om$ существует $N(\om)$,  такое что для всех $n \ge N(\om)$ событие $B_n$
не выполняется, то есть $\De_{nk} \le \frac{1}{2^{n\ga}}$ при всех $k = 1\sco 2^n$.

\medskip

Докажем теперь равномерную непрерывность на множестве~$D$. Опишем идею.
Нам нужно оценить приращение процесса на отрезке $[s,t]$. Мы можем разбить приращение
процесса от $s$ до $t$ на более мелкие шаги и~оценить сумму приращений на каждом шаге.
Стоимость шага показательно зависит от его длины, причём сумма всех возможных стоимостей сходится.
Если шагать одинаковыми шагами, то можно заплатить слишком большой штраф за количество шагов.
Поэтому мы будем шагать по возможности \emph{разными} по длине шагами.

Назовём \emph{правильным} шаг длины $\frac1{2^n}$, у~которого начало и~конец представимы в~виде
дробей вида $\frac{k}{2^n}$. Пусть $|s - t| < \frac{1}{2^N}$, а~$s$ и~$t$\т двоично\д рациональные
точки. Нам нужно представить отрезок $[s,t]$ в~виде дизъюнктного объединения правильных шагов.

Назовём \emph{самой круглой точкой} отрезка $[s,t]$ несократимую дробь вида $\frac{k}{2^n}$ с~минимальным
знаменателем. Легко видеть, что на данном отрезке такая точка существует и~единственна. Обозначим её через~$d$.

Теперь исчерпаем отрезки $[s,d]$ и~$[d,t]$ следующим образом (если так получилось, что $d$ совпадает
с~одним из концов отрезка, ситуация только улучшится). Опишем процесс заполнения отрезка $[d,t]$,
а~отрезок $[s,d]$ заполняется аналогично.

Будем исчерпывать число $t-d$ двоичными дробями с~числителем $1$, всегда используя самый большой возможный шаг.
Ввиду существования (и~единственности) разложения всякого двоично\д рационального числа
в~сумму таких дробей, мы рано или поздно исчерпаем весь отрезок от $d$ до $t$. Заметим, что
в~этом разложении не будет двух одинаковых по длине шагов, и~все шаги будут правильными.
В~разложении отрезка $[s,d]$ тоже не будет двух одинаковых по длине шагов. Таким образом,
в~разложении всего отрезка $[s,t]$ не встретится более двух одинаковых шагов.

Осталось оценить сверху штраф. Рассмотрим самый тяжёлый случай, когда придётся просуммировать все мыслимые
шаги, начиная с~некоторого шага длины $\frac{1}{2^m}$, где $m \ge N$.
Тогда штраф не превысит удвоенной суммы прогрессии:
\eqn{|\xi_s - \xi_t| \le 2 \suml{n=m}{\bes}\frac{1}{2^{n\ga}} =
\frac{1}{2^{m\ga}}\cdot 2 \sumnzi \hr{\frac{1}{2^\ga}}^n =: \hr{\frac{1}{2^m}}^\ga\cdot M.}
Пусть $n$ таково, что $\frac{1}{2^n} \le|s - t| < \frac{1}{2^{n-1}}$.
Тогда самый большой шаг, который можно сделать, равен $\frac{1}{2^n}$ или $\frac{1}{2^{n+1}}$.
Положим $m := n$, тогда
\eqn{|\xi_s - \xi_t| \le \hr{\frac{1}{2^n}}^\ga \cdot M \le M \cdot |s-t|^\ga.}
Но это и~означает равномерную непрерывность. Ура!
\end{proof}

Доказав теорему, можно собрать урожай: мы будем более подробно исследовать винеровский процесс.

\subsection{Винеровский процесс}

\subsubsection{Построение непрерывного винеровского процесса на полупрямой}

Рассмотрим винеровский процесс на отрезке $[0,1]$ и~применим к~нему теорему Колмогорова.
В~силу свойств винеровского процесса, имеем
\eqn{\eta := \frac{W_t - W_s}{\sqrt{t-s}} \sim \Nc(0,1),}
поэтому $\Mf|W_t - W_s|^4 = \Mf |\eta|^4 \cdot (t-s)^2 = 3 (t-s)^2$,
так как $\Mf|\eta|^4 = 3$. Итак, в~теореме Колмогорова достаточно взять $\al = 4$,
$\be = 1$, а~$C = 3$. Следовательно, винеровский процесс обладает
непрерывной модификацией.


Зная, что у~винеровского процесса на отрезке существует непрерывная модификация на отрезке,
построим непрерывный винеровский процесс на всей прямой. Пусть $W^{(i)}_t$\т
непрерывная модификация винеровского процесса на отрезке $[i, i+1]$. Пусть все $W^{(i)}$ независимы.
Тот факт, что можно построить счетное число независимых винеровских процессов, следует
из теоремы Колмогорова. Для этого нужно взять
\eqn{\cov\br{W^{(i)}_s, W^{(j)}_t} := \de_{ij} \min(s,t).}

Теперь построим процесс $W_t$ так:
\eqn{
W_t:=\case{W^{(0)}_t,                & t \in [0, 1];\\
          W^{(i-1)}_1+W^{(i)}_{t-i}, & t \in [i, i+1].}}
То, что построенный процесс непрерывен, ясно: мы брали непрерывные модификации~$W^{(i)}$.
Покажем, что это винеровский процесс. Воспользуемся первым определением.
Процесс гауссовский, потому что  $(W_{t_1}\sco W_{t_n})$\т объединение нескольких
гауссовских векторов,  соответствующих процессам $W^{(i)}$, независимых между собой.
Очевидно, что это тоже гауссовский вектор. Осталось посчитать ковариационную функцию.
Если~$s$ и~$t$ попали в~один отрезок, то результат сразу получается из того, что на
этом отрезке процесс винеровский. Пусть, для определённости, $s<t$ и~пусть
$s \in [i, i+1]$, а~$t\in [j, j+1]$. Тогда
\eqn{\cov(W_s, W_t)=\cov\br{W_s, W_j+W^{(j)}_{t-j}} = \cov(W_s, W_j)+\cov\br{W_s, W^{(j)}_{t-j}}.}
Второе слагаемое в~правой части равно нулю, так как процессы $W^{(i)}$ и~$W^{(j)}$
независимы при $i\neq j$. Осталось разобраться с~первым слагаемым:
\eqn{\cov(W_s,W_j)=\cov\br{W_s, W_{j-1}+W^{(j-1)}_1}=\ldots=\cov(W_s, W_{i+1})=s.}
При $t > s$ аналогично получаем $t$ вместо $s$. Итак, ковариационная функция нашего процесса $W_t$\т это $\min(s,t)$,
что и~требовалось доказать.

\subsubsection{Асимптотика траекторий винеровского процесса: оценка сверху}
\begin{theorem}
Пусть $W_t$\т винеровский процесс, имеющий почти наверное непрерывные траектории. Тогда
\eqn{\liml{t \ra \bes} \frac{W_t}{t} \eqas 0.}
\end{theorem}
\begin{proof}
Рассмотрим процесс
\eqn{\xi_t := \case{t \cdot W_{1/t},&t\neq 0,\\0, &t=0.}}
Покажем, что процесс $\xi_t$ гауссовский и~имеет ковариационную функцию $K_\xi(s,t) = \min(s,t)$.
Действительно, если вектор $(W_{t_1}\sco W_{t_n})$ гауссовский при любых $t_1\sco t_n$, то и~вектор
$\hr{t_1 W_{1/t_1}\sco t_n W_{1/t_n}}$\т тоже гауссовский, так как получается из гауссовского
линейной (даже диагональной) заменой. Значит, конечномерные распределения у~$\xi_t$ гауссовские.
Найдём ковариационную функцию. Пусть $s,t>0$. Тогда
\mln{
K_\xi(s,t) = \Mf(\xi_s\cdot \xi_t) = \Mf\hr{s \cdot W_{1/s}\cdot t \cdot W_{1/t}} =
st \cdot \Mf\hr{W_{1/s}\cdot W_{1/t}} = \\=
st \cdot K_W\hr{\frac1s,\frac1t} =
st \cdot \min\hr{\frac1s,\frac1t} =
\frac{st}{\max(s,t)} = \min(s,t),}
что и~требовалось. Очевидно, что $\Mf\xi_t \equiv 0$. Итак,
мы доказали, что процесс~$\xi_t$ является винеровским. Следовательно, для него имеет место
теорема Колмогорова и~у него существует непрерывная модификация~$\eta_t$. Кроме того,
очевидно, что $\xi_t$ непрерывен почти наверное при всех $t$, кроме, быть может, точки $t = 0$.

Множество тех $\om$, для которых траектория $\xi_t(\om)$ непрерывна на $(0,1]$, обозначим через $\Om_\xi$.
Это множество полной меры. Аналогично, через $\Om_\eta$ обозначим множество полной меры, на котором
непрерывны везде траектории $\eta_t$. Поскольку $\xi_t \sim \eta_t$, а~$\xi_0 = 0$ по определению,
получаем, что существует множество $\Om_0$ полной меры, на котором $\xi_0 = \eta_0 = 0$.
Далее, пусть $D := (0,1] \cap \Q$. Положим $\Om_q := \hc{\om \cln \xi_q(\om) = \eta_q(\om)}$.
В~силу стохастической эквивалентности процессов, множества $\Om_q$ тоже имеют полную меру. Наконец, рассмотрим
множество полной меры $\wt\Om$, на котором выполнены все нужные нам свойства, а~именно, положим
\eqn{\wt\Om := \Om_\xi \cap \Om_\eta \cap \Om_0 \cap \Br{\caps{q \in D} \Om_q}.}

Далее все события $\om$ будем брать только из $\wt\Om$. Поскольку $\eta_0 = 0$ и~непрерывен на отрезке,
$\eta_t \ra \eta_0 = 0$ при $t \ra 0$. Но на всех рациональных точках $q \in [0,1]$ процессы $\xi_q$ и~$\eta_q$
совпадают, поэтому $\xi_q$ тоже стремится к~нулю по рациональным точкам. Но все его траектории и
так были непрерывными везде, кроме, быть может, нуля, поэтому на самом деле он стремится к~нулю по всем точкам.
Таким образом, почти наверное
\eqn{\xi_t = t \cdot W_{1/t} \ra 0, \quad t \ra 0.}
Делая замену переменной $s = \frac{1}{t}$, получаем утверждение теоремы.
\end{proof}

Таким образом, траектории винеровского процесса почти наверное
лежат в~конусе $|y| \le t$.

\subsubsection{Неограниченность вариации винеровских траекторий}

Мы будем предполагать, что рассматриваемый винеровский процесс~$W_t$ имеет почти наверное
непрерывные траектории.

Пусть $T = [a,b]$ и~$P$\т разбиение $T$ точками $a = t_0<\ldots<t_n = b$.
Рассмотрим случайную величину
\eqn{S_P = \sumiun (W_{t_i} - W_{t_{i-1}})^2.}
Имеем
\eqn{\Mf S_P = \sumiun (t_i - t_{i-1}) = |T|.}

Покажем, что $S_P \ra |T|$  в~$L_2$, то есть $\Mf(S_P - |T|)^2 \ra 0$.
Для краткости положим $\De_i := W_{t_i} - W_{t_{i-1}}$ и~$d_i := t_i - t_{i-1}$.
Распишем второй момент для частичной суммы:
\mln{\Mf S_P^2 = \Mf \Br{\sums{i} \De_i^2}^2 = \sums{i} \Mf \De_i^4 + 2\sums{i < j}\Mf \De_i^2 \Mf \De_j^2 =
\sums{i} 3d_i^2 + 2\sums{i < j} d_i d_j  =\\=
2 \sums{i} d_i^2 + \sums{i} d_i^2 + 2\sums{i < j} d_i d_j = 2\sums{i} d_i^2 + \Br{\sums{i} d_i}^2 =
2\sums{i} d_i^2 + |T|^2.}
Далее, поскольку для всяких чисел $\hc{a_i}$ имеет
место очевидное неравенство $\sum a_i^2 \le \max |a_i| \cdot \sum |a_i|$,
получаем
\eqn{\Mf(S_P - |T|)^2 = \Df S_P^2  = \Mf S_P^2 - |T|^2 =
2\sums{i} d_i^2 \le 2\la(P)\sums{i} d_i = 2|T|\la(P) \ra 0, \quad \la(P) \ra 0,}
что и~требовалось показать.

\begin{stm}
Почти наверное вариация винеровской траектории на любом отрезке~$T$ неограничена:
\eqn{\suml{i=1}{n}|W_{t_i}-W_{t_{i-1}}| \ra \bes, \quad \la(P) \ra 0.}
\end{stm}
\begin{proof}
Применим доказанное выше утверждение: $S_P \ra |T|$  в~$L_2$ при $\la(P)\ra 0$. Из сходимости в~$L_2$,
конечно, не следует сходимость почти всюду, зато следует сходимость по мере (неравенство Чебышёва).
По теореме Рисса, из сходящейся по мере последовательности можно выделить подпоследовательность
разбиений $\hc{P_k}$, для которых $S_{P_k} \ra |T| > 0$ почти наверное.
Отсюда уже следует, что
\eqn{\suml{i=1}{n_k} |\De_i^{(k)}| \ra \bes, \quad k \ra \bes,}
потому что если бы такая последовательность была ограничена, то
\eqn{\suml{i=1}{n_k} \br{\De_i^{(k)}}^2  \le \maxl{i} |\De_i^{(k)}| \cdot
\suml{i=1}{n_k} |\De_i^{(k)}| \ra 0, \quad k \ra\bes,}
за счёт малости первого множителя. Полученная оценка означает, что вариация не является ограниченной.
\end{proof}
\begin{imp}
Винеровская траектория почти наверное не принадлежит классу $\Cb^1$.
\end{imp}
\begin{proof}
В~самом деле, на всяком отрезке $\Cb^1$\д функции всегда имеют ограниченную вариацию.
\end{proof}

На самом деле справедливо более сильное утверждение (но его мы доказывать не будем).
\begin{theorem}
Почти наверное винеровская траектория не дифференцируема ни в~одной точке.
\end{theorem}

\subsubsection{Асимптотика траекторий винеровского процесса: оценка снизу}

Выше было показано, что траектория винеровского процесса <<разбегаются>> не слишком быстро,
а~именно растут (по модулю) не быстрее, чем растёт $t$. Сейчас мы покажем, что вместе с~тем они
осциллируют достаточно сильно. Но сначала мы напомним одну из важнейших теорем из теории вероятностей.

\begin{theorem}[Закон 0 и~1 Колмогорова]
Пусть величины $\eta_n$ независимы. Рассмотрим $\si$\д алгебру <<хвостов>>, то есть
$\Fc_{\ge n} := \si(\eta_k \vl k \ge n)$.
Пусть $A \in \caps{n} \Fc_{\ge n}$. Тогда $\Pf(A) = 0$ или $\Pf(A)=1$.
\end{theorem}

\begin{theorem}
Почти наверное для траектории винеровского процесса справедливо свойство
\eqn{\uliml{t\ra\bes}\frac{W_t}{\sqrt t} = \bes, \qquad \uliml{n\ra\bes}\frac{W_n}{\sqrt n} = \bes.}
\end{theorem}
\begin{proof}
Докажем второе соотношение (первое, очевидно, из него следует). Пусть $C > 0$\т произвольное фиксированное число.
Рассмотрим событие
\eqn{A_C := \hc{\uliml{n\ra\bes}\frac{W_n}{\sqrt n}\le C}}
и~покажем, что $\Pf(A_C) = 0$. Фиксируем произвольное число $k\in \N$. Очевидно,
\eqn{A_C = \hc{\uliml{n\ra\bes}\frac{W_n -W_k}{\sqrt n}\le C},}
так как последовательности $\frac{W_n -W_k}{\sqrt n}$ и~$\frac{W_n}{\sqrt n}$ имеют один и~тот же предел (ибо $k$ фиксировано и~на сходимость не повлияет). А~теперь сделаем ещё одно тождественное преобразование:
\eqn{A_C = \hc{\uliml{n\ra\bes}\frac{(W_{k+1} -W_k)\spl(W_n-W_{n-1})}{\sqrt n}\le C}.}
Покажем, что это событие принадлежит $\si$\д алгебре $\Fc_{\ge k}$, где $\Fc_{\ge k}$ порождена
величинами $\eta_n := W_{n+1}-W_n$. Действительно, выражение под пределом есть $\frac{\eta_k\spl \eta_n}{\sqrt n}$,
а~каждое из слагаемых $\frac{\eta_i}{\sqrt n}$ измеримо относительно~$\Fc_{\ge i}$. Верхний предел событий есть
событие, поэтому
\eqn{A_C \in \caps{n}\Fc_{\ge n}.}
По закону нуля и~единицы $\Pf(A_C)=0$ или $\Pf(A_C)=1$.
Остаётся доказать, что второй случай невозможен.

Предположим, что $\uliml{n\ra\bes} \frac{W_n}{\sqrt n} \le C$ почти наверное. Тогда найдётся $n$ такое, что
$\frac{W_n}{\sqrt n} \le C+1$. Рассмотрим события
\eqn{F_n := \hc{\frac{W_m}{\sqrt m} \le C+1 \vl m \ge n}.}
Они вложены друг в~друга, и~$F_n\nearrow F := \bigcup F_n$. Очевидно, $A_C \subs F$. Если $A_C$ имеет полную меру,
то $F$ тем более имеет полную меру и~$1= \Pf(F) = \lim \Pf(F_n)$. С~другой стороны,
\eqn{\Pf(F_n)\le \Pf\hc{\frac{W_n}{\sqrt n} \le C+1}.}
Случайная величина $\xi := \frac{W_n}{\sqrt n}$ имеет нормальное распределение, а~$C <\bes$, поэтому
$\Pf(\xi \le C+1)< 1$. Значит, $\lim \Pf(F_n)< 1$. Противоречие. Значит, $\Pf(A_C) = 0$.
\end{proof}

Совершенно аналогично, заменив $W_t$ на $-W_t$, можно показать, что $\lliml{n\ra\bes}\frac{W_t}{\sqrt t} =-\bes$.

Естественно задаться вопросом: а~какова же всё\д таки асимптотика траекторий винеровского процесса?
Мы сформулируем ответ, а~доказательство можно прочесть в~\cite{bsh}.
\begin{theorem}[Закон повторного логарифма]
\eqn{\uliml{t\ra\bes} \frac{W_t}{\sqrt {2t\ln\ln t}} \eqas 1.}
\end{theorem}

\section{Марковские процессы}

\subsection{Марковские моменты. Мартингалы}

\subsubsection{Определение марковского момента}

\begin{df}
\emph{Потоком $\si$\д алгебр} на вероятностном пространстве $(\Om,\Fc, \Pf)$ называется такое семейство
$\si$\д алгебр $\Fc_t \subs \Fc$, что $\Fc_t \subs \Fc_s$ при $t < s$.
\end{df}

\begin{df}
Случайная  величина $\tau\cln\Om\ra \ol \R_+$ называется \emph{марковским моментом} относительно потока $\hc{\Fc_t}$,
если событие $\hc{\tau\le t} \in \Fc_t$ при всех $t$.
\end{df}

Через $\Fc_{\le t}(\xi)$ будем обозначать поток, порождённый процессом $\xi_t$:
\eqn{\Fc_{\le t}(\xi) := \si(\xi_s \vl s \le t).}
В~дальнейшем мы, как правило, будем рассматривать только потоки, порождённые процессами, поэтому будем писать
просто <<$\Fc_{\le t}$>>, если ясно, о~каком процессе идёт речь.

\begin{ex}
Пусть $\xi_t$\т непрерывный процесс, и~$\xi_0=0$. Положим $\tau_a := \min\hc{t\cln \xi_t =a}$, и~$\tau_a := \bes$,
если данное множество пусто. Это будет марковский момент, так как
\eqn{\hc{\tau_a\le t} = \hc{\exi s \le t\cln \xi_s = a} = \hc{\fa n \exi r<t\cln \xi_r > a -\frac1n} = \caps{n}\cups{r< t}\ub{\hc{\xi_r > a -\frac1n}}_A.}
Очевидно, можно брать только рациональные значения $r$, поэтому объединение можно считать счётным.
Поэтому событие $A$ лежит в~$\Fc_{\le r} \subs \Fc_{\le t}$.
\end{ex}

\begin{df}
Пусть $\tau$\т марковский момент. Он называется \emph{моментом остановки}, если $\tau <\bes$ почти наверное.
\end{df}


\subsubsection{Принцип отражения для винеровского процесса}

Пусть $W_t$\т винеровский процесс. Пусть $a > 0$.
Заметим, что $\Pf(W_t < a \text{ для всех } t) = 0$, поскольку $\uliml{t} \frac{W_t}{\sqrt t} = \bes$.
Отсюда следует, что марковский момент $\tau_a$, определённый в~примере выше, является моментом остановки.

\begin{theorem}[Принцип отражения]
Пусть $\tau$\т момент остановки для $W_t$. Тогда при всех $t$ имеем
\eqn{\Pf(\tau \le t, W_t > W_\tau) = \Pf(\tau \le t, W_t < W_\tau).}
Иначе говоря, винеровской траектории всё равно, куда идти в~следующий момент времени\т вверх или вниз.
\end{theorem}
\begin{proof}
Мы докажем эту теорему в~двух случаях: а) $\tau$ дискретна; б) величины $(W_t - W_\tau)$ и~$\tau$
имеют непрерывные распределения.

\smallskip

Доказательство для случая \textbf{а)}. Пусть $\Im\tau = \hc{t_1\sco t_n\etc}$. Тогда
\eqn{\Pf(\tau \le t, W_t > W_\tau) = \sums{k\cln t_k \le t} \Pf(\tau =t_k, W_t > W_{t_k}).}
Имеем $\hc{\tau = t_k} \in \Fc_{\le t_k}$, потому что
\eqn{\hc{\tau = t_k} = \hc{\tau \le t_k} \wo \cups{t_i < t_k} \hc{\tau \le t_i} \in \Fc_{\le t_k}.}
Далее, имеем
\eqn{\Fc_{\le t_k} \eqdef  \si(W_s \vl s \le t_k)  = \si(W_\be-W_\al \vl \al < \be \le t_k).}
В~самом деле, включение <<$\subs$>> очевидно (достаточно положить $\al=0$), а~обратное доказывается
по той же схеме, что и~измеримость суммы измеримых функций.

Далее, события $\hc{\tau= t_k}$ и~$\hc{W_t - W_{t_k} > 0}$ независимы, потому что первое
из них лежит в~$\Fc_{\le t_k}$, а~приращение винеровского процесса на отрезке, не пересекающемся
с~$[0,t_k)$, не зависит от $\Fc_{\le t_k}$. Далее, величина $W_t-W_{t_k}$ имеет
центрированное нормальное распределение, поэтому
$\Pf(W_t > W_{t_k}) =\frac12$. Значит,
\eqn{\label{eqn:mirrorPrinciple}\Pf(W_t > W_\tau\z \tau \le t) = \Pf(W_t - W_\tau>0) \cdot \Pf(\tau \le t) = \frac12\Pf(\tau\le t).}

\smallskip

Доказательство для случая  \textbf{б)}. Основная идея\т приблизить $\tau$ дискретными величинами и~перейти к~пределу.
Разобьём отрезок $[0,t]$ на $2^n$ частей и~положим
\eqn{\tau_n := \frac{k t}{2^n} \text{ при } \hc{\frac{(k-1) t}{2^n} < \tau \le \frac{k t}{2^n}}.}
Покажем, что $\tau_n$ является моментом остановки. Положим
\eqn{C(s) := \hs{\frac{s}{t}\cdot 2^n}\cdot \frac{t}{2^n}.}
Заметим, что $C(s)\le s$ и~оно просто является округлением $s$ вниз к~ближайшему узлу $\frac{kt}{2^n}$).
Тогда, очевидно, $\hc{\tau_n \le s} = \hc{\tau_n \le C(s)}$. Остаётся заметить, что в~силу выбора $\tau_n$,
если выполнено $\tau_n\le C(s)$, то выполнено и~$\tau < C(s)$ (и~наоборот).
Значит,
\eqn{\hc{\tau_n \le s} = \hc{\tau_n \le C(s)} = \bc{\ub{\tau < C(s)}_{\in \Fc_{\le s}}},}
и~тем самым проверено, что моменты $\tau_n$\т марковские. Остаётся заметить, что $\tau_n \rra \tau$.
Значит, $W_{\tau_n}\convas W_\tau$ и~$\tau_n\convas \tau$. Из сходимости почти наверное следует сходимость
по распределению, а~в~силу непрерывности распределений
\eqn{\Pf(\tau_n\le t , W_t>W_{\tau_n}) \ra \Pf(\tau \le t, W_t >W_\tau), \quad n \ra \bes.}
К~величинам $\tau_n$ применим о~утверждение теоремы для дискретного случая, а~в~силу этой сходимости оно
верно и~в~непрерывном случае.
\end{proof}
\begin{imp}
Пусть $\tau_a$\т момент достижения уровня $a$ процессом $W_t$. Тогда
\eqn{\Pf(\tau_a \le t) = 2\Pf(W_t > a).}
\end{imp}
\begin{proof}
Применим теорему к~моменту $\tau_a$, а~точнее, подставим $\tau_a$ в~ формулу \eqref{eqn:mirrorPrinciple}.
Имеем
\eqn{\Pf(\tau_a \le t, W_t > W_{\tau_a}) = \frac12\Pf(\tau_a \le t).}
Будем считать, что траектории $W_t$ почти наверное непрерывны. Если $W_t > W_{\tau_a}$, то по
определению $\tau_a$ имеем $t \ge \tau_a$ (\те одно событие содержит другое). Поэтому
\eqn{\Pf(\tau_a \le t, W_t > W_{\tau_a}) = \Pf(W_t > W_{\tau_a}).}
Кроме того, $W_{\tau_a} \eqas a$, поэтому $\Pf(\tau_a \le t) =2\Pf(W_t > a)$.
\end{proof}

\begin{imp}
Момент $\tau_a$ имеет плотность распределения
\eqn{p_\tau(t) = \frac{a}{t^{3/2}} \frac{1}{\sqrt{2\pi}} \exp\hr{-\frac{a^2}{2t}}, \quad t > 0.}
\end{imp}
\begin{proof}
По предыдущему следствию его функция распределения при $t>0$ равна $2\hr{1-\Ph\hr{\frac{a}{\sqrt t}}}$,
где $\Ph$\т функция стандартного нормального распределения.
Дифференцируя её, получаем эту самую формулу.
\end{proof}

\begin{imp}
Функция распределения максимума винеровского процесса $m_t := \maxl{[0,t]} W_s$ имеет плотность
\eqn{p_{m_t} := \sqrt{\frac2\pi} \cdot \frac1t \cdot \exp\hr{-\frac{x^2}{2t}}.}
\end{imp}
\begin{proof}
Очевидно, что $\Pf(m_t \ge x) = \Pf(\tau_x \le t)$. Отсюда
\eqn{F_{m_t} = \Pf(m_t < x) = 1-\Pf(\tau_x \le t) = 1-2\br{1-F_{W_t}(x)} = 2F_{W_t}-1.}
Как и~в~предыдущем рассуждении, находим плотность путём дифференцирования полученной формулы.
\end{proof}

\subsubsection{Мартингалы}

\begin{df}
Процесс $\xi_t$ называется \emph{мартингалом} относительно потока $\si$\д алгебр $\hc{\Fc_t}$, если $\Mf(\xi_t|\Fc_s) = \xi_s$ для всех $s \le t$.
\end{df}

В~силу свойств условного матожидания условие мартингальности можно записать так: $\Mf(\xi_t-\xi_s|\Fc_s)=0$.

Рассмотрим процесс с~независимыми приращениями, то есть $\Fc_t \indep (\xi_s - \xi_t)$ при всех $s > t$.
Тогда, если $\Mf \xi_t \bw = \const$, то $\Mf(\xi_t-\xi_s|\Fc_s) =\Mf\xi_t -\Mf \xi_s = 0$.

\begin{problem}
Доказать, что процесс Орнштейна\ч Уленбека не является мартингалом.
\end{problem}

\subsection{Марковские процессы}

\subsubsection{Шесть эквивалентных определений марковского процесса}

\hbox to \textwidth{\hfil\hbox{\vbox{\hsize=9cm\footnotesize
\noindent --- Продаём шесть определений марковского процесса за три копейки,
    целых шесть определений --- три копейки\dots\\
--- А~что так дёшево?\\
--- А~всё правильно: каждому определению --- грош цена.
\smallskip

\rightline{\emph{Из недавнего прошлого}}
}}}
\medskip

В~качестве потока $\si$\д алгебр всегда будем брать поток, порождённый процессом.

Напомним, что \emph{условное распределение} определяется так:
\eqn{\Pf(A\vl \Cc) := \Mf(\Ibb_A \vl \Cc).}

\begin{dfn}{1}
Процесс  $\xi_t$ называется \emph{марковским процессом}, если при всех $t_1\le\ldots\le t_n \le t$ имеем
\eqn{\Pf(\xi_t \in A \vl \xi_{t_1}\sco \xi_{t_n}) = \Pf(\xi_t \in A \vl \xi_{t_n}).}
\end{dfn}

Неформально говоря, марковость процесса означает, что для предсказания будущего
прошлое не важно.

\begin{dfn}{2}
Процесс  $\xi_t$ называется \emph{марковским процессом}, если при всех $t_1\le\ldots\le t_n \le t$ и~любой
ограниченной измеримой функции $f$ имеем
\eqn{\Mf(f(\xi_t)\vl \xi_{t_1}\sco \xi_{t_n}) = \Mf(f(\xi_t)\vl \xi_{t_n}).}
\end{dfn}

Из второго определения сразу следует первое: достаточно положить $f := \Ibb_A$.
В~обратную сторону доказательство проходит по стандартной схеме: по линейности
от индикаторов переходим к~ступенчатым функциям, а~затем равномерно приближаем ступенчатыми
произвольную ограниченную функцию $f$ и~переходим к~пределу.

\begin{dfn}{3}
Процесс  $\xi_t$ называется \emph{марковским процессом}, если для любой
ограниченной измеримой функции $f$ имеем
\eqn{\Mf(f(\xi_t)\vl \Fc_{\le s}) = \Mf(f(\xi_t)\vl \xi_s) =: \eta.}
\end{dfn}

Докажем эквивалентность определений~\textbf{(3)} и~\textbf{(2)}.
В~доказательстве нуждается только импликация \textbf{(2)}$\Ra$\textbf{(3)},
обратное очевидно (частный случай).

Ясно, что величина $\eta$ является $\Fc_{\le s}$\д измеримой, поскольку она даже
$\si(\xi_s)$\д измерима. Покажем, что если $F \bw\in \Fc_{\le s}$, то
$\Mf(\Ibb_F\cdot \eta) = \Mf(\Ibb_F \cdot f(\xi_t))$, тогда требуемое будет
следовать уже из определения условного математического ожидания.
Действительно, если $F \in \si(\xi_{t_1}\sco \xi_{t_n})$, то это выполнено по условию.
Рассмотрим множество
\eqn{\Fc^0_{\le s} := \cups{t_i < s} \si(\xi_{t_1}\sco \xi_{t_n}).}
Это, вообще говоря, не $\si$\д алгебра, но $\si(\Fc^0_{\le s}) = \Fc_{\le s}$,
поэтому доказываемое справедливо для всех порождающих $\Fc_{\le s}$, а~потому
верно и~для всей $\si$\д алгебры.


Есть ещё несколько определений марковского процесса. Для полноты картины сформулируем и~их тоже.
\begin{dfn}{4}
Процесс  $\xi_t$ называется \emph{марковским процессом}, если для любой
ограниченной измеримой функции $f$ имеем
\eqn{\Mf\Br{\prod f(\xi_{t_i})\vl \Fc_{\le s}} = \Mf\Br{\prod f(\xi_{t_i})\vl \xi_s}.}
\end{dfn}

\begin{dfn}{5}
Процесс  $\xi_t$ называется \emph{марковским процессом}, если при всех $F \in \Fc_{\ge s}$ имеем
\eqn{\Pf(F\vl \Fc_{\le s}) = \Pf(F\vl \xi_s).}
\end{dfn}

\begin{dfn}{6}
Процесс  $\xi_t$ называется \emph{марковским процессом}, если при всех $F_- \in \Fc_{\le s}$
и~$F_+ \in \Fc_{\ge s}$ имеем
\eqn{\Pf(F_-F_+ \vl \xi_s) = \Pf(F_-\vl \xi_s)\cdot \Pf(F_+\vl \xi_s).}
\end{dfn}

\subsubsection{Примеры марковских процессов}

Как показывает следующее утверждение, мы уже знаем один пример марковского процесса.

\begin{stm}
Всякий процесс с~независимыми приращениями является марковским процессом.
\end{stm}
\begin{proof}
Будем доказывать, используя третье определение. Рассмотрим $f(x) := e^{i\la x}$.
Для левой части определения имеем при $s < t$
\mln{\Mf\br{\exp\br{i\la \xi_t}\vl \Fc_{\le s}} =
\Mf\br{\ub{\exp\br{i\la\xi_s}}_{\Fc_{\le s}\text{-измерима}} \cdot \exp\br{i\la(\xi_t-\xi_s)}\vl \Fc_{\le s}}=
\exp\br{i\la\xi_s}\cdot \Mf\br{\ub{\exp\br{i\la(\xi_t-\xi_s)}}_{\text{не зависит от }\Fc_{\le s}}\vl \Fc_{\le s}}=\\=
\exp\br{i\la\xi_s} \cdot\Mf\br{\exp\br{i\la(\xi_t-\xi_s)}}.}
Но к~абсолютно такому же виду можно привести правую часть определения,
воспользовавшись измеримостью $\exp(i\la\xi_s)$ относительно~$\si(\xi_s)$.
Осталось заметить, что всякую функцию можно хорошо приблизить экспонентами, поэтому для
них это тоже верно.
\end{proof}

Таким образом, винеровский процесс автоматически является марковским.

\begin{problem}
Доказать, что процесс Орнштейна\ч Уленбека не является процессом с~независимыми
приращениями, однако является марковским.
\end{problem}

\begin{note}
Можно показать, что процесс Орнштейна\ч Уленбека\т единственный (с~точностью до пропорциональности)
стационарный центрированный марковский гауссовский процесс.
\end{note}

\subsection{Марковские цепи}

Важным классом марковских процессов являются процессы, у~которых
все случайные величины $\xi_t$ имеют одну и~ту же область значений $X$,
которая в~этом случае называется \emph{фазовым пространством}, или
\emph{пространством состояний}.
Именно такие процессы мы и~будем изучать.

Будем считать, что $T = [0,\bes)$.

\begin{df}
\emph{Цепью Маркова} называется марковский процесс, у~которого фазовое пространство~$X$
дискретно (то есть не более чем счётно). Естественно,  что в~качестве $\si$\д алгебры
на~$X$ выбирается дискретная $\si$\д алгебра $2^X$.
Для простоты мы будем считать, что $X$\т это множество~$\Z_+$.
\end{df}

\subsubsection{Переходные вероятности марковского процесса}

\begin{df}
Пусть $\xi_t$\т марковская цепь. \emph{Переходными вероятностями}
называются функции
\eqn{p_{ij}(s,t) = \Pf(\xi_t = j\vl\xi_s = i), \quad s \le t, \quad i, j \in X.}
\end{df}

В~дальнейшем мы не будем писать, что $s \le t$, подразумевая это. Заметим, что
это довольно естественно: система движется <<вперёд>>, но не <<назад>>.

Из этого определения сразу следуют три свойства:
\begin{points}{-2}
\item $p_{ij}(s,t) \ge 0$.
\item $\sums{j \in X} p_{ij}(s,t) = 1$ при всех $i \in X$.
\item $p_{ij}(s,s) = \de_{ij}$.
\end{points}

Следующее свойство чуть менее тривиально и~очень важно. По внешнему виду
оно напоминает правило перемножения матриц. Если фазовое пространство~$X$ конечно,
то так оно и~есть.

\begin{theorem}[Уравнение Маркова]
\eqn{p_{ij}(s,t) = \sums{k \in X} p_{ik}(s,u)\cdot p_{kj}(u,t).}
\end{theorem}
\begin{proof}
Нещадно эксплуатируя определение условной вероятности, пишем:
\eqn{
\begin{aligned}
p_{ij}(s,t) &= \Pf(\xi_s = j\vl \xi_s = i) = \frac{\Pf(\xi_t = j\z \xi_s = i)}{\Pf(\xi_s = i)} =\\
            &=\sums{k} \frac{\Pf(\xi_t = j\z \xi_s = i\z \xi_u = k)}{\Pf(\xi_s = i)}=\\
            &=\compr{-48.5mu}{\sums{k\cln \Pf(\xi_u = k\z \xi_s = i) \neq 0}} \Pf(\xi_t = j \vl \xi_u = k\z \xi_s = i)
              \cdot \frac{\Pf(\xi_u = k\z \xi_s = i)}{\Pf(\xi_s = i)}=\\
            &=\sums{k} \Pf(\xi_t = j \vl \xi_u = k)
              \cdot \Pf(\xi_u = k\vl \xi_s = i)=\\
            &=\sums{k} p_{ik}(t,u)\cdot p_{kj}(u,s),
\end{aligned}}
что и~требовалось доказать.
\end{proof}

\begin{df}
\emph{Начальным распределением} марковской цепи называют меру $\Law(\xi_0)$.
\end{df}

Совершенно ясно, что задать начальное распределение\т это всё равно что
задать набор чисел
\eqn{p_i(0) := \Pf(\xi_0 = i).}

\subsubsection{Конечномерные распределения марковской цепи}

\begin{theorem}
Конечномерные распределения марковской цепи $\xi_t$ однозначно определяются
переходными вероятностями и~начальным распределением.
\end{theorem}
\begin{proof}
Пусть $0 \le t_1 <\ldots t_n$. Положим $A_{n-1} := \hc{\xi_{t_1} = j_1\sco \xi_{t_{n-1}} = j_{n-1}}$.
Имеем
\mln{\Pf(\xi_{t_1} = j_1\sco \xi_{t_n} = j_n) = \Pf(\xi_{t_n} = j_n \vl A_{n-1})\cdot \Pf(A_{n-1}) =\\=
\Pf(\xi_{t_n} = j_n \vl \xi_{t_{n-1}} = j_{n-1}) \cdot \Pf(A_{n-1}) =
p_{j_{n-1}, j_n}(t_{n-1},t_n) \cdot \Pf(A_{n-1}).}
Таким образом, мы научились выражать $n$\д мерное распределение через $(n-1)$\д мерное
распределение и~переходную вероятность. Рассуждая по индукции, получаем формулу
\eqn{\Pf(\xi_{t_1} = j_1\sco \xi_{t_n} = j_n) = \Pf(\xi_{t_1} = j_1) \cdot p_{j_1, j_2}(t_1, t_2)\sd p_{j_{n-1}, j_n}(t_{n-1},t_n).}
Осталось выразить через начально распределение вероятность $\Pf(\xi_{t_1} = j_1)$. Это можно сделать
по формуле полной вероятности:
\eqn{\Pf(\xi_{t_1} = j_1) = \sums{i} p_i(0) p_{ij_1}(0,t_1).}
Механическую работу по подстановке одной формулы в~другую мы оставляем читателю.
\end{proof}

\subsubsection{Переходная функция марковского процесса}

Пусть $(X, \Ac)$\т измеримое пространство.

Чтобы сохранять естественный порядок аргументов, мы захотим писать дифференциал в~интеграле \emph{перед}
интегрируемой функцией. То есть примерно так: $\int \mu(dx) f(x)$.

\begin{df}
Функция $P(s,x,t,A)$, заданная для $s \le t \in T$, $x \in X$, а~$A \in \Ac$,
называется \emph{переходной функцией}, если
\begin{points}{-2}
\item При фиксированных $s,t,x$ функция $P(s,x,t,\cdot)$ является мерой на $(X, \Ac)$.
\item При фиксированных $s,t,A$ функция $P(s,\cdot,t,A)$ является $(\Ac,\Bs)$\д измеримой.
\item $P(s,x,s,A) = \de_x(A) = \case{1, & x \in A,\\ 0, & x \notin A.}$
\item Выполнено уравнение Колмогорова\ч Чепмена при всех $s < u < t$:
\eqn{\label{eqn:KolmogoroffChepman}P(s,x,t,A) = \ints{X} P(s,x,u,dy) P(u,y,t,A).}
\end{points}
\end{df}

\begin{df}
Говорят, что марковский процесс \emph{обладает переходной функцией},
если существует переходная функция, для которой
\eqn{\Pf(\xi_t \in A\vl \xi_s) \eqas P(s,\xi_s,t,A).}
\end{df}

Ясно, что для марковских цепей переходная функция всегда
существует и~задаётся формулой
\eqn{P(s,i,t,A) = \sums{j\in A} p_{ij}(s,t).}
В~общем случае её существование\т нетривиальный факт, доказательство которого получено совсем недавно.

\subsubsection{Однородные марковские процессы и~цепи}

\begin{df}
Марковский процесс, имеющий переходную функцию $P(s,x,t,A)$, называется \emph{однородным},
если эта функция по времени зависит только от разности $t - s$.
\end{df}

Для марковских цепей однородность означает, что переходные вероятности зависят только от разности аргументов.
Введём обозначения $p_{ij}(u) := p_{ij}(s,s+u)$. В~этих обозначениях уравнение Маркова
принимает вид
\eqn{p_{ij}(s+t) = \sums{k} p_{ik}(s) \cdot p_{kj}(t),}
что легко проверяется. Если теперь обозначить $P(t) := (p_{ij}(t))$,
уравнение Маркова можно записать в~более компактной (матричной) форме:
\eqn{P(s+t) = P(s)\cdot P(t).}
Это означает, что семейство матриц $\hc{P(t)}$ образует коммутативную полугруппу, которая
называется \emph{стохастической полугруппой матриц}.

\begin{df}
Однородная марковская цепь называется \emph{стандартной}, если
\eqn{\liml{t \ra 0+} p_{ij}(t) = \de_{ij}.}
\end{df}

Стандартность означает, что стохастическая полугруппа обладает единицей $P(0)$.

\begin{theorem}[О~дифференцируемости переходных вероятностей]
Если $p_{ij}$\т это переходные функции стандартной однородной цепи Маркова,
то существует правая производная
\eqn{\frac{d^+p_{ij}(t)}{dt}\evu{t=0}{11pt}{14pt} = q_{ij},}
причём внедиагональные элементы неотрицательны и~конечны, а~диагональные
неположительны и~могут быть бесконечными.
\end{theorem}

\begin{df}
Положим $q_i := - q_{ii}$. Матрица $Q = (q_{ij})$ называется
\emph{матрицей интенсивностей}.
\end{df}

\begin{df}
Цепь называется \emph{консервативной}, если все элементы матрицы интенсивностей
конечны и~при этом
\eqn{\sums{j \neq i} q_{ij} = q_i \text{ при всех } i.}
Иначе говоря, в~консервативной цепи сумма элементов в~каждой строке матрицы интенсивностей равна нулю.
\end{df}


\begin{theorem}[Обратная система уравнений Колмогорова]
Для консервативной марковской цепи при всех $t > 0$ имеет место равенство $P'(t)=QP(t)$.
\end{theorem}
\begin{proof}
Рассмотрим разностное отношение:
\mln{\frac{p_{ij}(t+h) - p_{ij}(t)}{h} = \frac1h\Br{\sums{k} p_{ik}(h)p_{kj}(t)  - p_{ij}(t)} =
\frac1h\Br{\sums{k\neq i} p_{ik}(h)p_{kj}(t) + p_{ii}(h)p_{ij}(t) - p_{ij}(t)} =\\=
p_{ij}(t)\frac{p_{ii}(h) - 1}{h} + L_{ij}(h,t),}
где
\eqn{L_{ij}(h,t) := \frac1h\sums{k\neq i} p_{ik}(h) p_{kj}(t).}
Из теоремы о~дифференцируемости получаем, что для всякого фиксированного $N$ выполнено
\eqn{\label{eqn:thmEqColmRevUpper}\uliml{h\ra0+} L_{ij}(h,t) \ge
\uliml{h\ra0+} \sums{\substack{k\neq i\\k \le N}} \frac{p_{ik}(h)}{h} p_{kj}(t)=
\uliml{h\ra0+} \sums{\substack{k\neq i\\k \le N}} \frac{p_{ik}(h) - p_{ik}(0)}{h} p_{kj}(t)=
\sums{\substack{k \neq i\\k \le N}} q_{ik} p_{kj}(t).}
Учитывая то, что $p_{kj}(t) \le 1$ и~$\sums{k} p_{ik}(h) = 1$, получаем, что при $N > i$
\eqn{L_{ij}(h,t) = \frac{1}{h}\Br{\sums{\substack{k \neq i\\k \le N}} + \sums{k > N}} \le
\sums{\substack{k\neq i\\k \le N}} \frac{p_{ik}(h)}{h} p_{kj}(t) + \frac1h \sums{k>N} p_{ik}(h)=
\sums{\substack{k\neq i\\k \le N}} \frac{p_{ik}(h)}{h} p_{kj}(t) +
  \frac1h\Br{1 - p_{ii}(h) - \sums{\substack{k\neq i\\k \le N}} p_{ik}(h)}.}
Вновь пользуясь теоремой, получаем
\eqn{\uliml{h\ra0+} L_{ij}(h,t) \le \sums{\substack{k\neq i\\k \le N}} q_{ik}p_{kj}(t) - q_{ii} -
\sums{\substack{k\neq i\\k \le N}} q_{ik}.}
Пользуясь теперь тем, что $q_{ii} = -q_i$ и~переходя к~пределу при $N\ra \bes$, получаем
\eqn{\label{eqn:thmEqColmRevLower}\uliml{h\ra0+} L_{ij}(h,t) \le \sums{k\neq i} q_{ik}p_{kj}(t) + \ub{q_i - \sums{k\neq i} q_{ik}}_0 =
\sums{k\neq i} q_{ik}p_{kj}(t).}
Комбинируя~\eqref{eqn:thmEqColmRevUpper} и~\eqref{eqn:thmEqColmRevLower}, получаем,
что
\eqn{\uliml{h\ra0+} L_{ij}(h,t) = \sums{k\neq i} q_{ik}p_{kj}(t).}

Аналогичное равенство можно получить, рассматривая нижний предел.
\end{proof}

\begin{theorem}[Прямая система уравнений Колмогорова]
Пусть цепь консервативна и~$|q_{ij}| < \bes$ при всех $i, j$. Пусть
\eqn{p_{ij}(h) = q_{ij}h + \al_{ij}(h), \quad \al_{ij} = o(h) \text{ равномерно по } i.}
Тогда при всех $t > 0$ имеет место равенство $P'(t)=P(t)Q$.
\end{theorem}
\begin{proof}
Вновь рассматривая разностное отношение, получаем:
\eqn{\label{eqn:thmEqColmDirect}\frac{p_{ij}(t+h) - p_{ij}(t)}{h} =
p_{ij}(t) \frac{p_{jj}(h) - 1}{h} + \frac1h\sums{k\neq j} p_{ik}(t) p_{kj}(h).}
При фиксированном $j$ найдём $h_0$ такое, что $\bm{\frac{\al_{ij}(h)}{h}} < \ep$ при всех $k$ и~$h \in (0,h_0)$.
Тогда
\eqn{\bbm{\sums{k\neq j} p_{ik}(t) \frac{\al_{kj}(h)}{h}} \le \ep \sums{k\neq j} p_{ik}(t) < \ep.}
Далее,
\eqn{\frac1h\sums{k\neq j} p_{ik}(t) p_{kj}(h) \le \frac{p_{ij}(t+h)}{h} \le \frac1h,}
и~тем самым установлена сходимость ряда в~правой части нужного нам уравнения.
Осталось перейти к~пределу при $h\ra 0+$ в~\eqref{eqn:thmEqColmDirect}.
\end{proof}



\begin{df}
Состояние $i$ называется \emph{мгновенным}, если $q_i = \bes$. Немгновенное состояние $i$
называется \emph{регулярным}, если $\sums{i \neq j}q_{ij} = q_i$, в~противном случае\т \emph{нерегулярным}.
\end{df}


\subsubsection{Стационарные процессы}

Рассмотрим однородную марковскую цепь~$\xi_t$.

Имеется \textbf{эргодическая теорема для марковских цепей}, которая говорит,
что при некоторых условиях
существует не зависящий от $i$ предел
\eqn{\wt p_{ij}  = \liml{t \ra \bes} p_{ij}(t).}
Это свойство можно неформально описать так: система, описываемая марковской цепью,
за большое время  <<забывает>>, из какого состояния она стартовала.
Положим $\wt p_j = \wt p_{ij}$.

\begin{df}
Назовем распределение \emph{стационарным}, если $\sum \wt p_j = 1$.
\end{df}

\begin{petit}
Термин <<стационарное распределение>> связан со~следующим обстоятельством.
\begin{stm}
Пусть процесс $\xi_t$ обладает набором стационарных вероятностей $\hc{\wt p_j}$.
Построим процесс $\eta$ (марковский, однородный) с~теми же переходными
вероятностями $p_{ij}$, но с~$p_i(0) = \wt p_i$.
Тогда этот процесс стационарен в~узком смысле.
\end{stm}

Доказательство этого факта здесь не приводится.
\end{petit}

\end{document}
