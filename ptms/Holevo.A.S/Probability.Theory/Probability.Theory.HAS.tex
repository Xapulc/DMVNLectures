\documentclass[a4paper]{article}
\usepackage{dmvn}

\newcommand{\I}[1]{I_{A_{#1}}}
\newcommand{\IS}[1]{I_{\hc{#1}}}


\newcommand{\ca}[2]{\bigcap\limits_{#1}^{#2}}
\newcommand{\cu}[2]{\bigcup\limits_{#1}^{#2}}
\newcommand{\cai}[1]{\bigcap\limits_{#1}^{\infty}}
\newcommand{\cui}[1]{\bigcup\limits_{#1}^{\infty}}
\newcommand{\smi}[1]{\sum\limits_{#1}^{\infty}}
\newcommand{\sem}{\setminus}
\newcommand{\p}{(\Omega,\As ,P)}

\newcommand{\capinui}{\capl{i=n+1}{\infty}}
\newcommand{\cupinui}{\cupl{i=n+1}{\infty}}
\newcommand{\capknui}{\capl{k=n+1}{\infty}}
\newcommand{\cupknui}{\cupl{k=n+1}{\infty}}

\newcommand{\capinzi}{\capl{i=n}{\infty}}
\newcommand{\cupinzi}{\cupl{i=n}{\infty}}
\newcommand{\capknzi}{\capl{k=n}{\infty}}
\newcommand{\cupknzi}{\cupl{k=n}{\infty}}

\newcommand{\suminui}{\suml{i=n+1}{\infty}}
\newcommand{\suminzi}{\suml{i=n}{\infty}}
\newcommand{\sumknui}{\suml{k=n+1}{\infty}}
\newcommand{\sumknzi}{\suml{k=n}{\infty}}

\newcommand{\scap}{\cap\ldots\cap}
\newcommand{\scup}{\cup\ldots\cup}

\newcommand{\im}{\implies}

\newcommand{\nrai}{n \ra \infty}
\newcommand{\limni}{\liml{\nrai}}

\newcommand{\dx}{\,dx}
\newcommand{\dy}{\,dy}

\newcommand{\vsr}{\overline{X}^{(n)}}
\newcommand{\iii}{\intl{-\infty}{+\infty}}

\tocsubsectionparam{2.7em}
\tocsubsubsectionparam{3.4em}


\begin{document}

\dmvntitle{Курс лекций по}{теории вероятностей}{Лектор\т Холево
Александр Семенович}{II курс, 4 семестр, поток математиков}{Москва, 2006 г.}

\pagebreak
\tableofcontents

\pagebreak
\pagestyle{plain}

\section*{Введение}

\subsection*{Предисловие}

Текст набирался в различное время тремя наборщиками: Д.\,Колосовым, Д.\,Мануйловым и С.\,Кузнецовым.
Потом он был существенно отредактирован \emph{DMVN Corporation}.

Большая просьба к читателям сообщать об ошибках и опечатках авторам.

На данном этапе возможны <<дыры>> в материале курса, которые образовались из\д за слияния работы трёх наборщиков.
Отнеситесь синсходительно и лучше чётко скажите, чего не хватает, иже такое заметите.

Раздел <<Суровые вопросы>> пока остаётся таким, как был. Он немного перевёрстан, часть формул сделана выключными, чтобы было
легче читать и сложнее списывать :) .

В этой редакции исправлена огромная куча опечаток, замеченных при подготовке одним из студентов,
коему большое спасибо (фамилия выясняется).


\medskip
\dmvntrail

\newpage

\section{Основные понятия}

\subsection{Элементарные понятия теории вероятностей}

\subsubsection{События и их вероятности}

\begin{df}
\emph{Детерминированное явление} $A$ (событие)\т это событие, которое всегда выполняется.
\end{df}

\begin{df}
\emph{Недетерминированные события}\т случайные события, такие
события и будут нас интересовать.
\end{df}
Рассмотрим $n$ повторений опыта, в котором может произойти
событие $A$. Пусть $n(A)$\т количество тех опытов, где $A$
выполнилось, тогда частота события $A$: $\nu (A)= \frac{n(A)}{n}$.

\begin{note}
Замечено, что $\nu (A)$ сгущаются вокруг некоторого конечного
значения $p(A)$.
\end{note}

\begin{ex}
Бросание монет. Под событием в данном случае можно понимать выпадание решки.

\qquad\tab{|c|c|c|}{\hline
n & n(a) & $\nu (A)$ \\
\hline
4040 & 2048 & 0.508 \\
\hline
24000 & 12012 & 0.5005\\
\hline}\quad В~данном случае $p(A)=\frac{1}{2}$.
\end{ex}

Вероятность события $A$\т теоретическое модельное значение, к
которому приближается $\nu(A)$ при бесконечно большом n.

Свойства частот:
\begin{nums}{-2}
\item $\nu_n (A) \ge 0$;

\item $\nu_n (\Om)=1 $, где $\Om$\т детерминированное событие;

\item $\nu_n (A \cup B)= \nu_n(A) + \nu_n(B)$, если $A\cap B=\es$.
\end{nums}

\subsubsection{Примеры вероятностных моделей}

\begin{ex}
\dmvnpicrh{pic.1}{-3}
Опыт с~конечным \dmvnpicra{pic.1}{1}{-2.5pc}
числом равновероятных событий. Пусть
$\om_1,\om_2\sco \om_n$\т элементарные события, и~в~опыте может
произойти одно и~только одно из них, тогда $\Om =\hc{\om_1\sco
\om_n}$\т пространство элементарных событий, или вероятностное
пространство.
\end{ex}

\dmvnpicrh{pic.1}{-2}
Будем бросать монету (наше вероятностное пространство будет иметь всего два элемента)
и рассматривать \emph{кривую случайного блуждания}\т график, который поднимается на~1
вверх, если выпадает орёл, и опускается на~1 вниз, если выпадает решка.
Пример такого графика изображён на рис.~1.

\begin{ex}
При игре в рулетку $\Om=\hc{0,1\sco 36}$.
\end{ex}

\begin{df}
$A$ называется \emph{событием}, если $A\subset \Om$, в~частности,
элементарное событие\т это событие.
\end{df}

\begin{df}
Событие $A\subset \Om$, $\Om=\hc{\om_1\sco \om_n}$
\emph{осуществимо} $\Lra$ осуществимо одно из элементарных событий,
составляющих $A$.
\end{df}

\begin{df}
\emph{Достоверное событие}\т $\Om$. \emph{Невозможное событие}\т
$\es$ (событие, которое не может произойти).
\end{df}

\emph{Вероятность} события $A$ в~опыте с~равновероятными исходами
определяется по формуле $P=\frac{|A|}{|\Om|}$.

\begin{ex}
На рулетке $P(чет)=P(нечет)=\frac{18}{37}<\frac12$.
\end{ex}

\begin{problem}[Д'Аламбера]
Какова вероятность того, что при двух бросаниях монеты орёл выпадет
хотя бы один раз?
\end{problem}
\begin{solution}
Пусть <<О>>\т орёл, а <<Р>>\т решка. Тогда $\Om =\{ОО,РО,ОР,РР\}$.
Нас устраивают 3 исхода из 4, значит, $P(A)=\frac{3}{4}$.
\end{solution}

\subsubsection{Комбинаторные формулы}

\begin{df}
Числом размещений из $N$ по $n$ называется число способов разместить
$n$ различных элементов на $N$ местах. Обозначается оно через
$A_N^n$.
\end{df}

Легко видеть, что \eqn{A_N^n = N(N-1)\ldots
(N-n+1)=\frac{N!}{(N-n)!}.} При $N=n$ получаем количество
перестановок из $N$ элементов: $S_N=N!$

Число сочетаний из $N$ по $n$ отличается от числа размещений тем,
что мы не различаем элементы:
\eqn{C_N^n=\frac{A_N^n}{n!}=\frac{N!}{(N-n)!n!}.} В частном случае
$C_N^0=A_N^0=0!=1$.

\begin{problem}
Выборочный контроль качества. Партия из $N$ изделий, среди
которых $M$ бракованных. Наугад выбирается $n$ изделий. Какова
вероятность того, что среди выбранных изделий окажется ровно $m$
бракованных?
\end{problem}
\begin{solution}
Элементарное событие в нашем случае\т произвольная выборка, значит
$|\Om|=C_N^n$. Надо найти вероятность события $A_m=\{ \textnormal{из
$n$ выбранных ровно $m$ бракованных}\}$. Выбираем из $M$ бракованных
изделий $m$, а из $(N-M)$ нормальных изделий\т $(n-m)$. Тогда
среди $n$ выбранных изделий будет ровно $m$ бракованных и
$|A_m|=C_M^m \cdot C_{N-M}^{n-m} \im P(A_m)=\dfrac{C_M^m\cdot
C_{N-M}^{n-m}}{C_N^n}$\т гипергеометрическое распределение
вероятности.
\end{solution}

\begin{problem}
Лотерея. Есть $N$ билетов, из которых $M$ выигрышных. Какова
вероятность выигрыша у~того, кто купил $n$ билетов?
\end{problem}
\begin{solution}
Вероятность того, что не будет выигрышных билетов: $P(m=0)=
P(A_0)=\dfrac{C_M^0\cdot C_{N-M}^n}{C_N^n}.$ Тогда вероятность того,
что будут выигрышные: $P(m>0)=1-P(m=0)=1-\dfrac{C_M^0\cdot C_{N-M}^n
}{C_N^n}$.
\end{solution}

\subsubsection{Опыт с непрерывным пространством элементарных событий}

Элементарное событие $\om=(x,y)\in \Om$. $A\subs \Om \im
P(A)=\frac{\mes A}{\mes \Om}$. Отсюда следует, что $A$ и $\Om$
должны быть измеримы.

\begin{problem}
Отрезок  $\bs{0,1}$ разламывают в 2-х местах случайным образом.
Какова вероятность того, что из полученных кусков можно составить
треугольник?
\end{problem}
\dmvnpicrh{pic.2}{2}
\begin{solution}
Пусть \dmvnpicr{pic.2}{2} отрезок разбивается на $x,y-x,1-y$. Рассмотрим случай $x\le y$
(второй вариант симметричен). Запишем неравенство треугольника для
всех трех сторон:
\dmvnpicrh{pic.2}{3}
$$\bcase{
x+(y-x)>1-y\\
x+(1-y)>y-x\\
(y-x)+ (1-y)>x}
\Lra\bcase{y>\frac12\\ y-x<\frac12\\
x<\frac12}.$$ Из графика видно, что $\mes A=2\cdot \frac18 = \frac14 \cdot
\mes\Om$.
\end{solution}

\subsection{Строгое определение вероятности. Аксиоматика Колмогорова}

\subsubsection{Основные определения}

\ctab{|c|c|}{\hline
{ \large Теория вероятности} & {\large Теория множеств}\phantom{\LARGE f}\\
\hline
Достоверное, невозможное событие & $\Omega$, $\es$\phantom{\LARGE f}\\
\hline
$A_1$ влечет $A_2$ & $A_1\subs A_2$\phantom{\LARGE f}\\
\hline
$A_1$ или(и) $A_2$ & $A_1\cup A_2 , \cupiun A_i$\phantom{\LARGE f}\\
\hline
$A_1$ и $A_2$ & $A_1\cap A_2, \capiun A_i$\phantom{\LARGE f}\\
\hline
$A_1$ и $A_2$ несовместны & $A_1\cap A_2=\es$\phantom{\LARGE f}\\
\hline
не $A$ & $\ol{A}=\hc{\mbox{совокупность $\om$, не входящих в A}}$\phantom{\LARGE f}\\
\hline
Выполняется $A_2$, но не $A_1$ & $A_2\sem A_1=A_2\cap \ol A_1$\phantom{\LARGE f}\\
\hline}

\begin{df}
Пусть $\Om$\т некоторое множество. Тогда назовем класс $\As \in
2^{\Om}$ подмножеств $\Om$ \emph{алгеброй множеств}, если:
\begin{nums}{-3}
\item $\Om \in \As$;
\item $A_1,A_2 \in \As \im A_1\cup A_2\in \As$ и $A_1\cap A_2\in \As$;
\item $A\in \As\im \ol A \in \As$.
\end{nums}
\end{df}

Свойства алгебры множеств таковы:

\begin{nums}{-3}
\item $\es \in \As$;
\item $A_1\sco A_n \in \As \im
\cupiun A_i \in \As, \capiun A_i \in \As$.
\end{nums}

\begin{df}
Алгебра $\As$ называется \emph{$\si$-алгеброй}, если выполнено
условие:
\begin{nums}{-2}
\item[$2'.$] $A_1,A_2\etc \in \As \im
\cupiui A_i\in \As, \capiui A_i \in \As$.
\end{nums}
\end{df}

\subsubsection{Вероятностное пространство и аксиомы Колмогорова}

\begin{df}
\emph{Вероятностное пространство}\т это тройка $\p$, где $\Om$\т
пространство элементарных событий; элементы $\Om$ называются
элементарными событиями; $\As$\т $\si$-алгебра подмножеств $\Om$,
элементы $\As$ называются событиями; $P$\т числовая функция,
определенная на $\As$ ($P\cln \As \ra \R$). Для $ A\in \As$,
$P(A)$\т вещественное число, которое называется \emph{вероятностью}
события $A$.
\end{df}

Следующие свойства вероятности $P$ на $\p$ называются \emph{аксиомами Колмогорова}.

\begin{nums}{-2}
\item $P(A)\ge 0$,  $\fa A\in \As$;
\item $P(\Om)=1$;
\item Если $A$ и $B$ несовместные события $(A \cap B =\es)$, то
$P(A\cup B)=P(A)+P(B)$;
\item Если $B_1\sups B_2\sups \ldots \sups B_n\sups \ldots$\т
убывающая последовательность событий, причем $\capiui B_i=\es$,
то $\limni P(B_n)= 0 $.
\end{nums}

\begin{ex}
Дискретное пространство элементарных событий. $\Om =\bc{\om_1\sco
\om_n\etc}$, $\As = 2^{\Om}$\т класс всех подмножеств в $\Om$.
 Пусть дана последовательность $\bc{p_1\sco p_n\etc}$, $p_n\ge 0$, $\sumnui p_n =1$. Тогда
$P(A)=\sums{i\cln \om_i\in A} p_i$.
\end{ex}

\begin{problem}
Доказать, что это действительно вероятностное пространство. Это
некоторое обобщение на счетные множества.
\end{problem}

{\bf Следствия аксиом Колмогорова:}

\begin{nums}{-2}
\item[]
\begin{nums}{-2}
\item $P(\ol A)=1-P(A)$.

Это следует из того, что $A \cap \ol A=\es, A\cup \ol A=\Om$ ,
откуда $ 1=P(\Om)=P(A\cup\ol A)=P(A)+P(\ol A)$. В~частности
$P(\es)=0$.

\item Монотонность: Если $A_1\subs A_2$, то $P(A_1)\le P(A_2)$.

$A_2=A_1\cup (A_2\sem A_1) \im P(A_2)=P(A_1)+P(A_2\sem A_1)\ge
P(A_1)$, так как $A_1 \cap (A_2\sem A_1)=\es$, $P(A_2\sem A_1)\ge
0$.

Отсюда, в~частности, имеем $P\in \bs{0,1}$, так как $\es\subseteq
A\subseteq \Om$.

\item $P(A_1\cup A_2)=P(A_1)+P(A_2)-P(A_1\cap A_2)$.

Докажем это: $A_1=(A_1\cap A_2)\cup (A_1\sem A_2)$; по аксиоме 3
$\Br{(A_1\cap A_2) \cap (A_1\sem A_2)=\es}$ имеем
\eqn{P(A_1)=P(A_1\cap A_2)+P(A_1\sem A_2).} Добавим к~обеим частям
$P(A_2)$, и~получим, что \eqn{P(A_1)+P(A_2)=P(A_1\cap A_2) +
P(A_1\sem A_2)+P(A_2)=P(A_1\cap A_2)+P(A_1\cup A_2).} Откуда,
\eqn{P(A_1\cup A_2)=P(A_1)+P(A_2)-P(A_1\cap A_2).}
\end{nums}
\end{nums}

\begin{problem}
Есть 2 кости. Какова вероятность того, что хотя бы на одной будет
$\ge$ 5 очков.
\end{problem}
\begin{solution}
Всего $6\times 6=36$  элементарных событий, вероятность
элементарного события равна $\frac{1}{36}$. Пусть
$A_1\bw=\hc{\mbox{на {1--\,ой} кости}\bw\ge 5}, A_2=\hc{\mbox{на
{2--\,ой} кости}\ge 5}$, тогда
$$P(A_1\cup A_2)=P(A_1)+P(A_2)-P(A_1\cap A_2)=\frac{12}{36}+\frac{12}{36}-\frac4{36}\bw=\frac59.$$
\end{solution}
\begin{nums}{-2}
\item[]
\begin{nums}{-2}
\item[г.] Конечная аддитивность: $A_1\sco A_n$\т попарно
несовместные события, то есть $(A_i\cap A_j=\es ,i\ne j)$. Тогда
$P\br{\cupiun A_i}=\sumiun P(A_i)$. Доказывается по индукции.
\end{nums}
\end{nums}

\begin{nums}{-2}
\item[$3^*$.] $\si$-аддитивность: Пусть $A_1\sco A_n\etc$\т попарно несовместные события. Тогда справедливо
равенство:

$$P\Br{\cupnui A_n}=\sumnui P(A_n).$$
Это мы докажем ниже.
\end{nums}

\subsubsection{Теорема равносильности систем аксиом}

\begin{theorem}
Система аксиом 1--4 (Аксиомы Колмогорова) равносильна системе аксиом $1$, $2$, $3^*$.
\end{theorem}
\begin{proof}
$\boxed{3, 4 \Ra 3^*}$ Пусть $A_1\sco A_n$\т попарно несовместные
события. Обозначим $B_n = \cupinui A_i$\т остаток ряда. Тогда
$\cupiui A_i= A_1\scup A_n \cup B_n$. Используя конечную
аддитивность, получаем \eqn{P(\cupiui A_i)=P(A_1)\spl
P(A_n)+P(B_n).}
 Ясно, что $B_1\sups B_2\sups \ldots \sups B_n
\sups \ldots$ и~$\capnui B_n = \es$\т это следует из~того, что
$\om\in \capnui B_n$ $\Lra \om\in B_n$ для $\fa n\in\N$; значит
$\exi i_0\cln \om\in A_{i_0}$, но~тогда $\fa n>i_0$ выполняется $\om
\notin B_n$, так как события $A_i$ попарно несовместны. Тогда из~4
имеем $P(B_n)\ra 0 \im P(\cupiui A_i)=\sumiui P(A_i)$.

$\boxed{3^* \Ra 3}$\т очевидно.

$\boxed{3^* \Ra 4}$ Рассмотрим $B_1 \sups \ldots \sups B_n \sups
\ldots \mbox{ и } \capnui B_n =\es$. Пусть $A_n=B_n \sem B_{n+1}$. %Следует нарисовать рисунок
Легко видеть, что $A_n$~несовместны и~$B_n=\cupknzi A_k$. Используя
счетную аддитивность, получаем \eqn{P(B_1)=P(\cupkui A_k)\bw=\sumkui
P(A_k),\quad P(B_n)=P(\cupknzi A_k)=\sumknzi P(A_k).} Значит,
$P(B_n)$\т остаток сходящегося ряда $\sumkui P(A_k)$, и, %Пунктуация???????????
следовательно, $ P(B_n)\ra 0$.
\end{proof}

\subsection{Условные вероятности. Формула полной вероятности. Формула Байеса}

\subsubsection{Условная вероятность}

\begin{ex}
Пусть $A$\т множество курящих, $B$\т множество больных. Если
эти множества пересекаются, то вводится понятие
\emph{относительной частоты} ${\nu}_n(B|A) = \frac{n(B\cap
A)}{n(A)}$. Если $n\gg 1$, то
$$n(A)\sim P(A)\cdot n, \quad n(B\cap A) \bw\sim P(B\cap A)\cdot n \im {\nu}_n \sim \frac{P(B\cap A)}{P(A)}.$$
\end{ex}

\begin{df}
Пусть $\p$\т вероятностное пространство. Пусть $A\in \As$ и
$P(A)>0$. Тогда \emph{условной вероятностью} $B\in \As$ при условии
$A$ называют $P(B|A):= \dfrac{P(B\cap A)}{P(A)}$.
\end{df}

\begin{note}
Если $B\subs A$,то $P(B|A)=\frac{P(B)}{P(A)}$. Если $B\cap A=
\es$, то $P(B|A)=0$.
\end{note}

\begin{ex}
Распад радиоактивного атома. Пусть мы знаем, что:
\begin{nums}{-2}
\item Вероятность того, что атом не~распадется до~$t_0+t$, при
условии, что он~не~распался до~$t_0$, зависит только~от~$t$~и~не
зависит от~$t_0$.
\item Эта вероятность стремится к~$1$ при $t \ra 0$.
\end{nums}
Найти закон распада.
\end{ex}
\begin{solution} $A(t):=\hc{\mbox{Атом не распался до момента времени } t}$. Нам известно, что:
\begin{nums}{-2}
\item $p(t)=P(A(t+t_0)|A(t_0))$;
\item $\liml{t\ra 0} p(t)=1$.
\end{nums}
$A(t_0+t)\subs A(t_0)$, поэтому
\eqn{p(t+s)=\frac{P(A(t_0+t+s))}{P(A(t_0))}=\frac{P(A(t_0+t+s))}{P(A(t_0+t))}\cdot
\frac{P(A(t_0+t))}{P(A(t_0))}=p(s)\cdot p(t)\im p(t+s)=p(s)\cdot
p(t).} С учётом условия 2 получаем экспоненциальный закон распада.
\end{solution}

\begin{df}
Семейство $A_i$ $\hc{A_i\in \As\mid i=1,2\sco n}$ называется
\emph{разбиением} $\Om$, если $\cupsql{i=1}{n} A_i=\Om$.
\end{df}

\subsubsection{Формула полной вероятности}

\begin{theorem}[Формула полной вероятности] Пусть $\hc{A_i}$\т разбиение $\Om$,
$P(A_i)>0, B\in \As$. Тогда \eqn{P(B)=\sumiun P(B|A_i)\cdot P(A_i).}
\end{theorem}
\begin{proof}
$B=\Br{B\cap A_1}\scup\Br{B\cap A_n}$. Так как $\hc{A_i}$\т
разбиение $\Om$, то все $(B\cap A_i)$ несовместны, поэтому
$P(B)=\sumiun P(B\cap A_i)=\sumiun P(B|A_i)\cdot P(A_i)$. Тут мы
использовали формулу условной вероятности.
\end{proof}

\subsubsection{Формула Байеса}

\begin{theorem}[Формула Байеса]
Пусть $\hc{A_i}$\т разбиение $\Om$, $P(A_i)>0$,
$B\in \As$, $P(B)>0$. Тогда выполняется:
\eqn{P(A_i|B)=\frac{P(B|A_i)\cdot P(A_i)}{\sumkun P(B|A_k)\cdot
P(A_k)}.}
\end{theorem}
\begin{proof}
Имеем \eqn{P(A_i|B)= \frac{P(A_i\cap B)}{P(B)}.} Применим формулу
полной вероятности для $P(B)$, а~$P(A_i\cap B)$ запишем как
$P(B|A_i)\cdot P(A_i)$. Тогда наша условная вероятность запишется
в~виде: \eqn{P(A_i|B)=\frac{P(B|A_i)\cdot P(A_i)}{\sumkun
P(B|A_k)\cdot P(A_k)}.}
\end{proof}

\begin{ex}
Партия состоит из $N_1+N_2+N_3$ изделий, выпускаемых
соответственно 1-м, 2-м и 3-м заводами. Каждому заводу
соответствует процент брака $P_1,P_2,P_3$. Наугад выбранное
изделие оказывается бракованным. Найти вероятность $p_j$ того,
что оно было выпущено $j$-м заводом.
\end{ex}
\begin{solution}
Пусть $B=\hc{\mbox{Изделие бракованное}}$, $A_j=\hc{\mbox{Изделие
выпущено $j$-м заводом}}$. Тогда \eqn{p_j=P(A_j|B)\bw=\frac{N_j\cdot
P_j}{N_1\cdot P_1+N_2\cdot P_2+N_3\cdot P_3}.}
\end{solution}

\subsection{Независимость. Схема Бернулли}

\begin{df}
События $A,B \in \As$ называются \emph{независимыми}, если $P(A\cap
B)=P(A)\cdot P(B)$.
\end{df}

\begin{note}
Названо так, потому что если $A$ и~$B$ независимы и~$P(A)>0$, тогда
$P(B|A)=P(B)$, то есть $A$ не~влияет на~$B$.
\end{note}

\begin{note}
Если $A$ и~$B$ независимы, то $A$ и~$\ol{B}$; $\ol{A}$ и~$B$;
$\ol{A}$ и~$\ol{B}$\т тоже независимы.
\end{note}

\begin{ex}
Из колоды в 36 карт вытягивается 1 карта. $A$\т вытянули пику;
$B$\т вытянули даму. Проверим зависимость $A$ и~$B$.
\end{ex}
\begin{solution}
$P(\mbox{элементарного события})=\frac{1}{36}$, $P(A\cap
B)=\frac1{36}$, $P(A)=\frac{1}{4}$, $P(B)=\frac19$ $\im$ $P(A\cap
B)=P(A)\cdot P(B)$, откуда следует, что события $A$ и~$B$
независимы.
\end{solution}

Случай нескольких событий $A_1\sco A_n$:

\begin{df}
\emph{Попарно независимы}: если $A_i,A_j$ ($\fa i,j; i\neq j$)
независимы.
\end{df}

\begin{df}
\emph{Независимы в совокупности}: $\fa A_{i_1}\sco A_{i_k}$ имеем
$P(A_{i_1}\scap A_{i_k})=P(A_{i_1})\sd P(A_{i_k})$.
\end{df}

\begin{problem}
Покажем, что из попарной независимости {\bf не следует}
независимость в совокупности.
\end{problem}
\begin{solution}
Пусть монета бросается 2~раза. Рассмотрим события $A$=\{в~первый раз
выпал орёл\}, $B$=\{во~второй раз выпал орёл\}, $C$=\{орёл выпал
ровно 1~раз\}. Тогда $P(A)=P(B)=P(C)=\frac12$, $P(A\cap
C)\bw=P(A\cap B)\bw=P{(B\cap C)}\bw=\frac14$, значит события $A,B,C$
попарно независимы, но~$P(A\cap B\cap C)=0$, следовательно они
\emph{не~независимы} в~совокупности.
\end{solution}

\begin{df}
\emph{Схема Бернулли}: последовательность $n$ одинаковых испытаний,
в~каждом из~которых с~вероятностью $p$ происходит успех,
а~с~вероятностью $(1-p)$\т неудача.
\end{df}

\begin{ex}
Пусть в~мишень производится $n$ независимых выстрелов. Какова
вероятность попадания в~мишень хотя бы 1~раз?
\end{ex}
\begin{solution}
Вероятность попадания в~мишень будет $P=1-(1-p)^n$, так как
$(1-p)^n$\т вероятность промаха во всех выстрелах.
\end{solution}
Вероятностное пространство схемы Бернулли:

<<Успех>>=1, <<неудача>>=0 $\im$ элементарное событие $\om
\bw=(\om_1,\bw\ldots, \om_n)$, где $\om_i\in \hc{0,1}$.
$\Om=\hc{\om}$, $|\Om|=2^n$, $\As=2^{\Om}$ (то есть $\As$\т
множество всех подмножеств $\Om$). $P(\om)\bw=p^m(1-p)^{n-m}$, где
$m$\т количество успехов~в~$\om$. $P(A)=\sums{\om\in A} P(\om)$.
Обозначим событие $A_m=$\{в $\om$ ровно $m$ успехов\}, тогда
$P(A_m)=C_n^m p^m (1-p)^{n-m}$. Это будет \emph{биномиальное
распределение}. Проверим корректность такого определения
вероятности: \eqn{\suml{m=0}{n}P(A_m)\bw=\suml{m=0}{n} C_n^m p^m
(1-p)^{n-m}\stackrel{!}{=}(p\bw+(1-p))^n=1.}

Переход <<!>> следует из формулы бинома Ньютона.

\subsection{Простейшие предельные теоремы}

\subsubsection{Теорема Бернулли}

\begin{theorem}[Бернулли (теор. аналог устойчивости частот)]
Для любого $\ep>0$ существует предел
$$\limni P\hc{A_m\cln\hm{\frac mn - p}<\ep}\bw=1,$$ где
$n$\т число испытаний в схеме Бернулли, $m$\т число успехов,
$p$\т вероятность успеха.
\end{theorem}
\begin{proof}
Очевидно, что
$$P\hc{A_m\cln\hm{\frac mn - p}<\ep}=1-P\hc{A_m\cln\hm{\frac mn - p}\ge \ep}.$$
Тогда
\eqn{P\hc{A_m\cln\hm{\frac mn - p}\ge \ep}=\sums{m\cln\hm{\frac mn -
p}\ge \ep} P(A_m) \stackrel{!}{\le} \suml{m=0}{n}\hr{\frac{m-np}{\ep
n}}^2 P(A_m)= \frac{1}{\ep^2 n^2} \suml{m=0}{n}(m-np)^2 P(A_m).}

Неравенство <<!>> обосновано тем, что $\dfrac{\hm{m-np}}{\ep n}\bw= \dfrac{\hm{\frac mn - p}}{\ep} \ge 1$.

\eqn{\suml{m=0}{n}(m-np)^2 P(A_m)=\suml{m=0}{n} m^2  P(A_m) -
\suml{m=0}{n} (2mnp)  P(A_m)+ \suml{m=0}{n} n^2 p^2  P(A_m).}

Рассмотрим \eqn{\suml{m=0}{n} x^m P(A_m)\bw=\suml{m=0}{n}C_n^m x^m
p^m (1-p)^{n-m} \stackrel{!}{=}(xp+1-p)^n.}

Переход <<!>> следует из формулы бинома Ньютона.

Продифференцируем по~$x$ и~подставим~$x=1$: \eqn{\suml{m=0}{n} m
C_n^m p^m (1-p)^{n-m}=np \Lra \suml{m=0}{n}m P(A_m)=np.}

Продифференцируем еще раз и~снова подставим~$x=1$:
\eqn{\suml{m=0}{n} m(m-1) C_n^m p^m (1\bw-p)^{n-m}=n (n-1) p^2 \Lra
\suml{m=0}{n}m(m-1)P(A_m)=n(n-1)p^2.}

В итоге получаем \eqn{
\begin{split}
\suml{m=0}{n}(m-np)^2 P(A_m)= \suml{m=0}{n} m^2 P(A_m)-
\suml{m=0}{n}(2mnp)&P(A_m) + \suml{m=0}{n} n^2 p^2 P(A_m)=\\
&=n(n-1)p^2 +np  - 2 n^2 p^2 + n^2 p^2 =n p(1-p),
\end{split}} значит $P\hc{
m\cln\hm{\frac mn - p}\ge \ep}\,\le\, \dfrac{1}{\ep^2 n^2}
\suml{m=0}{n}(m-np)^2 P(A_m) =\dfrac{p(1-p)}{\ep^2 n}\ra 0 $ при
$\nrai$, следовательно \eqn{\limni P\hc{m\cln\hm{\frac mn - p}<
\ep}\bw=\limni \bbbr{1 - P\hc{m\cln\hm{\frac mn - p}\ge \ep}}=1.}
\hfill\end{proof}
\emph{Принцип  малых вероятностей}: событие малой вероятности
следует рассматривать как невозможное при единичном испытании.

\subsubsection{Теоремы Муавра\ч Лапласа и Пуассона}

\begin{theorem}[Муавра\ч Лапласа]
Пусть в~схеме Бернулли $0<p<1$. Тогда при $\nrai$,
\eqn{P(A_m)\bw=\frac1{\sqrt{2\pi np(1-p)}}\bw\cdot e^{-\tfrac{x^2}2}
(1\bw+o(1)),} где $x=\dfrac{m-np}{\sqrt{np(1-p)}}$, а $o(1)$\т
равномерно по~$|x|\le C$.
\end{theorem}

Эта теорема доказывается с использованием формулы Стирлинга. Она
является следствием ЦПТ (центральная предельная теорема).

\begin{theorem}[Пуассона]
{\rm(}Полезна при $p\ra 0, p\ra 1${\rm)}. Рассмотрим
последовательность схем Бернулли, где $p_n=\frac{\la}n$~$(\la >0)$.
Тогда $P_n(A_m)\ra \frac{\la^m}{m!} e^{-\la}$, при $\nrai$.
\end{theorem}
\begin{proof}
\eqn{\begin{split} \quad P_n(A_m)=\frac{n!}{m!(n-m)!}
&\hr{\frac{\la}n}^m \hr{1-\frac{\la}n}^{n-m}=\\ \\
 &=\frac{\la^m}{m!}\cdot \frac{n(n-1)\sd
(n-m+1)}{n\cdot n \sd n}\cdot \hr{1-\frac{\la}n}^{n-m} \ra
\frac{\la^m}{m!}\,e^{-\la}\mbox{ при }\nrai.\end{split}}
\end{proof}

$\dfrac{\la^m}{m!}\,e^{-\la}$\т распределение Пуассона редких
событий.
\smallskip

В~применениях $n$ велико, а~$np=\la \sim 1 $, $p$\т малое
фиксированное.

\begin{ex}
Рассмотрим $n \gg 1$ независимых радиоактивных атомов. Вероятность
распада за~время $t$ для атома: $p=1-e^{\la t} \sim \frac{\la T}{n},
t=\frac Tn$. Вероятность того, что к~моменту $T$ распадется $m$
атомов, равна $\frac{(\la T)^m}{m!}\, e^{-\la T}$.
\end{ex}

\section{Случайные величины. Функции распределения}

\subsection{Определения и примеры}

\subsubsection{Случайные величины}

Случайная величина $X(\om)$\т числовая функция от элементарного
события.

\begin{df}
Пусть $(\Om,\As,P)$\т вероятностное пространство. \emph{Случайной
величиной} $X$ на $\Om$ называется функция $X\cln \Om \ra \R$,
измеримая относительно $\As$, т.е. $\fa x \in \R$ имеем
$X^{-1}(-\infty;x) = \hc{\om\cln X(\om)<x} \in \As$.
\end{df}

\begin{note}
События $\hc{\om\cln X(\om)\le x}$, $\hc{\om\cln X(\om)\ge x}$,
$\hc{\om\cln x_1\le X(\om)\le x_2}$, $\hc{\om\cln X(\om)=x}$ и тому
подобные события тоже лежат в $\As$.
\end{note}
\begin{solution}
\begin{nums}{-2}
\item $\hc{\om\cln X(\om)\le x}=\capnui\hc{\om\cln X(\om)<x + \frac1n}$,
переходя к пределу получаем нужный результат, по свойству
$\si$-алгебры бесконечное пересечение принадлежит $\As$.

\item $\hc{\om\cln X(\om)\ge x}=\ol{\hc{\om\cln X(\om)<x}}\in \As$, поскольку со всяким
событием в $\si$-алгебру входит и его дополнение.

\item \label{3}$\hc{\om\cln x_1\le X(\om)\le x_2}=\hc{\om\cln x_1\le
X(\om)} \cap \hc{\om\cln X(\om)\le x_2}$.

\item Надо положить $x_1=x=x_2$ и~применить  \ref{3}.
\end{nums}
\end{solution}

\begin{df}
Случайная величина $X$ называется \emph{дискретной}, если она
может принимать только конечное или счетное число различных
значений.
\end{df}

Пусть $X(\om)$\т дискретная случайная величина. Перенумеруем
образ: $X(\Om)=\hc{x_1,x_2\etc}$. Положим $A_i\bw=\hc{\om\cln
X(\om)=x_i}$; $A_1\sco A_n\etc$ попарно не пересекаются;
$\hc{A_n}$\т (счетное) разбиение $\Om$; $\cupnui A_n = \Om$;
$p_i\bw=P(A_i)\bw=P\hc{\om\cln X(\om)=x_i}$; $\sumiui p_i = 1$;
$p_i\ge 0$. Тогда $\BC{(x_i,p_i)}$ называется \emph{распределением
вероятностей} дискретной случайной величины $X$.

\begin{ex}
\emph{Биномиальное распределение.} $X(\om)$\т число успехов в~$n$
испытаниях. \eqn{P\hc{\om\cln X(\om)=m}=C_n^m p^m (1-p)^{n-m}.}
\end{ex}


\subsubsection{Функции распределения}

Напомним, что на $\p$ была введена функция $X\cln \Om \ra \R$, такая
что $\fa x \in \R$ имеем $X^{-1} (-\infty;x) \in \As$.

\begin{df}
\emph{Функцией распределения} случайной величины $X$ называется
$F(x)= P\bc{\om\cln X(\om) < x}$ или $F(x):=P \bc{X^{-1}
(-\infty;x)}$.
\end{df}

\begin{theorem}
$F(x)$\т функция распределения обладает свойствами:
\begin{points}{-2}
\item $F(x)$ возрастает на $\R$;
\item $F(-\infty)=0$, $F(+\infty)=1$;
\item $F(x)$ непрерывна слева.
\end{points}
\end{theorem}
\begin{proof}
\begin{points}{-2} %есть у нас и такое окружение....
\item Пусть $x_1<x_2$, $A_1:=X^{-1}(-\infty; x_1)$, $A_2:=X^{-1}
(-\infty ; x_2)$. Так как $x_1<x_2$, то $A_1\subs A_2$, поэтому
$P(A_1)\bw\le P(A_2) \Lra F(x_1)\le F(x_2)$.

\item Пусть $x_n \searrow -\infty$. Покажем, что
$F(x_n) \ra 0$. Рассмотрим $A_n := X^{-1}(-\infty;x_n)$. Очевидно,
что $A_1\sups A_2 \bw\sups A_3 \ldots$ и~$\capnui A_n=\es$,
так как если $\om\in \capnui A_n$, то $\fa x_n$ верно %????????????????????????????
$X(\om)<x_n$, но это невозможно $\im$ по аксиоме непрерывности
вероятности получаем $0 =\limni P(A_n)= \limni F(x_n)$.

Аналогично рассмотрим $x_n  \nearrow  +\infty$, $A_1\subs A_2 \subs
\ldots$ и $\cupnui A_n=\Om$, откуда $\ol A_1\sups \ol A_2 \sups
\ldots \im \capnui \ol A_n=\es$, и~поэтому $\limni P(\ol A_n) =0=1 -
\limni P(A_n)$. Следовательно, $\limni P(A_n)=1$.

\item Сначала докажем ряд вспомогательных фактов.
\begin{note}
\begin{nums}{-2}
\item Пусть $A_1\sups A_2 \sups \ldots$ Рассмотрим $A:= \capnui A_n$.
Тогда $P(A)=\limni P(A_n)$. В~самом деле, рассмотрим $A_n^0 := A_n
\sem A$. К~ним применим аксиому непрерывности.

\item Если $A_1\subs A_2\subs \ldots$, то~$P(\cupnui A_n) = \liml{n
\ra \infty} P(A_n)$.
\end{nums}
\end{note} Теперь докажем пункт \pt{3} теоремы.

Рассмотрим $x_n \nearrow x_0$ и~положим $A_n := X^{-1}
(-\infty;x_n)$, $A_0:= X^{-1}(- \infty;x_0)$. Ясно, что $\cupnui A_n
= A_0$ (несложно видеть из того, что $x_n \nearrow x_0$).

Из пункта \emph{б} замечания имеем $\limni P(A_n) = P(A_0)$, где
$P(A_n)\bw=F(x_n)$, $P(A_0)=F(x_0)$. Заметим далее, что $P\hr{X^{-1}
(-\infty;x_0] }=\liml{x_n \ra x_0 + 0} F(x_n)=F(x_0 + 0)$. Положим
$A_0:= X^{-1}(-\infty;x_0]$. Очевидно $\capnui A_n = A_0 (x_n
\downarrow x_0) \im$ осталось перейти к пределу. Итак, теперь ясно,
что $P(X^{-1} [a,b]) = P(X^{-1}(- \infty;b]) - P(X^{-1} (-\infty;a)
)= F(b+0)- F(a)$. В итоге получаем: $P \bc{\om\cln X(\om)\in [a,b]}
= F(b+0) - F(a)$, в частности $P \bc{\om\cln X(\om) = a}=F(a+0)-F(a)
=: \De F(a)$.
\end{points} %есть у нас и такое окружение....
\end{proof}

\begin{problem}
Как выглядит функция распределения дискретной случайной величины?
\end{problem}

\begin{df}
Функция распределения $F(X)$ называется абсолютно непрерывной,
если $\exi p(x)\cln \fa a,b (a<b)$ имеем $F(b)- F(a) = \intl ab
 p(x)\dx$ (интегрируемость по Лебегу). Функция $p(x)$ называется плотностью распределения
 вероятности.
\end{df}

\begin{note}
Если $F$ абсолютно непрерывна, то она непрерывна.
\end{note}

\begin{nums}{-2}
\item $F'(x)=p(x)$ почти всюду. (Если $p(x) \in \SegC$ (кусочно-непрерывные), то $F'(x)=p(x)$ во всех точках
непрерывности).

\item $ a \ra -\infty  \im F(x)= \intl{-\infty}{x} p(\tau)\,d\tau$

\item $p(x) \ge 0 $ почти всюду.
\end{nums}

При $a \ra -\infty, b \ra +\infty : \intl{-\infty}{+\infty}
p(\tau)\,d\tau = 1$.

\begin{ex}

\begin{nums}{-2}
\item Равномерное распределение на $[a,b]$: $p(x)= \frac1{b-a}
{\chi}_{[a,b]}$;

\item Экспоненциальное распределение: $F(t) =\left\{\begin{aligned}
&0,\quad\qquad &t<0;\\
&1-e^{-\la t}, &t \ge 0.
\end{aligned}\right.
$
\end{nums}
\end{ex}

Нормальное распределение:

$\intl{-\infty}{+\infty} e^{-\frac{x^2}2} \dx = I \im I^2 =
\intl{-\infty}{+\infty}\intl{-\infty}{+\infty} e^ {- \frac{x^2 +
y^2}2} \dx \dy = \intl 0{2 \pi} d\ph \cdot \intl{0}{+\infty} e^{-
\frac{r^2}{2}} r \,dr = 2\pi \im I=\sqrt{2\pi} \im p(x) \bw=
\frac1{\sqrt{2 \pi}} e^{-\frac{x^2}2}$\т плотность
распределения. $\Phi (x) := \intl{-\infty}{x} \frac1{\sqrt{2
\pi}} e^{-\frac{\tau^2}2} \,d\tau$\т функция нормального
распределения. Стандартное нормальное распределение: $\Ns (0,1)$.
Еще рассматриваются функции вида
$$p(x)= \frac1{\sqrt{2 \pi {\si }^2} } e^{- \frac{(x-m)^2}{2 {\si}^2}}, \quad  \si \neq 0, \quad  m \in \R.$$
Обозначение таких функций: $\Ns(m,\si)$.

\begin{ex}
Пусть $X$ имеет плотность $p(x)$. Найти распределение $Y= X^2$.
\end{ex}
\begin{solution}
\mln{F_Y (y) = P \bc{\om\cln Y(\om)<y} = P \bc{\om\cln X(\om)^2 < y} =
P \bc{\om\cln -\sqrt y < X(\om) < \sqrt y} = \\ =
\intl{-\sqrt{y}}{\sqrt{y}} p_X(x) \dx \im p_Y(y) =
\bcase{0, \quad y \le 0;\\ p_X(\sqrt y) \frac1{2\sqrt y} + p_X(-\sqrt y) \frac1{2\sqrt y},
\quad y\ge 0.}}
\hfill\end{solution}

\begin{note}
Кроме дискретных и абсолютно непрерывных, бывают сингулярные
случайные величины. Например функция распределения Кантора.

\emph{Теорема Лебега}: произвольная функция распределения
разлагается как $F(x)= C_1 F_1(x) + C_2 F_2(x) + C_3 F_3(x)$, где
$F_i(x)$\т соответственно функции распределения абсолютно
непрерывной, сингулярной и дискретной величин.
\end{note}

\subsection{Семейства случайных величин. Независимость}

Пусть $X_i(\om)$, $i=1\sco n$\т случайные величины на $\p$. Иначе
говоря, задан случайный вектор $X(\om) = (X_1(\om)\sco X_n(\om))
\in {\R}^n$.

\begin{df}
Функцией совместного распределения $X_1\sco X_n$ называется
$F(x_1\sco x_n) = P \bc{\om\cln X_1(\om)\bw< x_1; \ldots ;
X_n(\om)<x_n} = P(A)$.
\end{df}

\begin{note}
Функция определена корректно, так как $A$ есть пересечение: $A =
\bigcap X_i^{-1}(-\infty;x_i)$, а~так как $X_i$\т случайные
величины, то каждый элемент пересечения лежит в $\As$.
\end{note}

\subsubsection{Основные свойства}

1) $F(x_1,\ldots , x_n)$ является неубывающей по каждому аргументу
$x_j$, при фиксированных остальных аргументах, поскольку
вероятность возрастающего семейства множеств возрастает.

2) $\liml{x_j \ra -\infty } F(x_1 , \ldots ,x_j, \ldots ,x_n) = 0
$ при фиксированных $ x_1, \ldots , \widehat{x_j} , \ldots x_n$.
Доказательство этого свойства аналогично. $\liml{x1,\ldots , x_n
\ra +\infty} F(x_1 , \ldots ,x_j, \ldots ,x_n) = 1 $, поскольку
любая точка будет захвачена.

3) $F$ непрерывна слева по каждой из переменных. $\liml{x_2,\ldots
,x_n \ra +\infty} F(x_1,\ldots , x_n) = F(x_1)$\т~функция
распределения случайной величины $x_1$. Можно считать что
\mln{x_2,\ldots ,x_n \uparrow +\infty \ra \liml{x_2,\ldots ,x_n  \uparrow +\infty} F(x_1,\ldots ,x_n) = \\ =
\liml{x_2,\ldots ,x_n \uparrow +\infty} P \bc{\om: X_1(\om)<x_1,\ldots, X_n(\om)<x_n} =\\=
P \bc{\om: X_1(\om)<x_1, X_2< +\infty , \ldots, X_n(\om)<+\infty} = P\bc {\om: X_1 < x_1} = F_1(x_1)}\т
функция распределения $x_1$.

4) Вероятность попадания в $n$\д мерный полуинтервал (это
$\bc{(x_1,\ldots , x_n) : a_1 \le x_1 < b_1,\ldots , a_n \le x_n <
b_n}$ )

Рассмотрим $P\bc{\om: x_1 \le X_1(\om) < x_1 + h_1 , \ldots , x_n
\le X_n(\om) < x_n + h_n } =: \De^{(1)}_{h_1} \ldots
\De^{(n)}_{h_n} F(x_1,\ldots ,x_n)$, где
$\De_{h_j}^{(j)} F(x_1 , \ldots , x_n) = F(x_1, \ldots ,
x_j+h_j, \ldots , x_n ) - F(x)$.

\begin{proof}
Доказательство для $n=2$.
\mln{P \bc{\om: x_1 \le X_1(\om) < x_1 + h_1 , x_2 \le X_2(\om) < x_2 + h_2} = \\=
P \bc{\om:  X_1(\om) < x_1 + h_1 , X_2(\om) < x_2 + h_2}  - P \bc{\om: X_1(\om) < x_1, X_2(\om) < x_2 + h_2 } - \\-
P \bc{\om: X_1(\om) < x_1 + h_1 , X_2(\om) < x_2 } + P \bc{\om: X_1(\om) < x_1 , X_2(\om) < x_2 } = \\ =
F(x_1 + h_1, x_2 + h_2) - F(x_1 + h_1,x_2) - F(x_1 , x_2 + h_2) + F(x_1 , x_2) = \De_{h_1}^{(1)} \De_{h_2}^{(2)} F(x_1 , x_2).}
\hfill\end{proof}

\begin{df}
Пусть $\Kc$\т некоторый класс подмножеств из $X : \Kc \subs 2^X
$. Пересечение всех $\si$-алгебр, содержащих $\Kc$ называется $\si$-алгеброй,
порожденной $\Kc$. Обозначается как $\si(\Kc)$. В частности,
\emph{борелевской} $\si$-алгеброй называется $\si$-алгебра, порожденная всеми открытыми\footnote{Можно считать,
что это все открытые прямоугольники (так устроена топология в $\R^n$).}
множествами в $\R^n$. (Обозначается $\Bc(\R^n)$)
\end{df}

\begin{theorem}
Пусть $(X_1 , \ldots , X_n )$\т случайный вектор в $\R^n$. Пусть
$B$\т событие из $\si$-алгебры $\Bc (\R^n)$. Тогда $\bc{\om:
(X_1(\om),\ldots , X_n(\om)) \in B} \in \As$.
\end{theorem}

\begin{proof}
Обозначим через $\sum$ все $B \subs \R^n$ : для них выполнено
$\bc{\om: (X_1(\om),\ldots , X_n(\om)) \in B} \in \As$.

1) $\sum$ содержит все полуинтервалы в $\R^n$.



2) $\sum$ является $\si$-алгеброй. Тогда ясно, что $\Bc (\R^n) \subseq \sum$.

Пусть $X(\om)$\т тот самый случайный вектор. $X(\om) : =
(X_1(\om),\ldots , X_n(\om))$. Тогда $\bc{\om: (X_1(\om),\ldots ,
X_n(\om)) \in B}$ имеет вид $\bc{\om:X(\om)\in B}$.

1) $\R^n \in \sum : \bc{\om : X(\om) \in \R^n} = \Om \in \As$.

2) Пусть $\bc{\om:X(\om)\in B} \in \As$, тогда $\bc{\om: X(\om) \in
\ol{B}} = \bc{\om: X(\om) \not\in B}$, то есть если $B  \in \sum \im
\ol{B} \in \sum$.

3) Если $B_i \in \sum \im$ покажем, что $\cui{} B_i \in \sum$, $\cai{} B_i \in \sum$.

$\bc{\om: X(\om) \subs \cui{} B_i} = \cui{} \bc{\om:X(\om)\in
B_i} \in \As$, т.к. $\As$\т $\si$-алгебра. Аналогично доказывается и
для $\cai{} B_i$. тем самым доказано, что $\sum$\т
$\si$-алгебра.
\end{proof}

\begin{df}
$P_X (B) = P \bc{\om: X(\om) \in B}$ называется распределением
вероятности случайного вектора $X= (X_1, \ldots , X_n)$, либо
совместным распределением вероятности. Здесь $B \in \Bc(\R^n)$.
\end{df}

В частности, если $B=[x_1,x_1+h_1) \times [x_2,x_2+h_2) \times
\ldots \times [x_n,x_n + h_n)$, то $P_X (B) =
\De_{h_1}^{(1)} \ldots \De_{h_n}^{(n)} F(x_1,\ldots
,x_n)$. В силу теоремы о продолжении меры (о единственности), $P_X
(B)$ однозначно определяется по $F(x_1 , \ldots , x_n)$

\begin{df}
Совместное распределение $P_X (B)$ называется \emph{абсолютно непрерывным},
если $$P_X(B) = \ints{B} p(x)\dx,$$ где $B\in \Bc (\R^n)$, $p(x)$\т плотность совместного распределения, а $x$\т
$n$\д мерный вектор.
\end{df}

Если $B$\т полуинтервал, то
$$\De_h F(x) = \intl{x_1}{x_1 + h_1} \ldots \intl{x_n}{x_n + h_n} p(y_1 \ldots y_n) d y_1 \ldots d y_n.$$ Ясно, что при $n=1$ получается немного более широкое
определение, чем для одномерного классического случая (там для
полуинтервала из $\R^1$, а здесь \т для произвольного $B \in
\Bc(\R^1)$). Тогда :

1) $\frac{\partial^n F(x)}{\partial x_1 \ldots
\partial x_n} = p(x_1 \ldots x_n)$ для всех $(x_1 \ldots x_n)$. В
частности, для кусочно-непрерывных функций имеем это равенство во
всех точках непрерывности.

2) $ F(x_1 \ldots x_n) = \intl{-\infty}{x_1} \ldots
\intl{-\infty}{x_n} p(y_1 \ldots y_n) d y_1 \ldots d y_n$.

3)$P(x) \ge 0 $ почти всюду ( это следует из монотонности $F$ ), а
$ \intl{-\infty}{+\infty} \ldots \intl{-\infty}{+\infty} p(y_1
\ldots y_n) d y_1 \ldots d y_n =1 $\т единичная масса,
размазанная по всему пространству.

\subsubsection{Независимость случайных величин}

\begin{df}
Случайные величины $X_1 \ldots X_n$ называются независимыми, если
их совместное распределение обладает свойствами: $\fa B_1, \ldots
B_n \in \Bc (\R)$, имеем $P\bc{\om: X(\om) \in B_1 \times \ldots
\times B_n} = \prodl{j=1}{n} P(\om: X_j(\om) \in B_j)$.(н.1)
\end{df}

Упростим это определение: Пусть $ B_j = [x_j, x_j+h_j)$. Тогда
$\De_h F(x) = \prodl{j=1}{n} (\De_{h_j}^j
F_j(x_j))$ (н.2)

$F(x_1 \ldots x_n) = F_1(x_1) \cdot \ldots \cdot F_n(x_n)$ (н.3).

н.2 $\im$ н.1 следует из единственности продолжения меры

н.3 $\im$ н.2 несложно доказывается.

Получаем, что н.1 $\Leftrightarrow$ н.2 $\Leftrightarrow$ н.3

В частности если совместное распределение $X_1 \ldots X_n$
абсолютно непрерывно, то н.1 н.2 н.3 $\Leftrightarrow p(x_1 \ldots
x_n ) = p(x_1) \ldots p(x_n)$

\begin{ex}
1) Пусть $V\subs \R^n$\т ограниченное борелевское множество,
Пусть $p(x):= \frac{1}{\mes V} \chi(V) $, тогда $p\ge 0 $, и $\ints{\R^n} p \dx = 1 $, а
$P_X(B) = P \bc{\om: X(\om) \in B \cap V} = \frac{\mes (B \cap V)}{\mes V}$. Здесь $X = (X_1\sco X_n)$.
Это равномерное распределение на множестве $V$. Если $V=[a_1,b_1]\times \ldots \times [a_n,b_n]$, то $X_1 \ldots
X_n$\т независимые случайные величины.

2)Многомерное нормальное распределение:
$$p(t)=\frac{1}{\sqrt{2 \pi \si^2}} e^{-\frac{(t-m)^2}{2 \si^2}},$$
и тогда $p(x_1 \ldots x_n) = p_1(x_1) \ldots p_n(x_n)$.
\end{ex}


\begin{problem}
Пусть  $Y=f(X_1 \ldots X_n)$, $f$\т измеримая функция, а $X_1
\ldots X_n$ имеют плотность совместного распределения $p(x_1
\ldots x_n)$. Найти плотность распределения $Y$.
\end{problem}
\begin{solution}
$F_y(y) = P \bc{\om: Y(\om) < y} = P \bc{\om: f(X_1(\om),\ldots
X_n(\om)) < y}= P \bc{\om: (X_1(\om),\ldots , X_n(\om)) \in B_y}$,
где $B_y=\bc{(x_1,\ldots , x_n) : f(x_1 , \ldots , x_n) < y}$.
Тогда в силу абсолютной непрерывности
$$F_Y(y) = \ints{B_y} p(x_1, \ldots , x_n)\dx_1 \ldots\dx_n,$$ откуда $P_Y(y)=
\frac{dF_Y(y)}{dy} = \frac{d}{dy} \ints{B_y} p(x_1, \ldots ,
x_n)\dx_1 \ldots\dx_n$.
\end{solution}

\begin{ex}
Рассмотрим конкретный пример: Найти плотность и функцию
распределения $Y = X_1 + X_2$, где $X_1$ и $X_2$ имеют совместную
плотность $p(x_1,x_2)$.
\end{ex}
\begin{solution}
Имеем
\mln{F_Y(y) = P \bc{\om: X_1(\om) + X_2(\om) < y} = \ints{x_1 + x_2 < y} p(x_1,x_2)\dx_1\dx_2 = \\=
\intl{-\infty}{+\infty} \intl{-\infty}{y-x_1} p(x_1 , x_2)\dx_1\dx_2 = \intl{-\bes}{\bes}\intl{-\bes}{y}p(x_1,x_2-x_1)\dx_1\dx_2,}
откуда, дифференцируя по верхнему пределу, получаем $P_Y(y) = \intl{-\infty}{+\infty} p(x_1 , y-x_1)\dx_1$. Получаем свертку
функций с плотностями $p_1$, $p_2$ : $P_Y(y) =
\intl{-\infty}{+\infty} p_1(x_1) p_2 (y-x_1)\dx_1 $\т в случае
независимости $X_1$ и $X_2$.
\end{solution}

\subsection{Математическое ожидание случайных величин}

\subsubsection{Интеграл Лебега по вероятностной мере}

Пусть $X(\om)$\т случайная величина на $\p$ , и пусть пока $X$\т
дискретна $\im X$ принимает значения $\bc{x_1,x_2,\ldots} $ с
вероятностью $p_1 , p_2 , \ldots$

\begin{ex}
Математическим ожиданием дискретной случайной величины $X(\om)$
называется число $M\bc{X}\bw{=} E\bc{X} := \sums{i} x_i p_i$, при
условии абсолютной сходимости этого ряда.
\end{ex}

\begin{note}
$M$\т от слова <<Mean Value>>, $E$\т <<expectation>>.
\end{note}

\begin{note}
В случае $X(\om) \ge 0, (x_j \ge 0 , \fa j )$, если ряд
расходится, то $M\bc{X} = +\infty$.
\end{note}

Для сокращения $\sums{i} x_i p_i =\sums{x} x p_x$, где $p_x :=
P\bc{\om: X(\om) = x}$.

Вероятностный смысл $M\bc{X}$\т среднее ожидаемое значение
случайной величины. Если $n(x)$\т количество испытаний, в
которых происходит  выигрыш $x \im \ol{x} = \frac{1}{n}
\sums{x} x n(x) = \suml{x}{} x (\frac{n(x)}{n})    \approx
\sums{x} x p_x = M\bc{X}$, отметим что $(\frac{n(x)}{n})$\т
частота x.

\begin{ex}
$X = \case{1 , \text{ с вероятностью } p \\ 0 , \text{ с вероятностью } 1-p}
\im M\bc{X}=1p + 0 (1-p) = p$.
\end{ex}

\begin{ex}
Рассмотрим биномиальное распределение $X(\om) = m $ с вероятностью $p_m =
C_n^m p^m (1-p)^{n-m} , m=0,1,\ldots ,n$. Имеем
$M\bc{X}=\suml{m=0}{n} m C_n^m p^m (1-p)^{n-m} = \{$ выкладки в теореме Бернулли $\} = np$.
\end{ex}

\begin{ex}
Пуассоновское распределение с показателем $\la$ $X(\om) = m$,  $m = 0,1,2,\ldots$,
$$p_m = \frac{\la^m}{m!} e^{-\la} (\la > 0) , \quad M\bc{X} = \suml{m=0}{+\infty} m \frac{\la^m}{m!} e^{-\la} =
\suml{m=1}{+\infty} \frac{\la^m}{(m-1)!} e^{-\la} = \la
(\suml{m=0}{+\infty} \frac{\la^m}{m!}) e^{-\la} = \la.$$
\end{ex}

\begin{theorem}
(Основное свойство $M\bc{X}$) Пусть $Y = f(X_1 , \ldots , X_n)$,
где $X_1 ,\ldots, X_n$\т дискретные случайные величины. Тогда,
в предположении абсолютной сходимости ряда,
\eqn{M\bc{Y} = \sums{x_1, \ldots, x_m} f(x_1, \ldots ,x_m) P\bc{\om: X_1(\om) = x_1 , \ldots , X_m(\om) = x_m}.}
\end{theorem}
\begin{proof}
Докажем для $m=2$, в общем случае\т аналогично. По определению имеем
$$M\bc{Y} = \sums{y} y P\bc{\om: Y(\om) = y }, \quad y=f(x_1,x_2),$$
тогда
$$P\bc{\om: Y(\om) = y } = \sums{(x_1, x_2): f(x_1,x_2)= y} P\bc{\om: X_1(\om) = x_1 , X_2(\om) = x_2},$$
посмотрим, как через это можно записать $M\bc{Y}$:
\mln{\sums{y} f(x_1,x_2) \sums{(x_1 ,x_2):f(x_1,x_2) =y } P\bc{\om: X_1(\om) = x_1 , X_2(\om) = x_2} = \\
\sums{(x_1, \ldots, x_m)} f(x_1, \ldots ,x_m) P\bc{\om: X_1(\om) = x_1 , \ldots , X_m(\om) = x_m}.}
Поскольку ряд сходится абсолютно, то порядок сумм можно изменять.
\end{proof}

\begin{note}
Если $m=1 \im Mf(x) = \sums{X} f(x) P\bc{\om: X(\om)=x}$, тогда
$M\bc{|X|} = \sums{X} |x| P\bc{\om: X(\om)=x}$, то есть условие
абсолютной сходимости ряда для $M\bc{X} \lra M\bc{|X|} < +\infty $.
\end{note}

\subsubsection{Свойства математического ожидания}

\begin{theorem}
Пусть $M\bc{X}$ существует, тогда:

\begin{points}{-2}
\item $X(\om)\ge 0 \im M\bc{X} \ge 0$
\item Если $c=\const$\т неслучайная величина $\im M\bc{c}=c$;
\item $M\bc{(cX)}=cM\bc{X}$;
\item $M\bc{(X_1+X_2)}= M\bc{X_1} + M\bc{X_2}$;
\item Если $X_1 \le X_2 \im M\bc{X_1} \le M\bc{X_2}$. Кроме того, $|M\bc{X}| \le M\bc{|X|}$;
\item Если $X_1,\ldots X_m$ независимы $\im M(X_1 \ldots X_m)=MX_1 \cdot \ldots \cdot MX_m$.
\end{points}
\end{theorem}

\begin{proof}
\pt{1} Очевидно.

\pt{2} $Mc = c \cdot 1 $ (остальные члены ряда --- нули).

\pt{3} Применим теорему $f(x) = cx : M(cX) = \sums{x} (cx)
P\bc{\om:X(\om)=x} = cM\bc{X}$ (Константу мы просто вынесли из-под
знака суммы)

\pt{4}
\mln{M(X_1 + X_2) =\sums{(x_1,x_2)} (x_1 + x_2) P\bc{X_1(\om) = x_1 , X_2(\om) = x_2} = \\ =
\sums{(x_1,x_2)} x_1 P\bc{X_1(\om) = x_1 , X_2(\om) = x_2} + \sums{(X_1,X_2)} x_2 P\bc{X_1(\om) = x_1 , X_2(\om) = x_2} =
MX_1 + MX_2,}
так как $\sums{x_i} P(X_i(\om) = x_i) = 1 $.

\pt{5} Следует из того, что $X_2(\om) - X_1(\om) \ge 0 \im $ из 1)
$M\bc{X_2 - X_1} \ge 0$, по 3) 4) $MX_1 - MX_2 \ge 0$

Второе утверждение: имеем $-|X|\le X \le |X|$, поэтому $-M|X|\le M\bc{X} \le
M|X| \im |M\bc{X}| \le M|X|$.

\pt{6} Рассмотрим функцию $f(x_1 \ldots x_m) = x_1\sd x_m$. Имеем
\mln{M(X_1 \ldots X_m) = \sums{(x_1,\ldots x_m)} f(x_1\sco x_m) P\bc{X_1 = x_1, \ldots , X_m=x_m} \stackrel!=\\\stackrel!=
\sums{(X_1,\ldots X_m)} (x_1 \cdot \ldots \cdot x_m) P\bc{X_1 = x_1} \cdot  \ldots \cdot P\bc{X_m=x_m} \stackrel{!!}=
MX_1 \cdot \ldots \cdot MX_m.}

Переход, отмеченный <<!>>, верен в силу независимости величин $X_1,\ldots X_m$, а <<!!>>\т в силу абсолютной сходимости рядов.
\end{proof}

\begin{lemma}
Произвольная случайная величина $X(\om)$ на $\si$-алгебре может быть
равномерно аппроксимирована последовательностью дискретных
случайных величин $X_n(\om)$.
\end{lemma}

\begin{proof}
Покажем, что можно аппроксимировать линейную функцию. Пусть $x_k^{(n)}$\т разбиение $\R$. $\fa n = 1,2,\ldots $
пусть $\R~~x_k^{(n)} \in \R : \bc{x_k^{(n)} \uparrow}$ при
возрастании $k$, и $ 0 < x_{k+1}^{(n)} - x_k^{(n)} \le \ep_n$.
Введем случайную функцию:
$$\ph^{(n)} (x) \sums{k} x_k^{(n)} \chi_{[x_k^{(n)},x_{k+1}^{(n)}]},~k\in \Z \im$$
ясно,что $ 0 < x-\ph^{(n)} (x) \le \ep_n$, осталось положить
$X_n(\om):=\ph^{(n)}(X(\om))$\т это годится для любой случайной
величины, т.к. $x-\ph^{(n)} (x) \le \ep_n$.
\end{proof}

\begin{lemma}
Пусть $X_n(\om)$\т последовательность дискретных случайных
величин: $X_n \rra X $ и $ MX_n  $ существует для всех $n \im
$ существует $\lim MX_n$, и этот предел одинаков для $\fa
\bc{X_n}$.
\end{lemma}

\begin{proof}
$|MX_n - MX_m| \le M|X_n - X_m| \le M|X_n(\om) - X(\om)| +
M|X(\om) - X_m(\om)| \rightarrow 0$ (в силу равномерной
сходимости). То что предел единствен легко доказывается.
\end{proof}

\begin{df}
$ M\bc{X} := \lim MX_n $, если $X_n \rra X$.
\end{df}

Отсюда имеем
\mln{M\bc{X} = \liml{n \ra \infty} M\ph^{(n)} (X) = \liml{n \ra \infty} \sums{k} x_{k}^{(n)} P\bc{x_{k}^{(n)} \le
X(\om) \le x_{k+1}^{(n)}} = \\ =
\liml{n \ra \infty} \sums{k} x_{k}^{(n)} \hs{ F\br{x_{k+1}^{(n)}} - F\br{x_{k}^{(n)}}} =:
\intl{-\infty}{+\infty} x\,dF(x) \text {\т интеграл Лебега\ч Стилтьеса}.}

Свойства 1--6 имеют место для произвольных случайных величин, при
условие существования математического ожидания. Доказываются с
помощью предельного перехода от дискретных случайных величин.
Докажем свойство

4)Если $\exi M\bc{X}, M\bc{Y} \im M\bc{X} + M\bc{Y} =
M\bc{(X+Y)}$. В самом деле, найдутся дискретные величины
$X_n \rra X$, $Y_n \rra Y$. Тогда $MX_n \ra M\bc{X}; MY_n \ra MY$. Поскольку $X_n + Y_n \rra X + Y$, а
для дискретных это все выполняется, получаем что $M(X_n + Y_n ) =
MX_n + MY_n$. Следовательно $\exi \liml{n} M(X_n + Y_n) =:
M(X+Y)$, но по формуле он равен сумме пределов.

Пусть $X$\т случайная величина с функцией распределения $F(x)$.
Тогда
$$M\bc{X} = \liml{n} \suml{k = -\infty}{ + \infty} x_{k}^{(n)} \bs{F(x_{k+1}^{(n)}) - F(x_{k}^{(n)})},$$ здесь
$|x_{k}^{(n)} - x_{k+1}^{(n)}| \le \ep_n$, $\ep_n \ra 0$, и $x_{k}^{(n)} \ra \pm\infty$ при $k \ra \pm\infty$.
Тогда $M\bc{X} = \intl{-\infty}{+\infty} x\,dF(x)$.

\begin{problem}
Если $X$\т дискретная случайная величина, такая что событие $x_j$
происходит с вероятностью~$p_j$, то $\intl{-\infty}{+\infty}
x\,dF(x) = \sums{j} p_j x_j$.
\end{problem}

Рассмотрим случай абсолютно непрерывных случайных величин с
плотностью $p(x)$. Тогда
$$M\bc{X} = \intl{-\infty}{+\infty} xp(x)\dx.$$

\begin{theorem}
Пусть $\intl{+\infty}{-\infty} |x| p(x)\dx < \infty$. Тогда
$M\bc{X} = \intl{-\infty}{+\infty} xp(x)\dx$.
\end{theorem}

\begin{proof}
Приблизим $X$ равномерно случайными величинами: $\ph^{(n)} =
x_{k}^{(n)}$, если $x \in [x_{k}^{(n)},x_{k+1}^{(n)})$. Докажем,
что $M\ph^{(n)} (x)$ существует и стремится к
$\intl{-\infty}{+\infty} xp(x)\dx$. В самом деле,
\eqn{M\ph^{(n)} (x) = \suml{k = -\infty}{+\infty} x_{k}^{(n)} P\bc{x_{k}^{(n)} \le
X(\om) < x_{k+1}^{(n)}}, P\bc{x_{k}^{(n)} \le X(\om) <
x_{k+1}^{(n)}} = \intl{x_{k}^{(n)}}{x_{k+1}^{(n)}} p(x)\dx.}
Тогда

\eqn{\suml{k = -\infty}{+\infty} |x_{k}^{(n)}|
\intl{x_{k}^{(n)}}{x_{k+1}^{(n)}} p(x)\dx   \le \suml{k =
-\infty}{+\infty} \intl{x_{k}^{(n)}}{x_{k+1}^{(n)}} (|x| +\ep_n )
p(x)\dx =\intl{-\infty}{+\infty} (|x| +\ep_n ) p(x)\dx  =
\intl{-\infty}{+\infty} |x| p(x)\dx +\ep_n.}
Значит сходится.
\end{proof}

Теперь покажем совпадение математических ожиданий. Имеем
\eqn{M\ph^{(n)}(x) = \suml{k=-\infty}{+\infty} \intl{x_{k}^{(n)}}{x_{k+1}^{(n)}} \ph^{(n)} (x) p(x)\dx  =
\intl{-\infty}{+\infty} \ph^{(n)} (x) p(x)\dx,}
а потому
\eqn{\bm{M\ph^{(n)}(x)   - \intl{-\infty}{+\infty} x p(x)\dx} = \intl{-\infty}{+\infty} (\ph^{(n)}(x) -x) p(x)\dx \le
\intl{-\infty}{+\infty} \bm{\ph^{(n)} (x) -x} p(x)\dx \le \ep_n
\intl{-\infty}{+\infty} p(x)\dx \ra 0 (n\ra\bes).}

\begin{ex}
Равномерное распределение: $p(x) = \case{\frac{1}{b-a} , x\in
[a,b] \\ 0 , x \not\in [a,b]}$  $$M\bc{X} = \frac{1}{b-a}
\intl{a}{b} xdx = \frac{a+b}{2}$$.
\end{ex}

\begin{ex}
Показательное распределение: $p(x) =\case{ \la e^{-\la x } , x \ge
0 \\ 0 , x< 0}$, причем $\la > 0$. $$M\bc{X} = \intl{0}{+\infty} x
\la e^{-\la x}\dx = \la^{-1}.$$
\end{ex}

\begin{ex}
$p(x) = \frac{1}{\sqrt{2\pi \si^2}} e^{ -
\frac{(x-m)^2}{2\si^2}}$.  $$M\bc{X} =
\intl{-\infty}{+\infty}\frac{x}{\sqrt{2\pi \si^2}} e^{ -
\frac{(x-m)^2}{2\si^2}} \ = m.$$
\end{ex}

\begin{theorem}
Пусть $Y = f(x_1 , \ldots, x_n)$ $f$ --- измерима. $X_1 \ldots X_n$
имеют плотность совместного распределения $p(x_1 \ldots x_n)$.
Тогда
\eqn{MY = \intl{-\infty}{+\infty} \ldots \intl{-\infty}{+\infty} f(x_1 , \ldots, x_n) p(x_1 \ldots x_n)\dx_1 \ldots x_n}
(при условной сходимости интеграла).
$$Mf(x) = \intl{-\infty}{+\infty} f(x) p(x)\dx.$$
\end{theorem}

\begin{proof}
\mln{MY = \liml{n\ra\bes} M\ph^{(n)} (Y) =
\liml{n\ra\bes} \suml{k=-\infty}{+\infty} y_{k}^{(n)} P\bc{\om: y_{k}^{(n)} \le Y(\om) < y_{k+1}^{(n)}} =\\=
\liml{n\ra\bes} \suml{k=-\infty}{+\infty} y_{k}^{(n)} P\bc{\om: y_{k}^{(n)} \le f(X_1(\om) , \ldots , X_n(\om)) < y_{k+1}^{(n)}} = \\
\liml{n\ra\bes} \suml{k=-\infty}{+\infty} y_{k}^{(n)} \mathop{\int \ldots\ldots\ldots\int}\limits_{(x_1\sco x_n): f(x_1, \ldots,  x_m) \in [y_{k}^{(n)},y_{k+1}^{(n)}]}
p(x_1, \ldots, x_m)\dx_1 \ldots\dx_m = \\=
\liml{n\ra\bes} \intl{-\infty}{+\infty} \ldots \intl{-\infty}{+\infty} \ph^{(n)}
(f(x_1, \ldots, x_n)) p(x_1, \ldots, x_m)\dx_1 \ldots\dx_m = \intl{-\infty}{+\infty} \ldots \intl{-\infty}{+\infty} f(x_1,
\ldots, x_n) p(x_1, \ldots, x_m)\dx_1 \ldots\dx_m.}
 Остается
только оценить разность интегралов: это делается абсолютно
также:
$$\Bm{\int f p - \int \ph^{(n)} p} \le \int \bm{f - \ph^{(n)}} p \le \ep_n \cdot 1.$$
\hfill\end{proof}

\subsection{Дисперсия. Неравенство Чебышева. Закон больших чисел}

\subsubsection{Дисперсия и моменты}

\begin{df}
Абсолютным моментом $k$---ого порядка для случайной величины $X$
называется $M|X|^{k}, k \in \N$.
\end{df}

Если $M|X|^{k} < \infty \im$ $k$---й момент равен $MX^{k}$

\begin{df}
Центральным $k$-м моментом называется $M(X- MX)^{k}$.
\end{df}

\begin{df}
Дисперсией случайной величины $X$ называется $DX = M(X - MX)^{2}$.
\end{df}

\begin{stm}
Если $MX^2 < \infty $, то существует $MX$ и $DX < \infty$, причем
$DX = MX^2 - (MX)^2$.
\end{stm}
\begin{proof}
Имеем $(|x| - 1)^2 \ge 0$, поэтому $|x| \le \frac{1}{2} +
\frac{1}{2} |x|^2 , MX^2 < \infty \im M|X| < \infty \im \exi MX
\im DX = M(x^2 - 2XMX +(MX)^2) = MX^2 - 2(MX)^2 + (MX)^2 = MX^2 -
(MX)^2$.
\end{proof}

Пусть $X$\т дискретная случайная величина, принимающая значение
$x_j$ с вероятностью $p_j$. Тогда $D\bc{X} = \sum (x_j -
M\bc{x})^2 p_j$

Пусть $X$\т абсолютно непрерывная случайная величина с плотностью
$p(x)$. Тогда
$$D\bc{X} = \intl {-\infty}{+\infty} (x - MX)^2 p(x) \dx =
\intl {-\infty}{+\infty} x^2 p(x)\dx - \bbr{\intl {-\infty}{+\infty} x p(x) \dx}^2 $$

\begin{ex}
$x = \case{1 ~с~вероятностью~p \\ 0 ~с~вероятностью ~1-p}$

Тогда $D\bc{X} =  p (1-p)$

\end{ex}

\begin{ex}
Пуассоновское распределение. $x_j = j$ с вероятностью $p_j =
\frac{\la ^ j }{j!} e^{-\la} , j=0,1,\ldots , \la > 0 $

Тогда
$$D\bc{X} = \suml{k=0}{+\infty} k^2 \frac{\la ^k}{k!} e^{-\la} - \la^2 = \la.$$
\end{ex}


\begin{ex}
Нормальное распределение.

$$D\bc{X} = \intl{-\infty}{+\infty} \frac{1}{\sqrt{2 \pi \si^2}}
(x-m)^2 e^{-\frac{(x-m)^2}{2\si^2}} = \si^2.$$
\end{ex}

\subsubsection{Неравенство Чебышева}

\begin{theorem}[Неравенство Чебышева]
Пусть $X$\т случайная величина, $M\bc{X^2} < \infty$. Тогда для любого $\ep > 0$
выполняется неравенство
$$ P\bc{\om: |X(\om) - M\bc{X}| \ge \ep} \le \frac{D\bc{X}}{\ep^2}.$$
\end{theorem}

\begin{proof}
Положим $\ph(x) := \case{0 , & |x|< \ep \\ \ep , &|x| \ge \ep}$. Тогда
$\ph(x)^2 \le x^2$, поэтому
$$\ph(X(\om)) = \case{0 , & \text{ с вероятностью } P\bc{\om: |X(\om)| < \ep} \\
\ep , &\text{ с вероятностью } P\bc{\om: |X(\om)| \ge \ep}.}$$
Тогда $ M\bc{\ph(x)^2} = 0 \cdot P\bc{\om: |X(\om)| <
\ep}  + \ep^2\cdot P\bc{\om: |X(\om)| \ge \ep} $ Так как
$\ph^2(X(\om)) \le X^2(\om)$, то
$$M\bc{\ph^2(x)} \le M\bc{X^2}, \quad \ep^2 P\bc{\om: |X(\om)| \ge \ep} \le M\bc{X^2}.$$
Осталось заменить $X$ на $X - M\bc{X}$.
\end{proof}

\begin{df}
$X(\om) = Y(\om) $ с вероятностью 1, если $P\bc{\om: X(\om)=
Y(\om)} = 1 $ (в случае меры Бореля-Лебега $X = Y$ п.в.)
\end{df}

%%%%%%%%
% Additions by S.K.
%%%%%%%%

\begin{theorem}
В предположении конечности дисперсий:
\begin{enumerate}
\item $DX \geqslant 0$, причём $DX = 0 \Leftrightarrow X(\omega) = MX$
с вероятностью 1;
\item $D(cX) = c^2 DX$, где $c$ --- неслучайная величина;
\item Если $X_1, \dots, X_n$ --- попарно независимые случайные величины,
то $D(X_1 + \dots + X_n) = DX_1 + \dots + DX_n$.
\end{enumerate}
\end{theorem}

\begin{proof}
\begin{enumerate}
\item $DX \geqslant 0$ --- очевидно.

Пусть $0 = DX = M(X-MX)^2$. По неравенству Чебышёва:
$$
P\left\{\omega: |X(\omega) - MX| \geqslant \frac 1n\right\} = 0 \quad \forall n
$$
$$
\{\omega: X(\omega) \ne MX \} = \cupl{n=1}{\infty} \left\{|X-MX| \geqslant \frac 1n\right\}
$$
По свойству непрерывности вероятности:
$$
P\{\omega: X(\omega) \ne MX \} = \lim_{n\rightarrow\infty} P\left\{\omega :
|X(\omega) - MX| \geqslant \frac 1n\right\} = 0
$$
$$
P\{\omega: X(\omega) = MX\} = 1 - P\{X \ne MX\} = 1
$$
$X(\omega) = MX$ почти наверное (с вероятностью 1).

\item очевидно
\item Аддитивность:
$|XY| \leqslant \frac 12 (X^2 + Y^2)$, $(X+Y)^2 \leqslant 2(X^2 + Y^2)$. Отсюда
$DX, DY < \infty \Ra D(X+Y) < \infty$.

\mln{D(X_1 + \dots + X_n) = M\left(\suml{i=1}{n} X_i - M\suml{i=1}{n} X_i\right)^2
 = M\left(\suml{i=1}{n} (X_i - MX_i)\right)^2 =\\=
 M\left(\suml{i=1}{n}(X_i - MX_i)\suml{j=1}{n} (X_j - MX_j)\right) = \suml{i,j=1}{n} M\bigl((X_i - MX_i)(X_j - MX_j)\bigr) =\\=
 \suml{i=1}{n} DX_i + \sums{i\ne j} M\bigl((X_i - MX_i)(X_j - MX_j)\bigr) = \suml{i=1}{n}DX_i.}
Последнее равенство верно в силу независимости. На самом деле требуется, чтобы
ковариации
$$\cov(X,Y) := M\br{(X-MX)(Y-MY)}$$
были равны нулю (что верно для независимых случайных величин, но условие независимости не является необходимым).
\end{enumerate}
\end{proof}

\subsubsection{ЗБЧ (Закон больших чисел)}

\begin{theorem} Пусть $X_1, \dots, X_n, \dots$ --- последовательность попарно
независимых случайных величин, $M(X_i^2) < \infty$, $m = MX_i$, $\si^2 =
DX_i$ (математические ожидания и дисперсии одинаковы). Рассмотрим выборочное
среднее: $$\vsr = \frac 1n \suml{i=1}{n} X_i.$$ Тогда
$$ \lim_{n\rightarrow\infty} P\left\{ \left| \vsr - m \right| \geqslant \varepsilon
\right\} = 0 \qquad (\varepsilon > 0).$$
\end{theorem}

Говорят, что $\overline{X}^{(n)}$ стремится к $m$ по вероятности.

\begin{proof}
$$ M\vsr = \frac 1n \suml{i=1}{n} MX_i = m$$
$$ D\vsr = \frac 1{n^2} \suml{i=1}{n} DX_i = \frac {\si^2}{n}$$
В силу неравенства Чебышёва:
$$ P\left\{ \omega : \left| \vsr(\omega) - m \right| \geqslant \ep \right\} \leqslant
\frac {\si^2}{n\ep^2} \underset{\nrai}{\longrightarrow} 0.
$$
\end{proof}

ЗБЧ\т широкое обобщение теоремы Бернулли.

\begin{ex}
Схема Бернулли.
$$
X_i(\omega) =
\begin{cases}
1 & \text{с вероятностью $p$} \\
0 & \text{с вероятностью $(1-p)$}
\end{cases}
\quad \text{--- независимы и одинаково распределены}
$$

$$
\vsr(\omega) = \frac 1n \suml{i=1}{n} X_i(\omega) = \nu \quad \text{--- выборочная
частота успехов}
$$

$$
m = MX_i = p, \qquad \si^2 = DX_i = p(1-p)
$$

$$
P\left\{\omega: |\nu(\omega) - p| \geqslant \ep \right\} \leqslant
\frac{p(1-p)}{n\ep^2} \underset{\nrai}{\longrightarrow} 0
$$

Точно такую оценку мы получили раньше.
\end{ex}

\subsubsection{Применение в статистике}

$X_1, \dots, X_n, \dots$ --- независимые с функцией распределения $F(x)$.
$F(x)$ неизвестна и её надо оценить, наблюдая $X_i$. Выборочная функция
распределения:

$$
F_n(x) = \frac 1n \left(\text{количество тех $X_j$, $j=1,\dots,n$, для которых $X_j < x$}\right) =
\frac 1n N(x)
$$

$$
p = P\left\{ \omega : X(\omega) < x \right\} = F(x)
$$

$$
\forall x \quad F_n(x) \underset{\nrai}{\longrightarrow} F(x) \quad \text{по вероятности}
$$

\section{Характеристические функции}

\subsection{Определения и примеры}


\begin{df} Комплексной случайной величиной называется $Z(\om) = X(\om) + iY(\om)$, где
$X$ и $Y$ --- вещественные случайные величины на $\p$.
\end{df}

$$ MZ := MX + iMY \quad \text{(при условии существования $MX$ и $MY$)} $$

Для комплексных случайных величин имеют место обычные свойства мат. ожидания.

\begin{lemma}
$ |MZ| \leqslant M|Z| $.
\end{lemma}

\begin{proof}
Для дискретных комплексных случайных величин: $Z$ принимает значение $z_k$
с вероятностью $p_k$.\newline $MZ = \sums{k} z_k p_k$.
В силу обобщённого неравенства треугольника
$$
|MZ| = \left| \sums{k} z_k p_k \right| \leqslant \sums{k} |z_k| p_k = M|Z|.
$$
Произвольную комплексную случайную величину можно равномерно аппроксимировать
дискретными.
\end{proof}

\begin{df} Характеристической функцией вещественной случайной величины $X$ называется
$$
\ph(t) = Me^{itX}, \quad t\in\R.
$$
\end{df}

$$
e^{i\alpha} = \cos \alpha + i\sin \alpha, \quad \alpha\in\R \quad
\text{(формула Эйлера)}
$$

Отсюда
$$
\left| e^{i\alpha} \right| = \sqrt{\cos^2 \alpha + \sin^2 \alpha} = 1
$$

Следовательно, $\ph(t) = Me^{itX}$ существует при всех $t\in\R$.

$$
\ph(t) = M(\cos tX) + iM(\sin tX)
$$

Пусть $X$ имеет функцию распределения $F$. Тогда для любой борелевской функции $f$ такой, что
$Mf(X)$ существует:
$$
Mf(X) = \int\limits_{-\infty}^{+\infty} f(x) \,dF(x)
\quad \text{--- интеграл Лебега\ч Стилтьеса}
$$

$$
\ph(t) = \int\limits_{-\infty}^{+\infty} e^{itx}\,dF(x) =
\int\limits_{-\infty}^{+\infty} \cos tx \, dF(x) +
i \int\limits_{-\infty}^{+\infty} \sin tx \, dF(x)
$$

\begin{tabular}{p{.45\textwidth}|p{.45\textwidth}}
$X$ --- дискретная случайная величина (принимает значение $x_k$ с
вероятностью $p_k$).
$$ \ph(t) = \sums{k} e^{itx} p_k = \sums{k} (\cos tx_k) p_k +
i \sums{k} (\sin tx_k) p_k $$
--- суперпозиция комплексных гармоник.
&
$X$ имеет абсолютно непрерывное распределение с плотностью $p(x)$.
$ dF(x) = p(x)\dx $
$$ \ph(t) = \int\limits_{-\infty}^{+\infty} e^{itx} p(x) \dx =$$$$=
\int\limits_{-\infty}^{+\infty} (\cos tx) p(x) \dx +
i \int\limits_{-\infty}^{+\infty} (\sin tx) p(x) \dx
$$
--- комплексное преобразование Фурье для $p(x)$.
Преобразование Фурье обратимо: по $\ph(t)$ можно восстановить $p(x)$.
$\ph(t)$ --- равнозначный способ задания случайной величины.
\end{tabular}

\begin{ex}
$
x =
\begin{cases}
1 & \text{ с вероятностью } p \\
0 & \text{ с вероятностью } (1-p)
\end{cases}
\quad \text{--- случайный бит.}
$\qquad
$ \ph(t) = (1-p) + e^{it} \cdot p $.
\end{ex}

\begin{ex}
Пуассоновское распределение.
$X: n = 0, 1, 2, \dots$ с вероятностью $p_k = \frac{\lambda^n e^{-\lambda}}{n!}$.
$$
\ph(t) = \suml{n=0}{\infty} e^{itn} \frac{\lambda^n e^{-\lambda}}{n!} =
e^{-\lambda} \suml{n=0}{\infty} \frac {(\lambda e^{it})^n}{n!} =
e^{\lambda(e^{it}-1)}
$$
\end{ex}

\begin{ex}
Стандартное нормальное распределение.
$$
p(x) = \frac 1 {\sqrt{2\pi}} e^{-\frac{x^2}{2}}
$$
$$
\ph(t) = \frac 1 {\sqrt{2\pi}} \intl{-\infty}{+\infty} e^{itx} e^{-\frac{x^2}{2}}
\dx =
\frac 1 {\sqrt{2\pi}} \intl{-\infty}{+\infty} \cos tx \, e^{-\frac{x^2}{2}} \dx
\quad \text{(для $\sin$ получится ноль)}
$$
$$
\ph'(t) = \frac 1 {\sqrt{2\pi}} \intl{-\infty}{+\infty} (-x\sin tx) e^{-\frac{x^2}{2}} \dx =
\frac 1 {\sqrt{2\pi}} \intl{-\infty}{+\infty} \sin tx \, d\!\left(e^{-\frac{x^2}{2}}\right) =
\underbrace{
\left. \frac 1 {\sqrt{2\pi}} \sin tx \, e^{-\frac{x^2}{2}} \right|_{-\infty}^{+\infty}
}_{0}
- \, t
\underbrace{
\frac 1 {\sqrt{2\pi}} \intl{-\infty}{+\infty} e^{-\frac{x^2}{2}} \cos tx \dx
}_{\ph(t)}
$$
$$ \ph'(t) = -t\ph(t) \quad \text{--- линейное дифференциальное уравнение} $$
$$ \ph(t) = Ce^{-\frac{t^2}{2}} ; \quad
 \ph(0) = \intl{-\infty}{+\infty} p(x) \dx = 1 \Ra C = 1 \Ra \ph(t) = e^{-\frac{t^2}{2}} $$
$$ p(x) = \frac 1 {\sqrt{2\pi}} \ph(x).$$
\end{ex}

\subsection{Свойства характеристических функций}

\begin{theorem}[Основные свойства характеристических функций]
\begin{points}{-2}
\item $\ph(0) = 1$, $|\ph(t)| \leqslant 1 \,\, \forall t \in \R$ (нормировка);
\item $\ph(t)$ является положительно определённой функцией, то есть
$$\suml{k,j=1}{n} \overline{c_k} c_j \ph(t_j - t_k) \geqslant 0$$
для всех $n \in \N$  и любых наборов $\hc{t_1, \dots, t_n} \subs \R$, $\hc{c_1, \dots, c_n} \subs \Cbb$;
\item $\ph(t)$ равномерно непрерывна по $t \in \R$;
\item $Y(\om) = aX(\om) + b$ (линейное преобразование) $\Ra$ $\ph_Y(t) = e^{itb} \ph_X(at)$;
\item Если $Y(\om) = X_1(\om) + \dots + X_n(\om)$, $X_1, \dots, X_n$ независимы, то
$\ph_Y(t) = \ph_{X_1}(t) \sd \ph_{X_n}(t)$.
\end{points}
\end{theorem}

\begin{proof}
\begin{enumerate}
\item $ \ph(t) = Me^{itX} $. $\ph(0) = Me^{i\cdot 0\cdot X} = M1 = 1$.
$\forall t \quad |\ph(t)| = \left|Me^{itX}\right| \leqslant M\left|e^{itX}\right| = M1 = 1$.
\item
$$ 0 \leqslant M\left| \suml{j=1}{n} c_j e^{it_jX} \right|^2 =
M\overline{\left(\suml{k=1}{n} c_k e^{it_kX} \right)}
\left( \suml{j=1}{n} c_j e^{it_jX} \right) =
\suml{k,j=1}{n} \overline{c_k} c_j M \underbrace{e^{-it_kX + it_jX}}_{e^{i(t_j-t_k)X}} =
\suml{k,j=1}{n} \overline{c_k} c_j \ph(t_j - t_k)
$$

\item\par
\begin{lemma} $\left| e^{i\alpha} - 1 \right| = 2\left| \sin \frac \alpha 2 \right|, \;
\alpha \in \R$.
\end{lemma}
\begin{proof}
$$ \left| e^{i\alpha} - 1 \right| = \sqrt{ (\cos \alpha - 1)^2 + \sin^2 \alpha } =
\sqrt{1 - 2\cos\alpha + \cos^2\alpha + \sin^2\alpha} =
\sqrt{2(1-\cos\alpha)} = \sqrt{4\sin^2\frac\alpha 2} = 2\left|\sin\frac \alpha 2\right|
$$
\end{proof}

$$|\ph(t) - \ph(s)| = \left| Me^{itX} - Me^{isX} \right| =
\left| Me^{itX} \left( 1 - e^{i(s-t)X} \right) \right| \leqslant
M\left| e^{itX} \left( 1 - e^{i(s-t)X} \right) \right| =
M\left| e^{i(s-t)X} - 1 \right| =$$$$= \{ \text{в силу леммы} \} =
2M\biggl| \sin \frac{\overbrace{(s-t)}^{\Delta} X}{2} \biggr|
\overset{(?)}{\ra} 0 \; (\Delta \ra 0)
$$
Обоснуем предельный переход $(?)$.
$f_\Delta(\om) = \left| \sin \frac{\Delta \cdot X(\om)}{2} \right|$.
$|f_\Delta(\om)| \leqslant 1$ --- равномерное ограничение.\newline
$\forall \text{ фикс. } \om \in \Omega \quad f_\Delta(\om) \ra 0 \; (\Delta \ra 0)$ ---
мажорированная сходимость.


\subsection{Теорема Лебега и ее трагические последствия}

\begin{theorem}[Лебега о мажорированной сходимости]
Если $X_n(\om) \ra X(\om)$ для всех $\om$, причём
$$|X_n(\om) \bw{-} X(\om)| \leqslant g(\om),$$
где $Mg(\om) < \infty$, то $MX_n \ra MX$.
\end{theorem}
(Это теорема из курса действительного анализа)

Поэтому $|\ph(t) - \ph(s)| \ra 0 \; (\Delta \ra 0)$.

\item $\ph_Y(t) = Me^{itY} = Me^{it(aX+b)} = M\bigl( e^{itaX} \overbrace{e^{itb}}^{\const}
\bigr) = e^{itb} Me^{i(ta)X} = e^{itb} \ph_X(at)$.

\begin{ex}  Характеристическая функция $\Ns(m, \si^2)$.\newline
$p(x) = \frac 1 {\sqrt{2\pi\si^2}} e^{-\frac{(x-m)^2}{2\si^2}}$.\newline
$X \sim \Ns(0,1) \Ra Y(\om) = \si X(\om) + m \sim \Ns(m, \si^2)$.\\
$\ph_X(t) = e^{-\frac{t^2}{2}} \Ra \ph_Y(t) = e^{itm - \frac{\si^2 t^2}{2}}$.\\
При $\si^2 = 0$: $\ph_Y(t) = e^{itm}$ --- характеристическая функция неслучайной
величины $X(\om) \equiv m$ (а плотность при $\si \ra 0$ не существует).
\end{ex}

\item $X_1, \dots, X_n$ --- независимые случайные величины $\Ra$ $e^{itX_1}, \dots,
e^{itX_n}$ --- тоже независимые.
$$
\ph_Y(t) = Me^{it(X_1+\dots+X_n)} = M\!\left(e^{itX_1}\dots e^{itX_n}\right) =
 \left(Me^{itX_1}\right)\dots
\left(Me^{itX_n}\right) = \ph_{X_1}(t) \dots \ph_{X_n}(t)
$$
\end{enumerate}
\end{proof}

\begin{df}
Моментом $n$-го порядка случайной величины $X$ называется $MX^n$ (если существует).
Абсолютным моментом $n$-го порядка называется $M|X|^n$ (может быть $= \infty$).
\end{df}

\begin{theorem}
Пусть $M|X|^m < \infty$. Тогда $\ph(t)$ $m$ раз дифференцируема при всех $t \in \R$,
причём $\ph^{(k)}(0) \bw{=} i^k MX^k$, $k \in \{1,\dots,m\}$. Имеет место разложение
в ряд Тейлора: $$ \ph(t) = \suml{k=0}{m} \frac{(it)^k}{k!} MX^k + o(t^m) \text{ при } t\ra 0.$$
\end{theorem}

\begin{proof}
$\ph^{(k)}(t) = i^k Me^{itX} (x)^k, \quad k\in \{0, \dots, m\}$ ---
получается формальным дифференцированием под знаком математического ожидания.
Нужно обосновать законность такого дифференцирования.

$$|X|^k \leqslant |X|^m + 1 \quad (k\in\{1,\dots,m\})\quad \Ra \quad M|X|^k < \infty$$

Индукция по $k$. При $k = 0$ формула тривиальным образом верна. Пусть верно для
$k < m$. Докажем для $k+1$.

$$
\left| \frac{\ph^{(k)}(t + \Delta) - \ph^{(k)}(t)}{\Delta} - i^{k+1}Me^{itX}(X)^{k+1}\right| =
\hc{\text{предп. инд.}} =
\left| \frac{i^k M(X^k) \left(e^{i(t+\Delta)X} - e^{itX}\right)}{\Delta} -
i^{k+1} MX^{k+1} e^{itX} \right| =$$$$=
\left| MX^k \left(\left( \frac{e^{i(t+\Delta)X} - e^{itX}}{\Delta} \right) -
iXe^{itX} \right) \right| \leqslant
M|X|^k \underbrace{\left| \frac{e^{i\Delta X} - 1}{\Delta} - iX \right|}_{Y_\Delta}
$$

Нужно показать, что $|Y_\Delta(\om) \leqslant C|X|$, $Y_\Delta(\om) \ra 0 \; \forall \om$.
Тогда мы сможем воспользоваться теоремой о мажорированной сходимости. (Тогда
$\bigl| |X|^k Y \bigr| \leqslant c|X|^{k+1}$.) $Y_\Delta \ra 0$ поточечно (т.к.
$\left(e^{i\Delta X}\right)'_{\Delta = 0} = iX$).

$$
\left| \frac{e^{i\Delta X} - 1}{\Delta} - iX \right| \leqslant
\left| \frac{e^{i\Delta X} - 1}{\Delta} \right| + |X| =
\frac{2\left| \sin \frac{\Delta X}{2} \right|}{|\Delta|} + |X|
\leqslant
\frac{2\left| \frac{\Delta X}{2} \right|}{|\Delta|} + |X| =
2 |X|
$$
\end{proof}


\subsection{Формула обращения}

\begin{theorem}[Формула обращения] Пусть $\ph(t)$ --- характеристическая
функция случайной величины $X$ с функцией распределения $F(x)$. Тогда для
любых точек непрерывности $x_1 < x_2$ функция распределения $F(x)$ удовлетворяет
условию:
$$
F(x_2) - F(x_1) = \lim_{\si \ra 0} \frac{1}{2\pi}
\underbrace{
\intl{-\infty}{+\infty} \ph(t) e^{-\frac{\si^2 t^2}{2}}
\frac{e^{-itx_2} - e^{-itx_1}}{-it} \, dt
}_{\mbox{\rm(\textasteriskcentered)}}
$$
\end{theorem}

Докажем эту теорему мы несколько позже, а пока займёмся следствиями.

\begin{lemma}[Следствие 1] Характеристическая функция однозначно определяет функцию
распределения $F(x)$. \quad $\ph(t) \leftrightarrow F(x)$.
\end{lemma}

\begin{lemma}[Следствие 2] Пусть характеристическая функция $\ph(t)$ интегрируема
(по Лебегу) на $\R$. Тогда $F(x)$ абсолютно непрерывна, и распределение имеет плотность
$$
p(x) = \frac{1}{2\pi} \intl{-\infty}{+\infty} e^{-itx} \ph(t) \, dt.
$$
(формула обратного преобразования Фурье)
\end{lemma}

\begin{proof}\textbf{(Доказательство следствия 2)}
$$
\ph(t) e^{-\frac{\si^2 t^2}{2}} \frac{e^{-itx_2} - e^{-itx_1}}{-it}
\underset{\si \ra 0}{\longrightarrow}
\ph(t) \frac{e^{-itx_2} - e^{-itx_1}}{-it}
\quad \forall t \in \R
\quad \text{(поточечная сходимость)}
$$

$$
\left| \ph(t) e^{-\frac{\si^2 t^2}{2}} \frac{e^{-itx_2} - e^{-itx_1}}{-it} \right| =
|\ph(t)| \left| e^{it(x_1 - x_2)} - 1 \right| \frac{e^{-\frac{\si^2 t^2}{2}}}{|t|}
\leqslant |\ph(t)| \frac{2|\sin(t(x_1 - x_2)/2)|}{|t|}
\leqslant \underbrace{|\ph(t)| \cdot |x_1 - x_2|}_{\text{интегрируемая мажоранта}}
$$
По теореме Лебега можно перейти к пределу под знаком интеграла:
$$
F(x_2) - F(x_1) = \frac{1}{2\pi} \intl{-\infty}{+\infty} \ph(t)
\frac{e^{-itx_2} - e^{-itx_1}}{it} \, dt =
\frac{1}{2\pi} \intl{-\infty}{+\infty} \ph(t)
\left(\intl{x_1}{x_2} e^{-ity} \dy\right) \, dt
$$
По теореме Фубини меняем порядок интегрирования:
$$
F(x_2) - F(x_1) = \dots = \intl{x_1}{x_2} \left( \frac{1}{2\pi} \intl{-\infty}{+\infty}
e^{-ity} \ph(t) \, dt \right) \dy,
$$
откуда $F$ абсолютно непрерывна, и $p(y) = \frac{1}{2\pi} \intl{-\infty}{+\infty}
e^{-itx} \ph(t) \, dt$.

\end{proof}

\begin{proof}\textbf{(Доказательство формулы обращения)}
Пусть $X$ --- случайная величина с функцией распределения $F(x)$ и характеристической
функцией $\ph(x)$. Рассмотрим сглаженную случайную величину $X + Y_{\si}$, где
$Y_{\si} \sim \Ns(0, \si^2)$, $X$ и $Y$ независимы. Характеристическая функция
$X + Y_{\si}$ есть $\ph(t) e^{-\frac{\si^2 t^2}{2}} = \ph_\si(t)$.
Функция $\ph(t)$ превращается в интегрируемую функцию $\ph_\si(t)$.

$p_\si(x)$ --- плотность распределения $X + Y_\si$. Докажем формулу обращения
для плотности:
$$
p_\si(x) = \frac{1}{2\pi} \intl{-\infty}{+\infty}
e^{-itx} \ph_\si(t) \, dt
$$
Далее получим:
$$
F_\si(x_2) - F_\si(x_1) = \intl{x_1}{x_2} p_\si(t) \, dt
= \frac{1}{2\pi} \underbrace{\intl{-\infty}{+\infty} \dots}_%
{\mbox{\rm(\textasteriskcentered)}}
$$
%Кроме того, $F_\si(x_{1,2}) \ra F(x_{1,2})$ ($\si \ra 0$).
В силу обобщения формулы свёртки:
$$
p_\si(x) = \intl{-\infty}{+\infty} \frac{1}{\sqrt{2\pi\si^2}}
e^{-\frac{(y-x)^2}{2\si^2}} \, dF(y)
$$
С другой стороны,
\begin{multline*}
\frac{1}{2\pi} \underbrace{
\intl{-\infty}{+\infty} e^{-itx} \ph_\si(t) \, dt
}_{\text{существует (инт. Лебега)}}
= \frac{1}{2\pi} \intl{-\infty}{+\infty} e^{-itx}
e^{-\frac{\si^2 t^2}{2}} \underbrace{\intl{-\infty}{+\infty} e^{ity} \, dF(y)}_{\ph(t)} \, dt
= \{\text{теорема Фубини}\} =\\=
\intl{-\infty}{+\infty} \frac{1}{2\pi} \intl{-\infty}{+\infty}
e^{it(y-x) - \frac{\si^2 t^2}{2}} \, dt \, dF(y)
= \intl{-\infty}{+\infty} \frac{1}{2\pi} \iii e^{it(y-x)}
\underbrace{\frac{1}{\sqrt{\frac{2\pi}{\si^2}}}
e^{-\frac{\si^2 t^2}{2}}}_{\text{плотность
$\Ns\!\left(0, \frac{1}{\si^2}\right)$}} \, dt \, dF(y) =\\=
\iii \frac{1}{\sqrt{2\pi\si^2}} e^{-\frac{(y-x)^2}{2\si^2}} \, dF(y) = p_\si(x)
\end{multline*}

Формула обращения для сглаженной случайной величины доказана:

$$
F_\si(x_2) - F_\si(x_1) = \frac{1}{2\pi} \iii \frac{e^{-itx_2} - e^{-itx_1}}{-it}
e^{-\frac{\si^2 t^2}{2}} \ph(t) \, dt.
$$

Осталось показать, что $\lim\limits_{\si \ra 0} F_\si(x) = F(x)$ в точках
непрерывности. Зафиксируем $\ep > 0$.
$$F_\si(x) = P\{X + Y_\si < x\} = P\{X < x - Y_\si\} \qquad \forall x.$$

\begin{multline*}
F_\si(x) \leqslant P\{X < x - Y_\si, |Y_\si| \leqslant \ep\} +
P\{|Y_\si| > \ep\} \leqslant \{ \text{по неравенству Чебышёва} \} \leqslant
P\{X < x + \ep\} + \frac{\si^2}{\ep^2} =\\= \{ \text{выберем $\si^2 = \ep^3$} \}
= F(x + \ep) + \ep.
\end{multline*}

\begin{multline*}
F_\si(x) \geqslant P\{X < x - Y_\si, |Y_\si| \leqslant \ep\} \geqslant
P\{X < x - \ep\} - P\{|Y_\si| > \ep\} \geqslant \{ \text{по неравенству
Чебышёва} \} \geqslant F(x - \ep) - \frac{\si^2}{\ep^2} =\\= F(x - \ep) - \ep.
\end{multline*}

Имеем: $ F(x - \ep) - \ep \leqslant F_\si(x) \leqslant F(x + \ep) + \ep $,
$ F(x - \ep) - \ep \ra F(x)$, $F(x + \ep) + \ep \ra F(x)$ (т.к. $x$ --- точка
непрерывности $F$). Отсюда $F_\si(x) \ra F(x)$.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%
%% Далее идут "наиболее суровые вопросы" :-Z
%%%%%%%%%%%%%%%%%%%%%%

\section{Наиболее суровые вопросы теории вероятностей}

\subsection{Теоремы Хелли}

\begin{df}
Последовательность $\hc{F_n(x)}$ функций распределения слабо сходится к $F(x)$, если $\fa x \in
\Cb(F)$ имеем $\lim F_n(x) = F(x)$. Тогда можно считать, что $0\le F\le 1$, если разрешить
переопределять $F$ в точках разрыва, чтобы $F$ была непрерывна слева. Обозначение:
$F_n \Ra F$.
\end{df}

\begin{theorem}[Хелли-I]
Из последовательности функций распределения можно выбрать слабо сходящуюся.
\end{theorem}
\begin{proof}
Рассмотрим счётное всюду плотное $D \subs \R$ и занумеруем его элементы: $D = \hc{x^1,x^2,x^3,\dots}$.
Рассмотрим $\hc{F_n(x^1)}$ и выберем (в силу её ограниченности) сходящуюся подпоследовательность $\hc{F_{1n}(x^1)}$.
Далее рассмотрим $\hc{F_{1n}(x^2)}$ и в ней выберем сходящуюся $\hc{F_{2n}(x^2)}$ и так далее. Таким образом,
количество точек сходимости на $n$-м шаге увеличивается на 1. Очевидно, что последовательность
$\hc{F_{nn}}$ сходится на $D$ к пределу (обозначим его $F$), поскольку $\fa k$ начиная с $n = k$
последовательность $\hc{F_{nn}(x^k)}$ является подпоследовательностью сходящейся.
Далее, покажем, что если $x \in \Cb(F)$, то $F_{nn}(x) \ra F(x)$.
Пусть $x', x'' \in D$, причём $x'< x< x''$, тогда $F(x') \le F_{nn}(x) \le F(x'')$.
Осталось устремить $x' \ra x \ot x''$, тогда по теореме о двух милиционерах имеем $F_{nn}(x) \ra F(x)$, ибо
это число зажато между $F(x')$ и $F(x'')$.
\end{proof}


\begin{theorem}[Хелли-II]
Пусть $F_n \Ra F$, где $F$\т функция распределения. Тогда $\ints{\R}f\,dF_n \ra \ints{\R}f\,dF$, если $f \in \Cb$
и ограничена на $\R$.
\end{theorem}
\begin{proof}
Пусть $|f| \le C$. Фиксируем $\ep > 0$, пусть $a < b \in \R$\т достаточно далёкие
точки непрерывности $F$, \те $F(a) < \ep$, а $F(b) > 1 - \ep$. Тогда, поскольку
$F_n \Ra F$, $\exi N\cln \fa n > N$ имеем $F_n(a) < \ep$ и $F_n(b) > 1 - \ep$.
Тогда
\eqn{\bbm{\intl{-\infty}{+\infty} f\,dF_n - \intl{a}{b}f\,dF_n}\le \intl{-\infty}{a}|f|\,dF_n + \intl{b}{+\infty}|f|\,dF_n \le 2C\ep,} и
\eqn{\bbm{\intl{-\infty}{+\infty} f\,dF - \intl{a}{b}f\,dF}\le \intl{-\infty}{a}|f|\,dF + \intl{b}{+\infty}|f|\,dF \le 2C\ep.}

Осталось показать, что $\intl ab f\,dF_n \ra \intl ab f\,dF$. Разобьём отрезок $[a,b]$ точками непрерывности
так мелко ($N$ штук), что $\om(f) < \ep$ на каждом элементе  разбиения. Так можно сделать в силу
равномерной непрерывности $f$ на $[a,b]$. Приблизим $f$ равномерно ступенчатой функцией $g$,
для которой $\|f - g\|_{[a,b]} \le \ep$, тогда
\mln{\bbm{\intl ab f\,dF_n - \intl ab f \, dF} \bw\le \intl ab |f - g|\,dF_n \bw+
\bbm{\intl ab g\,dF_n - \intl ab g \, dF} \bw+ \intl ab |f - g|\,dF \bw \le \\ \le
\ep \bw+ C \hr{\suml{k=1}{N}\br{F_n(x_k) - F(x_k) - F_n(x_{k-1}) + F(x_{k-1})}} \bw+ \ep,}
а последнее слагаемое можно сделать маленьким при $n \ra \infty$.
\end{proof}


\subsection{Предельные теоремы}

\begin{theorem}[Прямая предельная теорема]
Пусть $F_n, F$\т функции распределения, а $\ph_n, \ph$\т их характеристические функции. Пусть
$F_n \Ra F$, тогда $\ph_n \ra \ph$.
\end{theorem}
\begin{proof}
Применим вторую теорему Хелли к $f = e^{itx}$, тогда
$\ph_n(t) = \int e^{itx}\,dF_n \ra \int e^{itx}\, dF = \ph(t)$.
\end{proof}

\begin{lemma}[Оценка вероятности хвостов]\label{lemma.tails}
$\Pf\bc{|X| \ge \frac2u}\le \frac1u\intl{-u}{u}\br{1-\ph(t)}\, dt$, где $\ph = \Char X$.
\end{lemma}
\begin{proof}
Имеем
\mln{\frac1{2u} \intl{-u}{u}\br{1-\ph(t)}\,dt  = \frac1{2u}\intl{-u}{u}\hr{1 - \Mf e^{-itX}}\, dt = \lcomm \text{Фубини}\rcomm =\\=
1 - \frac1{2u}\Mf\intl{-u}{u}e^{itX}\,dt = 1 - \Mf\frac{e^{iuX} - e^{-iuX}}{2iuX} = 1 - M\frac{\sin uX}{uX} = (*).}
Имеем $1 - \frac{\sin x}{x} \ge g(x) := \frac12-\frac12\chi_{(-2,2)}$, поэтому
$$(*) \ge \Mf g(uX) = 0\cdot\Pf\bc{|uX| < 2} + \frac12\cdot \Pf\bc{|uX| \ge 2},$$
что и требуется.
\end{proof}

\begin{theorem}[Обратная предельная теорема]
Если $\hc{\ph_n = \Char F_n}$ сходится к $\ph$, непрерывной при $t = 0$, то $F_n \Ra F$, где $F$\т функция
распределения, для которой $\Char F = \ph$.
\end{theorem}
\begin{proof}
По первой теореме Хелли выделим $F_{n_k} \Ra F$. Беда в том, что она в общем случае не будет функцией распределения.
Докажем, что $F(-\infty) = 0$, а $F(+\infty) = 1$. Фиксируем $\ep > 0$, тогда,
поскольку $\ph \in \Cb(0)$, а $\ph(0) = \lim \ph_n(0)$, то $\exi U_u(0)\cln |1 - \ph(t)| \bw\le \ep$.
Тогда $\frac1u\intl{-u}{u}\br{1 - \ph(t)}\, dt\bw<\ep$, поэтому для достаточно больших $n$ имеем
$\frac1u\intl{-u}{u}\br{1 - \ph_n(t)}\, dt\bw<\ep$, ибо по теореме Лебега интегралы сходятся к интегралу
предела.

Применим лемму \ref{lemma.tails}, тогда $\Pf\bc{|X_n| \ge \frac2u} < \ep$.
Имеем $\Pf\bc{X_n \le -\frac2u} + \Pf\bc{X_n \ge \frac2u} = F_n\br{-\frac2u} + 1 - F_n\br{\frac2u}$.
Выберем $u$ так, чтобы $\pm\frac2u$ были точками непрерывности $F$, и устремим $n\ra\infty$. Отсюда
$\ep \bw\ge F\br{-\frac2u} \bw+ 1 \bw- F\br{\frac2u} \bw\ge F(-\infty) \bw+ 1 \bw- F(+\infty)$.

По второй теореме Хелли имеем $\ints{\R}e^{itx}\,dF_n \ra \ints{\R}e^{itx}\,dF$, откуда $\ph_{n_k} \ra \ph = \Char F$.
Если теперь допустить, что $F_n \nRightarrow F$, то найдутся две подпоследовательности $F_{n'} \ra F^*$
и $F_{n''} \Ra F^{**}$, но по прямой теореме $\ph_{n'} \ra \ph^*$ и $\ph_{n''} \ra \ph^{**}$, но их предел общий,
поэтому $\ph^* = \ph^{**} = \ph$, но это противоречит формуле обращения.
\end{proof}

\begin{theorem}[ЦПТ]
Пусть $X_1\sco X_n$\т независимые одинаково распределённые случайные величины, для которых
$\Mf X_i^2 < \infty$. Положим $m := \Mf X_i$, а $\si^2 := \Df X_i > 0$. Пусть $F_n$\т функции
распределения величин $S_n := \frac1{\sqrt{n}}\sumiun \frac{X_i - m}{\si}$.
Тогда
\eqn{\lim F_n = \Phi(x) =\frac{1}{\sqrt{2\pi}}\intl{-\infty}{x}e^{-\frac{y^2}{2}}\,dy.}
\end{theorem}
\begin{proof}
Рассмотрим величины $\frac{X_i - m}{\si}$, тогда $\Mf\frac{X_i - m}{\si} = 0$, а $\Df\frac{X_i - m}{\si} = 1$.
Пусть им соответствует характеристическая функция $\ph(t)$, тогда $\ph \in \Db^2$, кроме того,
$\ph(0) = 1$, $\ph'(0) = 0$, $\ph''(0) = i^2 = -1$.
По формуле Тейлора имеем $\ph(t) = 1 - \frac{t^2}2 + o\hr{t^2}$, тогда
по мультипликативному свойству имеем $\Char S_n = \ph^n\hr{\frac{t}{\sqrt{n}}}$.
При фиксированном $t$ имеем $\ph^n\hr{\frac{t}{\sqrt n}} = \hr{1 - \frac{t^2}{2n} +
o\hr{\frac 1n}}^n \ra e^{-\frac{t^2}{2}} = \Char \Nc(0,1)$. Осталось применить теорему непрерывности.
\end{proof}

Применения ЦПТ\т статистика. Пусть имеются одинаково ошибочные наблюдения с двухсторонними
погрешностями, \те $X_i = m + Y_i$, где $Y_i$\т ошибки, а $m$\т истинное значение.
Предположение состоит в том, что $\Mf Y_i = 0$, а $MY_i^2 < \infty$, \те разброс конечен.
Введём $S_n := \frac1n\sumiun X_i$, тогда
рассмотрим $\Pf \bc{\om\cln \hm{S_n - m} \ge \ep} \bw=
\Pf \BC{\frac{1}{n}\sumiun (X_i - m) \bw< \frac{x\si}{\sqrt{n}}} \bw\ra \Phi(x)$.
Отсюда $\Pf\BC{\om\cln \bm{\frac{1}{n}\sumiun (X_i - m)} \bw\ge \frac{x\si}{\sqrt{n}}} \bw\ra 1 \bw-
\Phi(x) \bw+ \Phi(-x) \bw= 1 \bw- 2\Phi_0(x)$,
где $\Phi_0(x) = \frac{1}{\sqrt{2\pi}}\intl{0}{x}e^{-\frac{y^2}{2}}\, dy$.
Примерные данные показывают следующее:
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline $x \backslash \Pf_x$ & ЗБЧ        & ЦПТ           \\
\hline 1                    & $\le 1$    & $\le 0.317$   \\
\hline 3                    & $\le 0.1$  & $\le 0.0026$   \\
\hline 10                   & $\le 0.01$ & $\sim 0$   \\
\hline
\end{tabular}
\end{center}

\subsection{Теорема Ляпунова}

\begin{theorem}[Ляпунова]
Пусть $X_1\sco X_n$\т независимые случайные величины. Пусть $\Mf |X_i|^3 < \infty$ и
$$
\frac{\sqrt[3]{\sum \Mf|X_i - \Mf X_i|^3}}{\sqrt{\sum \Mf|X_i - \Mf X_i|^2}} \ra 0 \quad \Ra \quad
\Pf\hc{\frac{\sum(X_i - \Mf X_i)}{\sqrt{\sum \Mf|X_i - \Mf X_i|^2}} < x} \ra \Phi(x).
$$
\end{theorem}

\hrule

\vskip 10pt


\subsection{Закон 0 и 1 Колмогорова}

Рассмотрим вероятностное пространство $(\Om, \Ac, \Pf)$ и на нём\т $X_1\sco X_n$\т случайные величины.
Мы доказывали, что если $B \in \Bc(\R^n)$, то $\hc{\om\cln \br{X_1(\om)\sco X_n(\om)} \in B} \in\Ac$.
Более того, если рассмотреть все $B \in \Bc$, то совокупность всех прообразов образуют $\si$\Д алгебру,
поскольку полный прообраз уважает необходимые теоретико-множественные
операции. Эта $\si$\Д алгебра называется порождённой $X_1\sco X_n$ и обозначается $\Ac(X_1\sco X_n)$,
кроме того, ясно, что она содержится в $\Ac$.

Рассмотрим бесконечную последовательность $\hc{X_n}$. Рассмотрим $\Ac(X_{n+1}\sco X_m)$ и зафиксируем $n$. Рассмотрим
$\Ac_n \bw{:=} \cupl{m=n+1}{\infty} \Ac(X_{n+1}\sco X_m)$. Очевидно, что это алгебра, но не $\si$\Д алгебра,
так что её надо расширить: возьмём $\Ac(X_{n+1},\dots) := \si(\Ac_n)$. Теперь пересечём их всех, и получим
искомую $\si$\Д \emph{алгебру остаточных событий}: $\Ac_\infty = \capl{n=0}{\infty}\Ac(X_{n+1},\dots)$.
Ясно, что при пересечении ничего не испортится. Каждое из событий $A \in \Ac_\infty$ называется \emph{остаточным},
или \emph{хвостовым}.

\begin{df}
Пусть дана последовательность случайных величин $\hc{X_i}$. Они называются независимыми,
если $\fa n$ имеем $X_1\sco X_n$ независимы.
\end{df}

\begin{df}
Назовём $\si$\Д алгебры $\Ac_1$ и $\Ac_2$ независимыми, если $\fa A_1\in\Ac_1,\, \fa A_2 \in \Ac_2$
имеем $\Pf(A_1A_2) = \Pf(A_1)\Pf(A_2)$.
\end{df}

\begin{theorem}[Закон <<0 или 1>> Колмогорова]
Пусть $X_1,\dots$\т последовательность случайных величин, а $\Ac_\infty$\т порождённая ею $\si$\Д алгебра остаточных
событий. Пусть $A \in \Ac_\infty$, тогда либо $\Pf(A) = 0$, либо $\Pf(A) = 1$.
\end{theorem}
\begin{proof}
Пусть $A\in\Ac_\infty$, тогда $A \in \Ac(X_{n+1},\dots)$ для всякого $n \in \N_0$. Фиксируем $n$ и
пусть $A_1\in \Ac(X_1\sco X_n)$. Имеем $\Ac(X_1\sco X_n)$ и $\Ac(X_{n+1},\dots)$\т независимые
$\si$\Д алгебры, поскольку они порождены независимыми случайными величинами. Теперь рассмотрим все
$n$, тогда получим, что $A$ не зависит от любого $A_1 \in \Ac(X_1,\dots)$ (теорема о единственном
продолжении меры с сохранением $\si$\Д аддитивности), поэтому, в частности, $A$ не зависит от
самого себя, ибо $\Ac_\infty \subs \Ac(X_1,\dots)$. Это означает, что (положим в формуле $A_1 :=
A$) $\Pf(AA) = \Pf(A)\Pf(A)$, откуда следует утверждение теоремы.
\end{proof}

\subsection{Усиленный закон больших чисел}


\begin{df}
Сходимость по вероятностной мере: $X_n \convp X$, если $\fa C > 0$ имеем $\Pf\hc{\om\cln |X_n - X| \ge C} \ra 0$.
\end{df}

\begin{df}
Сходимость распределений: $F_n \Ra F$, \те $\fa x \in \Cb(F)$ имеем $F_n(x) \ra F(x)$.
\end{df}

\begin{lemma}
Сходимость по вероятности влечёт слабую сходимость.
\end{lemma}
\begin{proof}
Имеем $F_n(x) = \Pf\hc{X_n < x} = \Pf\hc{X_n < x,\, |X-X_n| < \ep} \bw+ \Pf\hc{X_n \bw< x,\, |X_n \bw- X| \ge \ep} \bw\le
\Pf\hc{X_n \bw< x \bw+ \ep} \bw+ \Pf\hc{|X \bw- X_n| \ge \ep} = F(x + \ep) \bw+ \Pf\hc{|X \bw- X_n| \ge \ep}$.
Аналогично $F_n(x) = \Pf\hc{X_n < x} \ge \Pf\hc{X_n < x,\, |X \bw- X_n| < \ep} \ge
\Pf\hc{X < x - \ep} - \Pf\hc{|X_n- X|\ge \ep} = F(x-\ep) - \Pf\hc{|X_n- X|\ge \ep}$. Если $x$\т точка непрерывности
$F$, то $F_n(x)$ никуда на денется.
\end{proof}

\begin{df}
Сходимость почти всюду по вероятностной мере называется сходимостью почти наверное.
\end{df}

\begin{lemma}
Из сходимости почти всюду вытекает сходимость по вероятности. Обратное неверно.
\end{lemma}
\begin{proof}
Пусть $X_n \ra X$ почти всюду. Фиксируем $C > 0$. Рассмотрим
$A_n:= \caps{k \ge n}\hc{|X_k - X| < C}$.
Тогда $A_n \bw\subs A_{n+1} \bw\subs \dots$, и $\Om \bw= \bigcup A_n$ с точностью до множества меры 0,
поэтому $\Pf A_n \ra 1$, откуда $\Pf(\Om \wo A_n) \ra 0$, но это и означает сходимость по мере.
\end{proof}


\begin{theorem}[Лемма Бореля\ч Кантелли]
Пусть $\hc{A_n}$\т последовательность событий в $(\Om, \Ac, \Pf)$. Рассмотрим $B =
\capkui\cupl{n=k}{\infty} A_n$. Тогда:

\pt{1}. Если $\sum \Pf(A_n)$ сходится, то $\Pf(B) = 0$.

\pt{2}. Если $\hc{A_n}$ независимы и $\sum \Pf(A_n)$ расходится, то $\Pf(B) = 1$.
\end{theorem}
\begin{proof}
Событие $B$ содержит то, что происходит бесконечно много раз. Дополнение к $B$ содержит то,
что происходит лишь конечное число раз. Для доказательства первого утверждения
достаточно заметить, что $\Pf(B) \bw\le \Pf\bc{\cupl{n=k}{\infty}A_n} \le \suml{n=k}{\infty} \Pf(A_n) \ra 0$.
Отсюда $\Pf(B) = 0$. Докажем второе утверждение. Имеем $\ol{\cupl{n=k}{\infty}A_n} = \capl{n=k}{\infty}\ol{A}_n
\subs \capl{n=k}{K}\ol{A}_n$. Отсюда $\Pf\bc{\ol{\cupl{n=k}{\infty}A_n}} \le \Pf\bc{\capl{n=k}{K}\ol{A}_n} =\lcomm
$ независимость $\rcomm=\prodl{n=k}{K}\Pf\hr{\ol{A}_n} \le \exp\hr{-\suml{n=k}{K}\Pf(A_n)}$, ибо
$1-x \le e^{-x}$ при $x > 0$. Устремим $K$ к бесконечности, получим в пределе 0 в силу расходимости ряда.
Далее остаётся воспользоваться счётной аддитивностью меры: объединение множеств меры нуль есть множество
меры нуль. Итак, дополнение имеет меру нуль, откуда следует наше утверждение.
\end{proof}

\begin{df}
Индикатор (характеристическая функция) события $A \in \Ac$\т функция $I_A\cln \Om\ra \R$, равная
единице на $\om \in A$ и нулю на $\om \notin A$.
\end{df}

\begin{lemma}[Неравенство Колмогорова]\label{lemma.kolmogorov}
Пусть $X_1\sco X_n$\т независимые случайные величины, для которых $\Mf X_i = 0$, а $\Df X_i = \si_i^2$.
Тогда $\fa a > 0$ имеем $\Pf\bc{ \maxl{i \le n} |X_1\spl X_i| \ge a } \le \frac1{a^2}\sum \si_i^2$.
Физический смысл леммы: оценка вероятности того, что кривая случайного блуждания суммы $X_i$ зашкалит за уровень $a$.
\end{lemma}
\begin{proof}
Введём $S_i := X_1\spl X_i$. Рассмотрим искомое событие $A = \bc{\om\cln \maxl{i \le n} |S_i| \ge a}$
и разобьём его на куски: $A_j := \hc{\om\cln \fa i < j\, |S_i|< a,\, |S_j| \ge a}$, \те это события, отвечающие
за то, что зашкал произошёл именно на $j$\Д м шаге. Тогда $I_A = \suml{j=1}{n} \I j$, кроме того,
в силу независимости и того, что $\Mf X_i = 0$, имеем
$$\Mf(X_1\spl X_n)^2 = \Mf X_1^2\spl \Mf X_n^2,$$ откуда
\mln{\sum \si_i^2 = \Mf S_n^2 \ge \Mf S_n^2\cdot I_A = \sum \Mf S_n^2 \cdot\I j = \sum \Mf (S_n - S_j + S_j)^2\cdot \I j =\\=
\sum \hr{\Mf (S_n - S_j)^2\cdot \I j + 2\Mf (S_n - S_j)S_j\cdot \I j + \Mf S_j^2 \cdot \I j} = (*).}
Второе слагаемое здесь равно нулю, ибо первый множитель выражается через $X_{j+1}\sco X_n$, а второй\т через $X_1\sco X_j$.
Первое слагаемое мы отбросим совсем, поскольку оно неотрицательно. Продолжаем: $(*) \ge \sum \Mf S_j^2\cdot \I j \ge
\sum \Mf  a^2\cdot \I j  = a^2\Mf \sum \I j = a^2 \Mf I_A = a^2\Pf(A)$, что и требовалось доказать.
\end{proof}

\begin{imp}
В условиях неравенства Колмогорова и сходимости $\sum \si_i^2$ имеем $\sum X_i$ сходится п.\,н.
\end{imp}
\begin{proof}
Проверим выполнение критерия Коши почти всюду:
$S_{N + i} - S_N = X_{N+1}\spl X_{N + i}$. Рассмотрим $\ep > 0$, тогда
$\Pf\bc{\maxl{i\le n} |S_{N + i} - S_N|\ge \ep} \le \frac{1}{\ep^2}\suml{j=1}{n}\si_{N+j}^2$.
Переходя к пределу при $n\ra\infty$, получаем оценку вида $\le \frac{1}{\ep^2}\suml{j=N+1}{\infty}\si_j^2$, а
подходящим выбором $N$ эту величину можно сделать сколь угодно маленькой. Отсюда всё следует, поскольку объединение
множеств меры нуль есть множество меры нуль.
\end{proof}

\begin{lemma}\label{lemma.convergence}
Из сходимости ряда $\sum \frac{a_i}{i}$ следует $\frac{1}{n}\suml{i=1}{n}a_i \ra 0$.
\end{lemma}
\begin{proof}
Рассмотрим $b_i := \frac{a_i}{i}$ и переформулируем: из сходимости $\sum b_i$ следует $\frac{1}{n}\sumiun ib_i \ra 0$.
Положим $S_n := b_1\spl b_n$, тогда $S_n$ равномерно ограничены некоторым числом $B$. Положим $S_0 = 0$. Очевидно, что
$\frac{1}{n}\sumiun i b_i = \frac{1}{n}\br{(S_n - S_0) \bw+ (S_n - S_1)\spl (S_n - S_{n-1})}$.
В силу критерия Коши имеем $\fa \ep > 0 \exi N\cln\fa n \ge m \ge N$ имеем $|S_n - S_m| \le \ep$. Тогда
при $n \ge N$ получаем $\frac{1}{n}\bm{\sumiun ib_i} \le \frac{1}{n}\br{2BN + (n-N)\ep} = 2BN\frac{1}{n} + \frac{n - N}{n}\ep$.
Теперь всё доказано, так как подходящим выбором $n$ и $\ep$ можно сделать это число сколь угодно малым.
\end{proof}

\begin{theorem}[ЗБЧ $2.0\beta$] \label{theorem.lbnbeta}
Пусть $X_1\sco X_n$\т независимые случайные величины, $m_i = \Mf X_i$, а $\si_i^2 = \Df X_i$. Пусть
сходится $\sum \frac{\si_i^2}{i^2}$, тогда
$\liml{n} \frac{1}{n}\suml{i=1}{n}(X_i - m_i) = 0$ почти всюду.
\end{theorem}
\begin{proof}
Рассмотрим $\frac{X_i- m_i}{i}$, тогда $\Mf\frac{X_i-m_i}{i} = 0$, а $\Df\frac{X_i - m_i}{i} = \frac{\si_i^2}{i^2}$.
В силу следствия, $\suml{i=1}{n}\frac{X_i-m_i}{i}$ сходится почти всюду. Осталось применить лемму \ref{lemma.convergence}.
\end{proof}

\begin{lemma}\label{lemma.convsquare}
Пусть $\Mf |X| < \infty$, тогда
$$\sum \frac{1}{n^2}\Mf X^2\cdot \IS{|X| < n} < \infty.$$
\end{lemma}
\begin{proof}
Положим $A_n = \hc{n - 1\le |X| < n}$, тогда $\Mf |X| = \sum \Mf |X|\cdot \I{n}$.
Обозначим $b_n = \Mf |X| \cdot\I{n}$, откуда
$\Mf X^2 \cdot \IS{|X| < n} = \sumkun \Mf X^2\cdot \I{k} \le \sumkun k \Mf |X|\cdot\I{k} = \sumkun k b_k$.
Осталось доказать сходимость ряда $\suml{n=1}{\infty}\frac{1}{n^2}\sumkun kb_k =
\sumkui kb_k \suml{n=k}{\infty}\frac{1}{n^2}=
b_1\cdot \frac{\pi^2}{6} + \suml{k=2}{\infty} kb_k \suml{n=k}{\infty}\frac{1}{n^2}$. Используем
оценку для величины остатка гармонического ряда с помощью интеграла: $\suml{k=n}{\infty} \frac{1}{n^2} \le \frac{1}{k-1}$,
поэтому второе слагаемое оценивается так: $\suml{k=2}{\infty} kb_k \suml{n=k}{\infty}\frac{1}{n^2}
\le \suml{k=2}{\infty} \frac{k}{k-1}b_k$, а последний ряд сходится, поскольку $\sum b_k$ сходится.
\end{proof}

\begin{theorem}[УЗБЧ]
Пусть $X_1\sco X_n$\т независимые одинаково распределённые случайные величины.
Тогда $\liml{n}\frac{1}{n}\sumiun X_i = m$ почти всюду равносильно $\Mf X_i = m$, при условии
$\Mf |X_i| < \infty$.
\end{theorem}
\begin{proof}
\emph{Достаточность.} Переходя к $X_n' = X_n - m$, мы всё сведём к рассмотрению величин с нулевым $\Mf$. Пусть $X$ имеет то же
распределение, что и $X_i$, причём $\Mf X = 0$.
Рассмотрим $X_n = Y_n + Z_n$, где $Y_n = X_n \cdot \IS{|X_n| < n}$, \те срезки.
Тогда
\mln{\sum \Pf\hc{Z_n \neq 0} = \sum \Pf\hc{|X_n| \ge n} = \sum n \Pf\hc{n \le |X_n| < n+1} \le\\\le
\sum\Mf\hc{|X_n| \cdot \IS{n \le |X| < n+1}} = \Mf\hc{|X| \cdot\sum\IS{n \le |X| < n+1}} = \Mf|X|< \infty.}
Осталось шарахнуть леммой Бореля\ч Кантелли.

Обозначим $M_i := \Mf Y_i$, и всё сведено к доказательству сходимости
$\frac{1}{n}\sum Y_i \ra 0$ почти наверное. Имеем $\Mf Y_n = \Mf X_n\cdot \IS{|X_n| < n}$,
поэтому, раз $\Mf X = 0$, а $\Mf |X| < \infty$, то $\Mf \hc{X_n\cdot \IS{|X_n| < n}} = \Mf X \cdot \IS{|X| < n} \ra \Mf X = 0$
по теореме о мажорируемой сходимости, ибо срезки $|X|$ сходятся к $|X|$ почти всюду.
Из анализа известно, что если $m_i \ra 0$, то и $\frac{1}{n}\sumiun m_i \ra 0$, поэтому
$\Df Y_n = \Mf Y_n^2 - \br{\Mf Y_n}^2 \le \Mf Y_n^2 = \Mf X_n^2\cdot \IS{|X_n| < n} = \Mf X^2 \cdot \IS{|X| < n}$.
По лемме~\ref{lemma.convsquare} получаем
$$\sumnui \frac{\Df Y_n}{n^2} \le \sumnui \frac{1}{n^2} \Mf X^2\cdot \IS{|X| < n} < \infty,$$
поэтому применима теорема \ref{theorem.lbnbeta}, по которой $\frac{1}{n}\sumiun (Y_i - m_i) \ra 0$ почти всюду, что
и требовалось доказать.

\emph{Необходимость.} Имеем
$$\frac{x_n}{n} = \frac{1}{n}\sumiun X_i - \frac{n-1}{n}\cdot \frac{1}{n-1}\suml{i=1}{n-1}X_i \ra m - 1\cdot m = 0$$
по условию теоремы. С вероятностью $1$ происходит лишь конечное число событий
$A_n = \hc{\frac{|X_n|}{n} \ge 1}$, поэтому в силу ЛБК и одинаковой распределённости имеем
$\sum \Pf(A_n) = \sum \Pf\hc{|X| \ge n} < \infty$.

Далее, легко видеть, что
\begin{multline*}
\sumnui \Pf\hc{|X| \ge n} = \sumnui n \Pf\hc{n \le |X| < n+1} =
\sumnzi (n+1)\Pf\hc{ n \le |X| < n+ 1} - \sumnzi\Pf\hc{ n \le |X| < n+ 1} \ge \\\ge
\suml{n=0}{\bes}\Mf \hc{|X| \cdot \IS{n \le |X| < n+1}} - 1 = \Mf |X| \sumnzi \IS{n\le|X| < n+1}-1 = \Mf|X| -1,
\end{multline*}
поэтому существует последнее в цепочке математическое ожидание. Теперь можно воспользоваться достаточностью
УЗБЧ, откуда следует утверждение теоремы.
\end{proof}

\end{document}
