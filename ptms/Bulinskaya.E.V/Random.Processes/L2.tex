\section{lection 2}

Продолжим доказательство Теоремы Колмогорова.

Осталось проверить счетную аддитивность меры $\Pi$ на алгебре $\vA$. Поскольку
любое множество из $\vA$ --- это конечная сумма непересекающихся интервалов
и $\Pi$ --- конечно-аддитивна, достаточно проверить, что
\[
        \Pi(I)=\sum^\infty_{k=1}\Pi(I_k),\text{ если }I=I_1+I_2+\dots
\]
(т.е. интервал  $I=I_{t_1,\dots,t_{n_0}}$ есть объединение счетного числа
непересекающихся интервалов $I_k=I_{t_1,\dots,t_{n_k}}$, $n_0\le n_1\le\dots$,
$n_k\to\infty$ при $k\to\infty$; $I_j\cap I_l$, $j\ne l$).

Так как $I\supset I_1+\dots+I_m$, то в силу конечной аддитивности,
\[
        \Pi(I)\ge\sum^m_{k=1}\Pi(I_k),
\]
для любого $m\ge 1$.

Переходя в этом неравенстве к пределу при $m\to\infty$, получим
\[
        \Pi(I)\ge\sum^\infty_{k=1}\Pi(I_k).
\]

Предположим, что
\[
        \Pi(I)=\sum^\infty_{k=1}\Pi(I_k)+\ga,
\]
для некоторого $\ga>0$, и придем к противоречию.

Положим $A_0=I$, $A_m=I\setminus(I_1+\dots+I_m)$, $m\ge 1$.

Очевидно, $A_m=\pi_{t_1,\dots,t_m}^{-1}A_m^*$, где $A_m^*$ --- конечная сумма
$n_m$-мерных интервалов (или параллелепипедов из $\RR^{n_m}$), являющихся
основаниями интервалов, составляющих $A_m$.

При этом $A_0\supset A_1\supset\dots$ и $\Pi(A_m)\ge\ga$ при всех $m\ge 0$.

В силу свойств конечномерных функций распределения, в том числе непрерывности
сверху $F_{t_1,\dots t_{n_m}}$ в любой точке $(x_1,\dots,x_{n_m})$, для
произвольного $\ep>0$ в каждом из составляющих $A_m^*$ параллелепипедов можно
найти замкнутый ограниченный параллелепипед, такой, что для их суммы $B^*_m$
верно соотношение $P_{t_1,\dots t_{n_m}}(A^*_m\setminus B^*_m)<\ep/2^{m+1}$.

А это означает, что $\Pi(A_m\setminus B_m)<\ep/2^{m+1}$.

Пусть, далее, $C_m=B_0 B_1\dots B_m=\pi_{t_1,\dots t_{n_m}}^{-1}C^*_m$, где
$C_m^*$ --- конечная сумма $n_m$-мерных замкнутых ограниченных
параллелепипедов.

Так как
\begin{gather*}
        A_m\setminus C_m=A_m\bar C_m=A_m(\bar B_0\cup\dots\cup\bar B_m)
        \subset A_0 B_0\cup\dots\cup A_m B_m,\\
\intertext{то}
        \Pi(A_m\setminus C_m)\le\sum_{k=0}^m\Pi(A_k\bar B_k)\le\ep.
\end{gather*}

Отсюда $\Pi(C_m)=\Pi(A_m)-\Pi(A_m\setminus C_m)\ge\ga-\ep>0$ (при $\ep<\ga$).

При любом $m$ множества $C_m$ не пусты, следовательно, из каждого $C_m$
можно выбрать точку $x_m$, т.е. функцию вида
\[
    x_m(t)=\begin{cases}y_{m_s}, & t=t_s,\ s=\overline{1,n_m},\\
                0, &\text{для остальных }t.
        \end{cases},
\]
при этом вектор $(y_{m1},\dots,y_{mn_m})$ --- это точка одного из
параллелепипедов, составляющих $C^*_m$ (основание $C_m$).

Из построения $C_m$ следует, что при фиксированном $s$ последовательности
$\{y_{m_s}\}_{m\ge 0}$ --- ограничены. С помощью диагональной процедуры можно
найти такую последовательность $m_1<m_2<\dots$, что $y_{m_ks}\to y_s$,
$k\to\infty$, при всех $s=1,2,\dots$.

Поскольку $C_0\supset C_1\supset\dots$ и все $C_m$ замкнуты, то
\[
        x(t)=\begin{cases}y_s, & t=t_s,\ s=1,2,\dots\\
                    0, &\text{для остальных }t.
    \end{cases},
\]
(как точка $\RR^T$) принадлежит любому $C_m$, а, значит, и $A_m$. А это
показывает, что $x\in I$, но $x\notin I_m$, $m\ge 1$, т.е. пришли к
противоречию с равенством $I=I_1+I_2+\dots$.

Таким образом, счетная аддитивность $\Pi$ на алгебре $\vA$ доказана, чем и
закончено доказательство теоремы Колмогорова.

\begin{problem} Проверить, что аналог теоремы Колмогорова справедлив для
случайных функций со значениями в польских пространствах.
\end{problem}

Итак, семейство конечномерных распределений, удовлетворяющих условиям симметрии
и согласованности, задает случайный процесс. И классификацию процессов можно
проводить в соответствии со свойствами их конечномерных распределений.

Познакомимся с некоторыми классами случайных процессов.
\begin{enumerate}
\item
\underline{\bf Процессы с независимыми значениями.}

Говорят, что $X=\{X(t),\ t\in T\}$ имеет {\em независимые значения}, если
для любых $t_i\in T$, $i=\overline{1,n}$, $n\ge 2$ случайные величины
$X(t_1),\dots, X(t_n)$ (взаимно) независимы.

Нетрудно проверить, что для задания такого процесса достаточно знать лишь
одномерные распределения $F_t(x),\ t\in T$. Действительно, полагая
\[
        F_{t_1,\dots,t_{n_m}}(x_1,\dots,x_n)=\prod^n_{j=1}F_{t_j}(x_j),
\]
получим семейство конечномерных распределений, удовлетворяющих условиям
теоремы Колмогорова. Значит, такой процесс действительно существует.

В частности, мы установили существование последовательности независимых
случайных величин с заданными функциями распределения, которые использовали
при доказательстве ЗБЧ и ЦПТ в курсе теории вероятностей. (Достаточно положить
$T=\{1,2,\dots\}$).

Если взять $F_t(x)=F(x)$, $t\in T$, то получим процесс с независимыми одинаково
распределенными значениями.
\item \underline{\bf Процессы с независимыми приращениями.}

Процесс $X=\{X(t),\ t\in T\}$ имеет {\em независимые приращения}, если для
любых $t_i\in T$, $i=\overline{1, n}$, $n\ge 3$ случайные величины
$X(t_2)-X(t_1),X(t_3)-X(t_2),\dots,X(t_n)-X(t_{n-1})$ независимы.
\end{enumerate}

\begin{problem} Что надо знать для того, чтобы построить процесс с независимыми
приращениями?
\end{problem}

\begin{enumerate}
\setcounter{enumi}{2}
\item \underline{\bf Стационарные процессы.}

Существуют стационарные процессы в узком и широком смысле.
\begin{enumerate}
\item
Процесс $X=\{X(t),\ t\in T\}$ называется {\em стационарным в узком смысле},
если все его конечномерные распределения не меняются при сдвиге, т.е.
\[
        \forall t_i\in T,\ t_i+h\in T,\ i=\overline{1,n},\ n\ge 1,\
    P_{t_1+h,\dots,t_n+h}=P_{t_1,\dots,t_n}.
\]

Примером стационарного в узком смысле процесса может служить процесс с
независимыми одинаково распределенными значениями.

\item
Процесс $X=\{X(t),\ t\in T\}$ называется {\em стационарным в широком смысле},
если при сдвиге не меняются его моменты первого и второго порядка, т.е.
\begin{gather*}
        \forall t\in T,\ s\in T,\ t+h\in T,\ s+h\in T\\
        a(t+h)=a(t),\qquad R(s+h, t+h)=R(s,t),\\
        \text{где }
    a(t)=EX(t),\qquad R(s,t)=EX(s)X(t).
\end{gather*}
\end{enumerate}
\end{enumerate}
\begin{problem} Как между собой связаны классы стационарных в узком и широком
смысле процессов?
\end{problem}

\begin{enumerate}
\setcounter{enumi}{3}
\item \underline{\bf Гауссовские (или нормальные) процессы.}

Случайный процесс $X=\{X(t),\ t\in T\}$ называется {\em гауссовским}, если все
его конечномерные распределения гауссовские.

Вспомним, что вектор $\gx=(\gx_1,\dots,\gx_n)$ {\em гауссовский}, если его
характеристическая функция  $\gf(\gl)=Ee^{i(\gl,\gx)}$ (иначе она записывается
$\gf(\gl_1,\dots,\gl_n)=Ee^{i\sum_{j=1}^n\gl_j\gx_j}$) имеет вид
\[
    \gf(\gl)=Ee^{i(\gl,a)-1/2(B\gl,\gl)}
\]

Более подробно можно записать
\[
    \gf(\gl_1,\dots,\gl_n)=e^{i\sum_{j=1}^n\gl_j a_j
    -1/2\sum_{j,l=1}^n\gl_j\gl_lb_{jl}},
\]
где $a=(a_1,\dots,a_n)$ --- вектор математических ожиданий $a_j=E\gx_j$,
$j=\overline{1,n}$ и $B=(b_{jl})_{j,l=\overline{1,n}}$ --- матрица ковариаций
\[
    b_{jl}=cov(\gx_j,\gx_l)=E(\gx_j-E\gx_j)(\gx_l-E\gx_l).
\]
\end{enumerate}

\begin{problem} Вектор $\gx=(\gx_1,\dots,\gx_n)$ тогда и только тогда
гауссовский, если любая линейная комбинация его координат --- гауссовская
случайная величина.
\end{problem}

\begin{problem} Матрица $B$ неотрицательно определена. Если $B$ положительно
определена, то распределение вектора $\gx$ имеет плотность
\[
        p(x_1,\dots,x_n)=(2\pi)^{-\frac n2}(\det B)^{-\frac n2}\exp
        \left\{-\frac 12\sum^n_{k,j=1} B_{kj}(x_k-a_k)(x_j-a_j)\right\}
\]
где $B_{kj}$ --- это элементы матрицы, обратной к $B$.
\end{problem}

\begin{problem} Если матрица $B$ имеет ранг $r<n$, то с вероятностью 1 вектор
$\gx$ принадлежит $r$-мерному линейному многообразию.
\end{problem}

\begin{problem} Если компоненты гауссовского вектора некоррелированы, то они
независимы. Это утверждение неверно, если лишь (одномерные) распределения
компонент гауссовские.
\end{problem}

Теперь сформулируем теорему существования гауссовского процесса.

\begin{Theorem} Для любой действительной функции $a(t)$, $t\in T$, и
действительной функции двух переменных $B(s, t)$, $s\in T$, $t\in T$,
удовлетворяющей условиям:
\begin{align*}
        1)&\ B(s,t)=B(t,s)\\
        2)&\ \sum^n_{k,j=1}B(t_k,t_j)\ge 0
\end{align*}
для произвольных действительных $\gl_1,\dots,\gl_n$ и $t_k\in T$,
$k=\overline{1,n}$, $n\ge 1$, --- существует гауссовский процесс
$X=\{X(t),\ t\in T\}$, для которого $a(t)=EX(t)$ и $B(s,t)=cov(X(s),X(t))$.
\end{Theorem}

\begin{proof} Воспользуемся теоремой Колмогорова. А именно, построим
семейство конечномерных (гауссовских) распределений и покажем их симметрию и
согласованность.

Для произвольных $t_1,\dots,t_n$ определим характеристическую функцию
следующим образом
\[
        \gf(\gl_1,\dots,\gl_n)=\exp\left\{i\sum_{k=1}^n\gl_k a(t_k)
        -\frac 12\sum_{k,j=1}^n\gl_k\gl_j B(t_k,t_j)\right\},
\]

Это характеристическая функция гауссовского вектора с математическим
ожиданием $(a(t_1),\dots,a(t_n))$ и матрицей ковариаций
$(B(t_k,t_j))_{k,j=\overline{1,n}}$.

Нетрудно видеть, что условия симметрии и согласованности (в терминах
характеристических функций) выполнены, а значит, требуемый гауссовский процесс
существует.
\end{proof}

Итак, гауссовский процесс задается своими первыми и вторыми моментами.

Рассмотрим два {\bf примера}.

\begin{itemize}
\item Пусть $a(t)=0$, $B(s,t)=\gs^2\gd(s,t)$, где $\gd(s,t)=1$ если $s=t$ и
$\gd(s,t)=0$ при $s\ne t$.

Очевидно, что такая функция $B(s,t)$ удовлетворяет условиям 1) и 2) предыдущей
теоремы.  Следовательно, существует гауссовский процесс, соответствующий этим
функциям $a(\cdot)$ и $B(\cdot)$. Значения процесса в различных точках
некоррелированы, а поскольку любой из наборов $(X(t_1),\dots,X(t_n))$
гауссовский, то указанные случайные величины независимы.

Таким образом, это гауссовский процесс с независимыми (одинаково
распределенными) значениями.

\item Пусть теперь $T=[0,\infty)$, $a(t)=0$, $B(s,t)=\min(s,t)$.

Условие 1 теоремы очевидным образом выполнено.

Проверим условие 2 неотрицательной определенности. Положим
\[
    \ind_{(-\infty,t]}(u)=\begin{cases} 1, & u\le t,\\
                            0, & u>t.
                \end{cases}
\]
тогда можно записать
\[
        \min(s,t)=\int\limits^\infty_0\ind_{(-\infty,s]}(u)\ind_{(-\infty,t]}(u)\,du
\]
Следовательно,
\begin{align*}
    \sum_{k,j=1}^n\gl_k\gl_j B(t_k,t_j) &=
        \sum_{k,j=1}^n\gl_k\gl_j\int\limits^\infty_0\ind_{(-\infty,t_k]}(u)
    \ind_{(-\infty,t_j]}(u)\,du=\\
        &=\int\limits^\infty_0\left(\sum_k^n\gl_k\ind_{(-\infty,t_k]}(u)
    \right)^2\,du\ge 0
\end{align*}
и, значит, существует гауссовский процесс с указанными параметрами.
\end{itemize}

\begin{problem} Проверить, что конечномерные распределения построенного
процесса имеют плотность и найти ее явный вид.
\end{problem}

\begin{Lemma} Гауссовский процесс с параметрами $a(t)=0$, $B(s,t)=\min(s,t)$,
$s,t\ge 0$, удовлетворяет следующим условиям:
\begin{itemize}
\item Это процесс с независимыми приращениями,
\item При $s<t$ приращение $X(t)-X(s)$ --- это гауссовская случайная величина с
нулевым средним и дисперсией $(t-s)$,
\item $X(0)=0$.
\end{itemize}
\end{Lemma}

\begin{proof} При любых $0\le t_1<t_2<\dots<t_n$ случайный вектор
$(X(t_1),\dots,X(t_n))$ --- гауссовский. Вектор $(X(t_2)-X(t_1),X(t_3)-X(t_2),
\dots,X(t_n)-X(t_{n-1}))$, полученный из предыдущего с помощью линейного
преобразования, также гауссовский с параметрами
\begin{align*}
        E(X&(t_j)-X(t_{j-1}))=a(t_j)-a(t_{j-1})=0,\\
        cov(X&(t_j)-X(t_{j-1}),X(t_l)-X(t_{l-1}))=\\
        &=E(X(t_j)-X(t_{j-1}))(X(t_l)-X(t_{l-1}))=\\
        &=EX(t_j)X(t_l)-EX(t_j)X(t_{l-1})-\\
        &\quad-EX(t_{j-1})X(t_l)+EX(t_{j-1})X(t_{l-1})=\\
        &=\min(t_j,t_l)-\min(t_j,t_{l-1})-\\
        &\quad-\min(t_{j-1},t_l)+\min(t_{j-1},t_{l-1}).
\end{align*}
Отсюда следует, что при $l\ne j$ мы имеем
\[
    cov(X(t_j)-X(t_{j-1}),X(t_l)-X(t_{l-1}))=0,
\]
а при $l=j$ получаем $D(X(t_j)-X(t_{j-1}))=t_j-t_{j-1}$.

Поскольку компоненты гауссовского вектора некоррелированы, они независимы, т.е.
условие 1 выполнено.

Справедливость условия 2 вытекает из предыдущих рассуждений. Достаточно взять
$n=2$ и положить $t_1=s$, $t_2=t$.

Что касается условия 3, то из того, что $EX(0)=0$, $D X(0)=0$, вытекает
$X(0)=0$ почти наверное.
\end{proof}

\begin{problem}(обязательная). Доказать, что процесс, удовлетворяющий условиям
1--3 леммы, является гауссовским с
\[
        EX(t)=0\text{ и }cov(X(s),X(t))=\min(s,t),\ s,t\ge 0.
\]
\end{problem}

Процесс называется {\em однородным по времени}, если распределения приращений
$X(t)-X(s)$, $s<t$, зависят лишь от разности $t-s$.

Рассмотренный процесс является однородным. Поскольку этот процесс предназначен
для описания {\em броуновского движения}, то естественно потребовать выполнение
еще одного условия:

\begin{itemize}
\item Все траектории процесса непрерывны.
\end{itemize}

Процесс, удовлетворяющий условиям 1--4, называется также {\em стандартным
винеровским}, поскольку в указанных условиях процесс изучался Винером в 20-е
годы XX века.

Теорема Колмогорова, как мы уже видели, позволяет построить процесс, обладающий
свойствами 1--3. Однако множество $C^T\subset R^T$ непрерывных функций не
является борелевским ($C^T\notin\vJ^T$), поэтому мы не можем не только
утверждать, что все траектории процесса непрерывны (или почти все они
непрерывны, т.е. $\Pi(C^T)=1$), но и вообще определить вероятность этого
множества (так как оно неизмеримо).

Существует несколько путей преодоления этой трудности. Один из них основан на
понятии эквивалентности процессов.

Два случайнх процесса
\[
        X=\{X(t),\ t\in T\}\text{ и }Y=\{Y(t),\ t\in T\},
\]
определенные на одном и том же вероятностном пространстве $(\gO,\vF,P)$ и
имеющие одно и то же параметрическое множество $T$, называются
{\em эквивалентными}, если $P(X(t)=Y(t))=1$ для любого $t\in T$.

\begin{problem} Эквивалентные процессы имеют одинаковые конечномерные распределения.
Обратное, вообще говоря, неверно.
\end{problem}

Эквивалентный случайный процесс называется также {\em модификацией} исходного
процесса.

Понятие эквивалентности приводит к различным последствиям для процессов с
дискретным и с непрерывным временем.

В то время как для процессов с дискретным временем из эквивалентности следует
совпадение почти всех траекторий (т.к. $P\{\cap_{t\in T}(X_t=Y_t)\}=1$, если
$T$ счетно), для процессов с непрерывным временем это вовсе не так. А именно,
множество совпадающих траекторий может иметь любую меру от 0 до 1 или вообще
быть неизмеримым.

Рассмотрим {\bf пример}.

Пусть $T=[0,1]$, $\gO=[0,1]$, $\vF=\vJ^{[0,1]}$ --- борелевская $\gs$-алгебра
на $[0,1]$, а вероятность $P$ --- мера Лебега. Положим $X(t,\go)=0$ для всех
$t\in T$, $\go\in\gO$, а $Y(t,\go)=1$ при $t=\go$ и $Y(t,\go)=0$ при $t\ne\go$.

\begin{itemize}
\item Очевидно, что эти процессы эквивалентны, т.к. при фиксированном $t$ они
отличаются лишь в одной точке $\go$, но
\[
    P(X(t)=Y(t),\ t\in [0,1])=0,
\]
ни одна из траекторий у двух процессов не совпадает.
\item У процесса $X$ все траектории непрерывны, а у $Y$ --- разрывны.
\item Далее, $\sup\limits_{t\in[0,1]}X(t)=0$, а
$\sup\limits_{t\in [0,1]}Y(t)=1$ с вероятностью 1.
\end{itemize}

\begin{problem} Как видоизменить определение процесса $Y$, чтобы множество
совпадающих траекторий $X$ и $Y$ было неизмеримым?
\end{problem}

В отличие от дискретного времени, где $\sup_t X_t$, $\inf_t X_t$,
$\varlimsup_{t\to t_0}X_t$, $\varliminf_{t\to t_0}X_t$ являются
случайными величинами, для непрерывного времени это не так. Многие интересные
для практики множества не являются борелевскими. В результате их вероятность
либо вовсе не задана, либо не определна однозначно конечномерными
распределениями.

Итак, обычно вопрос ставится таким образом: существует ли у данного процесса
модификация, обладающая нужными нам свойствами (а не так, обладает ли сам
рассматриваемый процесс этими свойствами). Исходя из этих соображений, в
следующий раз докажем существование винеровского процесса.

Мы увидим, что требование непрерывности накладывает ограничение на
конечномерные распределения.

Если рассматривать второе определение случайного процесса (как измеримое
отображение из $\gO$ в $R^T$), мы приходим к изучению свойств траекторий.

Говорят, что $X=\{X(t),\ t\in T\}$ {\em выборочно непрерывен} ({\em
дифференцируем} или {\em интегрируем}) в точке $\go$, если это верно для
соответствующей траектории, т.е. функции $X(\cdot,\go)$ от $t$.

Процесс {\em выборочно непрерывен на множестве} $A\in\vF$, если траектории
непрерывны для всех $\go\in A$.

В том случае, когда $P(A)=1$, говорят, что {\em почти все траектории процесса
непрерывны} или процесс {\em выборочно непрерывен с вероятностью 1}.

Если же исходить из первого определения случайного процесса как кривой в
пространстве случайных величин, можно дать 4 определения непрерывности
случайного процесса (в соответствии с 4 типами сходимости).

\begin{enumerate}

\item Процесс {\em непрерывен с вероятностью 1} в точке $t_0\in T$, если
\[
        P\left(X(t)\underset{t\to t_0}\lra X(t_0)\right)=1.
\]

\item Процесс {\em непрерывен по вероятности} (или {\em стохастически
непрерывен}) в точке $t_0$, если $P(|X(t)-X(t_0)|\ge\ep)$ при $t\to t_0$ для
$\forall\ep>0$. (Иначе, $X(t)\underset{t\to t_0}{\stackrel{P}{\lra}}X(t_0)$).

\item Процесс {\em непрерывен в среднем квадратичном} в точке $t_0$, если
\[
        E(X(t)-X(t_0))^2\underset{t\to t_0}\lra 0
\]
(или l.i.m$_{t\to t_0}X(t)=X(t_0)$).

\item Процесс {\em непрерывен слабо} (или {\em по распределению}) в точке
$t_0$, если
\[
        F_t(x)\underset{t\to t_0}\lra F_{t_0}(x)
\]
(в точках непрерывности предельного распределения $F_{t_0}$).
\end{enumerate}

Процесс (в соответствующем смысле) {\em непрерывен} (или {\em непрерывен на
$T$}), если указанное свойство непрерывности выполнено в любой точке
$t_0\in T$.

\begin{problem} Как связаны между собой введенные выше 5 свойств непрерывности?
\end{problem}
