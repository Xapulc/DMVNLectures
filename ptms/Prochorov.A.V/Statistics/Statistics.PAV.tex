\documentclass[a4paper,draft]{article}
\usepackage{dmvn}
\usepackage[unicode]{hyperref}
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{bookmark}
\usepackage{pgfplots}
\usepackage{csquotes}

\hypersetup{
    final, 
    linktoc=all,
    colorlinks=true,
    linkcolor=blue,
    urlcolor=cyan,
}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\newcommand{\raP}{\stackrel{\Pf}{\ra}}
\newcommand{\rad}{\xrightarrow{\mathrm{\:d\:}}}
\newcommand{\raPnrai}{\xrightarrow[n\rightarrow\infty]{\Pf}}
\newcommand{\radnrai}{\xrightarrow[n\rightarrow\infty]{\mathrm{d}}}
\newcommand{\ranrai}{\xrightarrow[n\rightarrow\infty]{}}
\newcommand{\nrai}{n\ra\infty}

\newcommand{\iii}{\int\limits_{-\infty}^{+\infty}}

% Буквы с крышкой
\newcommand{\hatFn}{\widehat{F}_n}
\newcommand{\hatRn}{\widehat{R}_n}
\newcommand{\hattheta}{\hat{\theta}}

% Жирная тэта
\newcommand{\btheta}{\boldsymbol{\theta}}

% Нормальное распределение
\newcommand{\Norm}{\mathcal{N}}

\tocsubsectionparam{2.6em}
\tocsubsubsectionparam{3.5em}

% Для интервальных оценок
\def\utheta{{\underline\theta}}
\def\otheta{{\overline\theta}}
\def\ulp{\underline p}
\def\olp{\overline p}

% Modified by Steve L. Kuznetsov and Alexander V. Kharitonov
% Minor lazha correction by Timofey A. Arkhangelsky, January 2007

\begin{document}
\dmvntitle{Курс лекций по}{математической статистике}{Лектор\т Александр Владимирович Прохоров}
{III курс, 5 семестр, поток математиков}{Москва, 2019 г.} \pagebreak

\tableofcontents

\pagebreak
\section*{Предисловие}

Ну что Вам рассказать про Сахалин?\dots Это незавершённый курс, который,
вообще  говоря, можно до\TeX ать. Но у нас нет на это времени.
Если у кого-то из читателей найдутся силы и желание это делать,
это будет просто замечательно. Исходные тексты будут выданы всякому,
кто захочет набрать остатки.

\subsubsection*{Дополнение к предисловию}
Текст существенно дополнен С.\,Л.\,Кузнецовым и А.\,В.\,Харитоновым.
Работа постепенно продвигается к завершению. Нам ещё много нужно сделать,
но многое мы уже сделали. Спасибо Тиме Архангельскому и Владимиру Гаврилину
за обнаружение и исправление лажи.
\begin{flushright}\it С.К., А.Х.\end{flushright}

\subsubsection*{Дополнение к предисловию 2}
Документ переработан В.\,В.\,Харламовым 
под курс лекций 2019 года.
{\bf Важно}, что дальнейший текст не является в полной мере
конспектом, но в то же время есть хорошее приближение материала лекций.
Некоторые доказательства и примеры могут отличаться от данных в аудитории.

В \hyperref[tasks]{конце} читатель может найти список 
экзаменационных вопросов 2019 года
и примерное соответствие их с материалом.
Так как DMVN не подаёт признаков жизни,
то огромная просьба писать на 
\href{vi.v.kharlamov@gmail.com}{мою почту}
при обнаружении ошибок. 
Самая актуальная версия конспекта находится
на \href{https://github.com/Xapulc/DMVNLectures/tree/master/ptms/Prochorov.A.V/Statistics/generated/statistics-prochorov.pdf}{Github}.
В 2020 году Bitbucket прекращает поддержку репозиториев
с системой контроля версий Mercurial, поэтому я скопировал 
репозиторий на Github.

От себя:
\begin{displayquote}
 \textit{Математическая статистика -- важный раздел современной математики.
С одной стороны, хорошее понимание материала даёт возможность 
применять всевозможные статистические пакеты и понимать что происходит внутри.
С другой стороны, математическая статистика красива сама по себе:
в данном курсе надо будет вспомнить немного комбинаторики, 
действительного анализа, линейной алгебры. Будет довольно много 
теории вероятностей и математического анализа.
Наш курс математической статистики обозревает основные статистические задачи
и вводит в их суть.
Уверен, что хорошее понимание курса может быть полезно во многих профессиях,
куда может пойти математик.}
\end{displayquote}
\begin{flushright}\it В.Х.\end{flushright}

\medskip\dmvntrail


\pagebreak

\section{Краткий обзор курса}

Математическая статистика\т это наука, посвященная разработке {\it наиболее верного} вывода, 
основанного на {\it неизвестных} закономерностях.
В данном курсе рассматривается 
{\it параметрическая статистика},
где мы {\it пытаемся по имеющимся данным
оценить значения параметров}. 
Далее в этом разделе приведены примеры
типичных задач параметрической статистики,
в которые мы углубимся в последующих разделах\footnote{
Этот раздел явно не включён в программу, однако
в нём даётся большинство основных определений,
а также разбираются первые примеры главных разделов статистики~--- 
\emph{примеч. В.\,Х.}}.

Напомним некоторые основные определения из курса теории
вероятностей.

\begin{df}
\emph{Пространством элементарных событий} называется множество исходов некоторого эксперимента. 
\emph{Элементарным событием} называется любой
элемент пространства элементарных событий. 
\emph{Событием} называется
любое подмножество пространства элементарных событий.
\emph{Экспериментом} называется функция, 
принимающая значение на
пространстве элементарных событий.
\end{df}

\begin{df}
\emph{Генеральной совокупностью} называется достаточно большое, быть может, бесконечное подмножество элементарных событий.
\end{df}

\begin{df}
\emph{Случайной величиной} называют функцию от элементарного события.
\end{df}

\subsection{Модель конечного случайного выбора}

Рассмотрим модель \лк Выбор без возвращения\пк. Пусть $N$ ---
общее число элементов генеральной совокупности, $M$ --- число
отмеченных (каким-то свойством) элементов, $n$ --- размер
выборки, \те число элементов, выбранных из генеральной
совокупности, $m$
--- число отмеченных элементов в выборке.

Вероятностная задача рассматривает случай, когда $n$, $M$ и $N$
заданы, а $m \in \hc{0 \sco \min(n,M)}$.
Тогда вероятность того, что среди выборки размера $n$ окажется
ровно $m$ отмеченных элементов, может быть вычислена по известной
формуле
$$Q_{n,m}(N,M)=\frac{\Cb_M^m \Cb_{N-M}^{n-m}}{\Cb_N^n}.$$

Статистическая задача ставится несколько иначе. Например:

а) Допустим, что $n$, $m$, $N$ известны, а $M$ --- неизвестно.
Требуется оценить $M$. Это в некотором смысле задача, обратная
вероятностной. Решить ее не так-то просто. Простейшее (но довольно
грубое) приближение для $M$ можно найти, например, из соотношений
$$\frac{M}{N}\approx\frac{m}{n},\quad M\approx\frac{m}{n}N.$$
Для того, чтобы найти более точные оценки, нужны специальные
методы, которыми и занимается математическая статистика.

б) Пусть заданы $n$, $m$ и $M$, а $N$ неизвестно. Требуется
оценить $N$. Пример такой задачи\т оценка числа рыб в водоеме:
производится выборка размера $M$, помечаются все рыбы из этой
выборки, а спустя некоторое время производится еще одна выборка
размера $n$ и подсчитывается число помеченных рыб $m$ из этой
выборки. По этим данным требуется оценить число рыб в водоеме.
Для решения этой задачи рассматривается вероятность
$\wt{Q}_{n,m}(N)$ как функция переменной $N$. Оказывается, что
функция $\wt{Q}$ сначала возрастает, а затем убывает. В качестве
оценки искомого значения $N$ выбирается такое целое $N_*$, для
которого $\wt{Q}_{n,m}(N)$ максимально. Можно показать, что
$$N_*=\hs{M\frac{n}{m}}\le M\frac{n}{m}.$$


Рассмотрим следующий эксперимент: два раза независимо друг от
друга бросается монетка. Можно рассматривать две модели этого
эксперимента:

1) 4 исхода: выпали последовательно орел--орел, орел--решка,
решка--орел, решка--решка. Каждому исходу приписывается
вероятность $\frac{1}{4}$.

2) 3 исхода: 2 орла, 2 решки, 1 орел и 1 решка; каждому исходу
приписывается вероятность $\frac{1}{3}$.

Практика показывает, что первая модель более соответствует
действительности, чем вторая: при большом числе испытаний каждый
из четырех исходов появляется с частотой, близкой к $\frac{1}{4}$,
в то время как во второй модели последний исход появляется с
частотой, близкой к $\frac{1}{2}$, а первые два --- с частотой
$\frac{1}{4}$, что плохо соответствует приписанным вероятностям.

В некотором смысле задача математической статистики обратна задаче
теории вероятностей. В теории вероятностей в каждой конкретной
ситуации вероятность считается полностью определенной и основной
задачей теории вероятностей является разработка методов нахождения
вероятностей различных сложных событий (исходя из известных
вероятностей более простых событий) для данной вероятностной
модели. В математической статистике рассматривается статистическая
модель, которая описывает такие ситуации, когда в вероятностной
модели изучаемого эксперимента имеется та или иная
неопределенность в задании вероятности, и задача математической
статистики состоит в том, чтобы уменьшить эту неопределенность,
уточнить (выявить) структуру статистической модели по результатам
проводимых наблюдений.

\subsection{Статистическая модель}

Фундаментальным понятием теории вероятностей является
вероятностная модель (вероятностное пространство)~--- это тройка
$(\Om,\As,\Pf)$, где $\Om$ --- пространство элементарных событий,
$\As$~--- $\sigma$-алгебра подмножеств этого пространства
(событий), $\Pf$~--- вероятностная мера на $\sigma$-алгебре $\As$.

Основным объектом исследования математической статистики является
статистическая модель. Определим это понятие.

Результатом статистического эксперимента являются вещественные
числа $x_1,\ldots,x_n$~--- статистические данные. Это
значения случайных величин $\xi_1,\ldots,\xi_n$. Их совокупность
$x^{(n)}=(x_1,\ldots,x_n)$ называется \emph{выборкой} размера (порядка) $n$.

\begin{df}
\emph{Статистической моделью} называется тройка\footnote{В общем смысле
$\Xs$ - множество значений случайных величин $\xi_i$, 
заданных на $(\Om, \As)$ и измеримых относительно $(\As,\Bs(\Xs))$.
Случайные величины и параметрическое семейство вероятностных мер порождают
вероятностные меры на $(\Xs, \Bs(\Xs))$, которые обозначаются 
как $\Ps$~--- \emph{примеч. В.\,Х.}} $(\Xs,\Bs(\Xs),\Ps)$, где
$\Xs=\{x^{(n)}\}$~--- выборочное пространство, \те совокупность
всевозможных выборок размера $n$, $\Bs(\Xs)$~--- $\sigma$-алгебра на
выборочном пространстве, $\Ps=\{\Pf\}$~--- некоторое \emph{семейство}
распределений вероятностей, заданное на $\Bs(\Xs)$.
\end{df}
В простейшем случае
считаем, что $\Xs\! \subseteq \mathbb{R}^m$, а $\Bs(\Xs)$ --- борелевская
$\sigma$-алгебра.

Примерами семейств распределений могут служить, например,
семейство бернуллиевских распределений, $p\in(0,1)$; семейство
пуассоновских распределений, $\lambda\in(0,\infty)$; семейство
биномиальных распределений с параметрами $(n,p)$, где $n$
фиксировано, а $p\in(0,1)$ и т.д.


Наша цель\т выделить из семейства распределений то единственное
распределение, которое наилучшим образом соответствует нашим
запросам, точнее, полученной выборке (после этого мы сможем работать
с вероятностной моделью).

Если $\Ps=\{\Pf_\theta \mid \theta\in\Theta\}$, где $\theta$~---
параметр, $\Theta\subset\R^m$~--- параметрическое множество, то
говорят, что $\Ps$~--- \emph{параметрическое семейство распределений},
а $(\Xs, \Bs(\Xs), \Ps)$~--- \emph{параметрическая модель}.

Пусть имеется случайный вектор $\xi^{(n)}=\{\xi_1,\ldots,\xi_n\}$
со значениями $(x_1,\ldots,x_n)$ в выборочном пространстве
$\Xs\!\!$.
В соответствии с определением статистической модели, $\Ps$~---
семейство распределений случайного вектора $\xi^{(n)}$.
Чтобы не путать набор случайных величин (случайный вектор)
$\xi_1,\ldots,\xi_n$ с его конкретными значениями
$x_1,\ldots,x_n$, говорят, что $x_1,\ldots,x_n$\т выборка, а
$\xi_1,\ldots,\xi_n$ $(\xi_i=\xi_i(\om))$\т случайная выборка.

\subsection{Статистические решения}

Допустим, наша цель найти функцию $\delta: \Xs \to D$,
чтобы по имеющейся выборке $x^{(n)}$ мы могли принимать 
решение $d = \delta(x^{(n)})$. 

\begin{df}
Такая функция $\delta: \Xs \to D$ будет называться
\emph{решающим правилом}, а элементы $D$ ---
\emph{статистическими решениями}. 
\end{df}

Множество статистических решений $D$ может быть самым разным.
Мы можем решать, будет ли снег на улице или нет
($D = \{\text{будет снег}, \text{снега не будет}\}$,
$\Xs$ -- какие-то метеорологические наблюдения);
какая температура будет завтра днём
($D = \{(t_{\min}, t_{\max}) \mid t_{\min} \leq t_{\max}\}$,
$\Xs$ -- то же самое),
в какой из выходных будет самая лучшая погода для рыбалки
($D = \{\text{какой-то день} \mid \text{день является выходным}\}$,
$\Xs$ -- то же самое). 

Чтобы формализовать "правильность" решающего правила $\delta$,
мы вводим понятие \emph{функции потерь} $\Ls(d, \theta)$,
где $\Ls$ -- измерима на $D \times \Theta$. Так как при разных значениях
выборки функция потерь может принимать разные значения, то вводится понятие
\emph{функции риска} как функции от решающего правила 
$R(\delta, \theta) := \Mf_{\theta} \Ls(\delta, \theta)$.
Здесь $\Ls(\delta, \theta)$ уже является случайной величиной.

\subsection{Типы статистических решений. 
Статистическая модель схемы Бернулли}

Зафиксируем число $n\in\mathbb{N}$. Рассмотрим случайные величины
$\xi_1(\om)$, $\xi_2(\om)$, \ldots, $\xi_n(\om)$ на некотором общем
вероятностном пространстве $(\Om,\As,\Pf)$, $\om\in\Om$. Их
совместное распределение:
$$\Pf(\xi_1=a_1,\xi_2=a_2,\ldots,\xi_n=a_n)=p^{a_1+\ldots+a_n}q^{n-(a_1+\ldots+a_n)},\quad
a_k\in\{0,1\}, \quad p,q\ge0,\quad p+q=1.
$$

Значение случайной величины $\xi_1$ --- исход первого испытания,
$\Pf(\xi_1=1)=p$, $\Pf(\xi_1=0)=q=1-p$, и аналогично для
$\xi_2,\ldots,\xi_n$. Отсюда $\Pf(\xi_1=a_1)=p^{a_1}q^{1-a_1}$, и
т.д. Значит,
$$\Pf(\xi_1=a_1,\xi_2=a_2,\ldots,\xi_n=a_n)=p^{a_1+\ldots+a_n}q^{n-(a_1+\ldots+a_n)}=
\prod\limits_{k=1}^n
\bbr{p^{a_k}q^{1-a_k}}=\Pf(\xi_1=a_1)\ldots\Pf(\xi_n=a_n).$$

Отсюда следует, что $\xi_1$, $\xi_2$, \ldots, $\xi_n$~---
независимые испытания.

Рассмотрим случайную величину $S_n=\xi_1+\ldots+\xi_n$. Она имеет
биномиальное распределение:
$$\Pf(\xi_1+\ldots+\xi_n=m)=\Cb_n^m p^m q^{n-m},\quad
S_n=\xi_1+\ldots+\xi_n,\quad \Mf S_n=np, \quad \Df S_n=npq.$$

Задача математической статистики~--- оценить неизвестное значение
$p$. Для этого используются три подхода~--- точечная оценка,
интервальная оценка и выбор из двух гипотез. Продемонстрируем
каждый из них на примере схемы Бернулли.

\subsubsection{Точечная оценка}

Пусть мы ищем значение параметра распределения $\theta \in \Theta$
по значениям данной нам выборки.
Тогда $d := \theta$, $D := \Theta$. В этом случае $\delta$ называется
\emph{статистической оценкой}.

Типичной функцией потерь для данного примера является 
$\Ls(d, \theta) := (d - \theta)^2$ --- квадратичная ошибка.
Тогда 
$$
R(\delta, \theta) = \Mf_{\theta} \Ls(d, \theta) =
\Mf_{\theta} (\delta - \theta)^2
$$

Такая функция риска называется \emph{среднеквадратичной ошибкой}.

Логично утверждать, что если для любого параметра $\theta \in \Theta$
верно $R(\delta_1, \theta) \leq R(\delta_2, \theta)$, то
$\delta_1$ \emph{не хуже}, чем $\delta_2$. Если ещё существует такой
параметр $\theta_0 \in \Theta$, что 
$R(\delta_1, \theta) < R(\delta_2, \theta)$, то $\delta_1$ 
\emph{лучше}, чем $\delta_2$\footnote{
Пытливый читатель может заметить, что мы ввели лишь отношение 
частичного порядка, где не все оценки сравнимы. 
Но всё же хочется уметь сравнивать произвольные оценки.
Ответ на то, как это делать, даётся в \hyperref[bayes_est]{байесовской
теории оценивания}~--- \emph{примеч. В.\,Х.}}.

\begin{df}
Если в некотором классе $\widetilde{\Theta} \subset \Theta$
существует такая $\delta^{*}$, что 
$R(\delta^{*}, \theta) \leq R(\delta, \theta)$ для любого параметра
$\theta \in \widetilde{\Theta}$ и для любой оценки $\delta$, то
$\delta^{*}$ называется \emph{оценкой с равномерно наименьшим риском}.
\end{df}

Вернёмся к бернуллиевскому случаю.
Запишем закон больших чисел для схемы Бернулли:
$$\frac{S_n}{n}=\frac{\xi_1+\ldots+\xi_n}{n}\raP \Mf\frac{S_n}{n}=\frac{np}{n}=p,
\quad n\to\infty.$$
\те частота появления успешного исхода $\frac{S_n}{n}$ сходится
по вероятности к параметру $p$: $\frac{S_n}{n}\raP p$,
$n\to\infty$.

Возьмем в качестве оценки параметра $p$ эту частоту
$\frac{S_n}{n}=:\hat{p}_n$. Это случайная величина со значениями $\frac{m}{n}$, $m=0,\ldots,n$.
Проведём небольшое численное моделирование $\hat{p}_n$

$$\begin{array}{|r|c|c|c|c|}
  \hline
           & n=10 & n=100 & n=1000 & n=10000  \\ \hline
  p = 0.15 & 0.2  & 0.21  & 0.146  & 0.1486   \\ \hline
  p = 0.5  & 0.5  & 0.48  & 0.477  & 0.5023   \\ \hline
  p = 0.85 & 0.8  & 0.87  & 0.844  & 0.8496   \\ \hline
\end{array}$$ 

Как мы видим, среднее и правда приближает реальное 
значение параметра, причём в общем случае чем больше номер,
тем больше точность (но не всегда это будет верно в силу
случайности величин).

\begin{theorem} Эта оценка обладает следующими свойствами:

1) Несмещенность: $\Mf \hat{p}_n=p$.

2) Состоятельность: $\hat{p}_n\raP p$, $n\to\infty$.

3) Оптимальность: Дисперсия частоты $\hat{p}_n$ является
наименьшей среди дисперсий всех других оценок, которые обладают
свойством 1)\footnote{Точнее, оценка является эффективной, а доказательство
этого факта идейно неотличимо от более общего 
\hyperref[task11]{неравенства Крамера-Рао}~--- \emph{примеч. В.\,Х.}}.
\end{theorem}

\begin{proof}
Выше уже было показано, что оценка $\hat{p}_n$ несмещенная (это
следует из того, что $\Mf S_n=np$), а в силу закона больших чисел
для схемы Бернулли она состоятельна; тем самым свойства 1) и 2)
доказаны.

Докажем свойство 3)~--- оптимальность. Пусть $\tilde{p}_n$~---
любая оценка параметра $p$, удовлетворяющая условиям 1) и 2)
(несмещённость и состоятельность). Рассмотрим величину $\Mf
(\tilde{p}_n-p)^2$.

Для несмещенных оценок средняя квадратическая ошибка совпадает с
дисперсией, в частности для нашей оценки $\hat{p}_n$: $\Mf
(\hat{p}_n-p)^2=\Df \hat{p}_n$.

Обозначим
$$\Pf(\xi_1=a_1,\xi_2=a_2,\ldots,\xi_n=a_n)=p^{a_1+\ldots+a_n}
(1-p)^{n-(a_1+\ldots+a_n)}=g(p;a_1,\ldots,a_n).$$
Для любого $p\in(0,1)$ имеет место равенство
$\sum\limits_{(a_1,\ldots,a_n)}g(p;a_1,\ldots,a_n)\equiv 1.$
Условие несмещенности оценки означает, что
$\Mf\tilde{p}_n=\sum\limits_{(a_1,\ldots,a_n)}\tilde{p}_n(a_1,\ldots,a_n)g(p;a_1,\ldots,a_n)=p.$
Рассмотрим $g(p;a_1,\ldots,a_n)=g(p)$ как функцию параметра $p$.
Тогда наши условия могут быть записаны в следующем виде
(суммирование ведется по всем наборам $(a_1,\ldots,a_n)$):
$$\left\{\begin{array}{l}
  \sum g(p)\equiv 1, \\
  \sum \tilde{p}_n\cdot g(p)\equiv p;
\end{array}\right.\quad 0<p<1.
$$

Продифференцируем каждое из этих соотношений по $p$, а затем,
умножив первое на $p$, вычтем его из второго; получим:
$$\sum (\tilde{p}_n-p) g'_p(p)\equiv 1.$$
Теперь представим $g'_p(p)$ как логарифмическую производную:
$g'_p(p)=g(p)\frac{\partial\ln g(p)}{\partial p}$, а затем
применим неравенство Коши--Буняковского, представив $g(p)$ в виде
$g(p)=\sqrt{g}\sqrt{g}$:
$$1\equiv \hr{\sum (\tilde{p}_n-p)g(p)\frac{\partial\ln g(p)}{\partial p}}^2
\le\hr{\sum (\tilde{p}_n-p)^2 g(p)}\hr{\sum \hs{\frac{\partial\ln
g(p)}{\partial p}}^2 g(p)}.$$

Так как $\Mf\tilde{p}_n=p$ (условие несмещённости оценки
$\tilde{p}_n$), то первый из множителей в правой части
неравенства~--- это дисперсия $\tilde{p}_n$. Обозначим второй
множитель через $I(p)$, тогда неравенство перепишется в виде
$1\le\Df\tilde{p}_n\cdot I(p)$, или
$\Df\tilde{p}_n\ge\frac{1}{I(p)}$. Найдем $I(p)$ в явном виде:
$$I(p)=\Mf\hs{\frac{\sum\xi_k}{p}-\frac{n-\sum\xi_k}{1-p}}^2=
\frac{\Mf\hr{\sum\xi_k-np}^2}{p^2(1-p)^2}=\frac{\Df\hr{\sum\xi_k}}{p^2(1-p)^2}=
\frac{np(1-p)}{p^2(1-p)^2}=\frac{n}{p(1-p)}.$$ Подставляя
найденное значение $I(p)$ в неравенство для дисперсии, получаем:
$\displaystyle\Df\tilde{p}_n\ge\frac{p(1-p)}{n}=\Df\hat{p}_n,$
\те оценка~$\hat{p}_n$ действительно обладает наименьшей
дисперсией из всех несмещенных состоятельных оценок $\tilde{p}_n$.
Теорема доказана.
\end{proof}

Если задана произвольная оценка $\hat{p}_n$ параметра $p$, то
представим ее математическое ожидание $\Mf \hat{p}_n$ в виде $\Mf
\hat{p}_n=p+\Delta_n$. Тогда $\Delta_n$ называется смещением
оценки $\hat{p}_n$. Несмещенные оценки обладают нулевым смещением:
$\Delta_n=0$.

Для нашей оценки $\hat{p}_n=\frac{S_n}{n}$, очевидно, $\Df
\hat{p}_n\to0$, $n\to\infty$, \те частота обладает наименьшим
рассеянием, если рассеяние измеряется с помощью дисперсии.

\subsubsection{Интервальная оценка}

Пусть теперь мы ищем некоторый интервал значений параметра, где скорее 
всего и лежит параметр распределения. Тогда $d := [\theta_1, \theta_2]$,
$D := \{[\theta_1, \theta_2] \mid \theta_1, \theta_2 \in \Theta\}$.
В данном случае удобно рассмотреть следующую функцию потери
$$
\Ls(d, \theta) :=
\begin{cases}
0, & \theta \in [\theta_1, \theta_2] \\
1, & \theta \notin [\theta_1, \theta_2]
\end{cases}
$$

Для неё 
$$
R(\delta, \theta) = \Mf_{\theta} \Ls(\delta, \theta) =
\Pf_{\theta}(\theta \notin \delta) =
\Pf_{\theta}(\theta \notin [\theta_1(\delta), \theta_2(\delta)])
$$
Такая оценка $\delta = [\theta_1(\delta), \theta_2(\delta)]$ называется
\emph{интервальной статистической оценкой} параметра $\theta$ 
(\emph{доверительным интервалом}).

Мы хотим, чтобы риск был поменьше (прогноз погоды был верен), для
этого строим такую оценку, чтобы $R(\delta, \theta) \leq \alpha$,
где $0 < \alpha < 1$. Такой $\alpha$ называется \emph{коэффициентом
доверия}, а $1 - \alpha$ -- \emph{доверительной вероятностью} 
(\emph{доверительным уровнем}). При этом мы хотим, чтобы длина
интервала была поменьше (прогноз погоды был конкретен), то есть
чтобы 
$$
R_2(\delta, \theta) := \Mf_{\theta} (|\theta_2(\delta) - \theta_1(\delta)|) \leq
\beta < \alpha
$$

\begin{ex}
Рассмотрим $n=100$ бросаний правильной монеты (схема Бернулли с
параметром $p=0{,}5$), $x_k$~--- исход $k$-го испытания (значение
бернуллиевской случайной величины $\xi_k$);
$S_n=\xi_1+\ldots+\xi_n$. Очевидно, $\Pf(0\le S_n\le 100)=1$.
Прямой подсчет вероятностей показывает, что $$\Pf(35\le S_n\le
65)=0{,}99822;\quad \Pf(39\le S_n\le 61)\approx 0{,}98.$$

Таким образом, $\bs{\frac{35}{100},\frac{65}{100}}$~---
доверительный интервал для параметра $p$ с доверительной
вероятностью $0{,}99822$; а
$\bs{\frac{39}{100},\frac{61}{100}}$~--- доверительный интервал
для параметра $p$ с доверительной вероятностью $\approx 0{,}98$.
\end{ex}

Для построения доверительного интервала для схемы Бернулли запишем
для оценки $\hat{p}_n=\frac{S_n}{n}$ неравенство Чебышёва:
$$\Pf\hr{\bbm{\frac{S_n}{n}-\Mf\frac{S_n}{n}}\ge\ep}\le\frac{\Df\frac{S_n}{n}}{\ep^2}
\quad \Longrightarrow \quad
\Pf\hr{\bbm{\frac{S_n}{n}-\Mf\frac{S_n}{n}}\le\ep}\ge
1-\frac{\Df\frac{S_n}{n}}{\ep^2},$$
$$\Pf\br{\bm{\hat{p}_n-p}\le\ep}\ge 1-\frac{pq}{n\ep^2}\ge
1-\frac{1}{4n\ep^2}\ge 1-\al, \quad \al=\frac{1}{4n\ep^2}.$$

Теперь зададим произвольное $\al\in(0,1)$. Тогда для
$\ep_\al=\sqrt{\frac{1}{4n\al}}$ получим:
$$\Pf\br{\bm{\hat{p}_n-p}\le\ep_\al}\ge 1-\al, \quad\mbox{ \те }
\quad\Pf\br{\hat{p}_n-\ep_\al\le p\le\hat{p}_n+\ep_\al}\ge
1-\al.$$

Таким образом, мы построили интервал $[\hat{p}_n-\ep_\al,\hat{p}_n+\ep_\al]$, в котором с задаваемой нами
вероятностью ошибки $\al$ находится неизвестный параметр $p$. 
Чем меньше мы выбираем
$\al$, тем больше этот интервал. Для заданного $\al$ длину интервала можно уменьшить за счёт увеличения числа
испытаний $n$.

Оценим вероятность ошибки $\alpha$ в конкретном примере
для разных $\alpha$ и $p$, проведя $m=1000$ раз 
по $n=1000$ испытаний. Для этого воспользуемся
полученным доверительным интервалом

$$\begin{array}{|r|c|c|c|c|}
\hline
       & \alpha=0.2 & \alpha=0.3 & \alpha=0.5  \\ \hline
p=0.15 & 0.001      & 0.009      & 0.045       \\ \hline
p=0.5  & 0.026      & 0.07       & 0.138       \\ \hline
p=0.85 & 0.002      & 0.01       & 0.056       \\ \hline
\end{array}$$ 

Видно, что наши оценки были очень грубыми.
Укажем еще один (более точный) способ нахождения интервальной
оценки в схеме Бернулли. По теореме Муавра--Лапласа число <<успехов>> схемы Бернулли с ростом $n$ стремится к нормальной
случайной величине:
\eqn{\frac{S_n-\Mf S_n}{\sqrt{\Df S_n}}=\frac{S_n-np}{\sqrt{n pq}}\stackrel{\Dc}{\to}
\Nc(0,1).}

Используя это, можно оценить вероятность
$$\textstyle\Pf\br{\bm{\hat{p}_n-p}\le\ep}=\Pf\bbr{\bm{\frac{S_n-np}{n}}\sqrt{\frac{n}{pq}}
\le\ep\sqrt{\frac{n}{pq}}}\simeq
\Phi\bbr{\ep\sqrt{\frac{n}{pq}}}-\Phi\bbr{-\ep\sqrt{\frac{n}{pq}}}=2\Phi(u)-1,
\quad u=\ep\sqrt{\frac{n}{pq}}.
$$

Здесь $\Phi(u)=\frac{1}{\sqrt{2\pi}}\int\limits_{-\infty}^u
e^{-\frac{t^2}{2}}\,dt$~--- функция распределения стандартной
нормальной случайной величины.

Фиксируем $0<\al<1$. Нам нужно, чтобы $2\Phi(u)-1=1-\al$, \те
$\Phi(u)=1-\frac{\al}{2}$. Обозначим такое значение $u$, при
котором это выполнено, через $u_{1-\frac{\al}{2}}$ (квантиль
порядка $1-\frac{\al}{2}$ нормального распределения, находится из
таблицы квантилей). Тогда искомое $\ep$ найдем из условия
$\ep\sqrt{\frac{n}{pq}}=u_{1-\frac{\al}{2}}$;
$\ep=u_{1-\frac{\al}{2}}\sqrt{\frac{p(1-p)}{n}}$. Итак,
неравенство
\eqn{\bm{\hat{p}_n-p}\le u_{1-\frac{\al}{2}}\sqrt{\frac{p(1-p)}{n}}}
выполняется с вероятностью $\simeq 1-\alpha$. Осталось найти границы
доверительного интервала. Возведем неравенство в квадрат:

$${\br{\hat{p}_n-p}}^2\le
u_{1-\frac{\al}{2}}^2\frac{p(1-p)}{n}.$$

Получили квадратное уравнение на $p$. В качестве $\bar p_n$ и
$\bar{\bar{p}}_n$ берут корни этого квадратного уравнения (можно
показать, что всегда $D>0$ и корней действительно два).

Как и ранее, оценим вероятность ошибки $\alpha$ 
в конкретном примере
для разных $\alpha$ и $p$, проведя $m=1000$ раз 
по $n=1000$ испытаний. Для этого воспользуемся
новым доверительным интервалом

$$\begin{array}{|r|c    |c|c|}
\hline
       & \alpha=0.2 & \alpha=0.3 & \alpha=0.5  \\ \hline
p=0.15 & 0.202      & 0.335      & 0.489       \\ \hline
p=0.5  & 0.196      & 0.298      & 0.508       \\ \hline
p=0.85 & 0.217      & 0.301      & 0.497       \\ \hline
\end{array}$$ 

\subsubsection{Выбор из двух гипотез}

Пусть теперь мы имеем $\Theta := \{\theta_1, \theta_2\}, \theta_1 < \theta_2$.
Мы хотим выбрать наиболее вероятный из этих параметров.
Тогда $d := \theta, D := \Theta$. В этом случае введём следующую 
функцию потерь
$$
L(d, \theta) :=
\begin{cases}
0, & d = \theta \\
1, & d \ne \theta
\end{cases}
$$
Тогда 
$$
R(\delta, \theta) = \Mf_{\theta} \Ls(\delta, \theta) =
\Mf_{\theta} I_{\delta = \theta}
$$
Пусть задано $\theta_0$. Рассмотрим две (взаимоисключающие) гипотезы о
параметре $\theta$: $H_0$ (основная, или нулевая, гипотеза) и $H_1$
(альтернативная, или конкурирующая, гипотеза). (Например,
$H_0\colon \theta=\theta_0$, 
$H_1\colon \theta \neq \theta_0$.) Наша задача: выбрать из
этих двух гипотез ту, которой соответствует наименьшая вероятность
ошибки.

\begin{df}
\emph{Вероятность ошибки I рода} --- это вероятность отклонить верную
гипотезу $H_0$. \emph{Вероятность ошибки II рода} --- это вероятность
принять неверную гипотезу $H_0$.
\end{df}

\emph{Критерий проверки гипотезы} $H_0$ --- это правило, на основании
которого мы можем считать, что она верна или неверна (\те
принимаем ее или не принимаем). Составим таблицу:
$$\begin{array}{r|c|c|}
  & H_0 & H_1 \\ \hline
  \mbox{\small принимаем } H_0 &  & \quad\beta\quad \\ \hline
  \mbox{\small отклоняем } H_0 & \quad\alpha\quad &
\end{array}$$

Здесь вероятность ошибки I рода (отклоняем $H_0$ в то время как
она верна) обозначена через $\al$, а вероятность ошибки II рода
(принимаем неверную гипотезу $H_0$)~--- $\beta$.

Рассмотрим две гипотезы $H_0\colon p=p_0$, $H_1\colon p\neq p_0$
($p_0$ задано). Пусть для параметра $p$ получена интервальная
оценка для заданной вероятности ошибки $\al$~--- доверительный
интервал $[\bar p_n,\bar{\bar{p}}_n]$. Тогда можно предложить
такой критерий:

\quad 1) Если $p_0\in[\bar p_n,\bar{\bar{p}}_n]$, то $H_0$
принимаем (и соответственно, отклоняем $H_1$);

\quad 2) Если $p_0\not\in[\bar p_n,\bar{\bar{p}}_n]$, то $H_0$
отклоняем (и тем самым принимаем $H_1$).

Поскольку $[\bar p_n,\bar{\bar{p}}_n]$~--- доверительный интервал
с доверительной вероятностью $1-\al$, то вероятность ошибки I рода
не превосходит $\al$.

\begin{ex} \label{task38}
Рассмотрим еще один пример гипотез о параметре $p$ схемы Бернулли.
Пусть $H_0\colon p=p_0$, $H_1\colon p=p_1$, где $p_0<p_1$~---
заданы, и пусть $\al$~--- вероятность ошибки I рода, $\beta$~---
вероятность ошибки II рода. Как всегда, обозначаем
$S_n=\xi_1+\ldots+\xi_n$. Тогда существует такой критерий: если
$S_n>m_*$, то $H_0$ отклоняем (тем самым принимая $H_1$), а если
$S_n\le m_*$, то $H_0$ принимаем ($H_1$ отклоняем). Число $m_*$
называется критическим значением и находится из соображений
минимизации при фиксированном $n$ сумм (вероятностей ошибок I и II
рода).

Пусть заданы $0 < \alpha_0 < 1$, $0 < \beta_0 < 1$.
Попробуем в данном критерии дать нижнюю оценку $n_0$ на $n$, чтобы
для $n \geqslant n_0$ гипотезы могли быть различимы с вероятностями ошибок I и II
рода не больше $\alpha_0$ и $\beta_0$ соответственно для некоторого $m^*$.

Так как $\Pf_p(S_n = m) = C_n^m p^m (1-p)^{n-m}$,
то вероятности ошибок I и II рода выражаются как
$$
\alpha = \sum_{m > m^{*}} \Pf_{p_0}(S_n = m)= 
\sum_{m > m^{*}} C_n^m p_0^m (1-p_0)^{n-m},
$$
$$
\beta = \sum_{m \leqslant m^{*}} \Pf_{p_1}(S_n = m)= 
\sum_{m > m^{*}} C_n^m p_1^m (1-p_1)^{n-m}.
$$
Положим
$$
x_m := \frac{\Pf_{p_1}(S_n=m)}{\Pf_{p_0}(S_n=m)},
$$
$$
\widetilde{S}_0 := \sum_{m > m^{*}} \Pf_{p_0}(S_n=m) \ln x_m, \,
\widetilde{S}_1 := \sum_{m \leqslant m^{*}} \Pf_{p_0}(S_n=m) \ln x_m, \,
\widetilde{S} := \widetilde{S}_0 + \widetilde{S}_1,
$$
$$
\widetilde{\Sigma}_0 := \sum_{m > m^{*}} \Pf_{p_1}(S_n=m) \ln x_m, \,
\widetilde{\Sigma}_1 := \sum_{m \leqslant m^{*}} \Pf_{p_1}(S_n=m) \ln x_m, \,
\widetilde{\Sigma} := \widetilde{\Sigma}_0 + \widetilde{\Sigma}_1.
$$
Распишем выражение
\begin{multline*}
\widetilde{S} = \sum_{m=0}^n
\ln \left(\frac{\Pf_{p_1}(S_n=m)}{\Pf_{p_0}(S_n=m)}\right) \Pf_{p_0}(S_n=m) =
\sum_{m=0}^n \left(m \ln \frac{p_1}{p_0} +
(n-m) \ln \frac{1-p_1}{1-p_0}\right) \Pf_{p_0}(S_n=m) = \\
\ln \frac{p_1}{p_0} \underbrace{\sum_{m=0}^n m \Pf_{p_0}(S_n=m)}_{
=\Ef_{p_0} S_n = n p_0} +
\ln \frac{1-p_1}{1-p_0} \underbrace{\sum_{m=0}^n (n-m) \Pf_{p_0}(S_n=m)}_{
= n - \Ef_{p_0} S_n = n (1-p_0)} = 
n p_0 \ln \frac{p_1}{p_0} + n (1-p_0) \ln \frac{1-p_1}{1-p_0} = \\
-n \left(p_0 \ln \frac{p_0}{1 - (1-p_1)} + 
(1-p_0) \frac{1-p_0}{(1-p_1)}\right) = -n \varphi(p_0, 1-p_1),
\end{multline*}
где $\varphi$ - функция Кульбака 
$$
\varphi(a, b) = a \ln \frac{a}{1-b} + (1-a) \ln \frac{1-a}{b}.
$$
Эта функция положительна и убывает по каждой переменной.

Пользуясь неравенством Йенсена, получаем оценку
\begin{multline*}
\widetilde{S}_0 = \sum_{m > m^{*}} 
\ln \frac{\Pf_{p_1}(S_n=m)}{\Pf_{p_0}(S_n=m)} \Pf_{p_0}(S_n=m) \leqslant
\underbrace{\sum_{m > m^{*}} \Pf_{p_0}(S_n=m)}_{= \alpha}
\ln \left(\sum_{m > m^{*}} \frac{\Pf_{p_1}(S_n=m)}{\Pf_{p_0}(S_n=m)}
\frac{\Pf_{p_0}(S_n=m)}{\sum_{m > m^{*}} \Pf_{p_0}(S_n=m)}\right) =\\
\alpha \ln \left(\frac{1}{\alpha} \sum_{m > m^{*}} \Pf_{p_1}(S_n=m)\right) =
\alpha \frac{1-\beta}{\alpha}.
\end{multline*}
Аналогично $\widetilde{S}_1 \leqslant (1-\alpha) \ln (\beta / (1-\alpha))$.
Поэтому
$$
-n \varphi(p_0, 1-p_1) = \widetilde{S} = \widetilde{S}_0 + \widetilde{S}_1 \leqslant
\alpha \frac{1-\beta}{\alpha} + (1-\alpha) \frac{\beta}{1-\alpha} = 
- \varphi(\alpha, \beta),
$$
откуда
$$
n \geqslant \frac{\varphi(\alpha, \beta)}{\varphi(p_0, 1-p_1)} \geqslant
\frac{\varphi(\alpha_0, \beta_0)}{\varphi(p_0, 1-p_1)} =: \gamma_0.
$$
Аналогично получаем оценку снизу
$$
n \geqslant \frac{\varphi(\alpha, \beta)}{\varphi(p_1, 1-p_0)} \geqslant
\frac{\varphi(\alpha_0, \beta_0)}{\varphi(p_1, 1-p_0)} =: \gamma_1.
$$
Искомый $n_0 := \max\{\gamma_0, \gamma_1\}$.
$$\begin{array}{|r|c    |c|c|}
\hline
p_0  & p_1  & n_0  & m^{*}  \\ \hline
0.4  & 0.5  & 150  & 70     \\ \hline
0.2  & 0.4  & 36   & 11     \\ \hline
0.01 & 0.02 & 1000 & 15     \\ \hline
\end{array}$$ 
\end{ex}

На этом мы завершаем обзор. Далее речь пойдёт подробнее о точечных оценках,
интервальных оценках и проверке гипотез.

\section{Эмпирическая функция распределения}
\subsection{Модель повторных испытаний}

\begin{df}
\emph{Моделью повторных испытаний} называется статистическая модель, в
которой случайные величины $\xi_1,\ldots,\xi_n$ (со значениями
$x_1,\ldots,x_n$ соответственно, $(x_1,\ldots,x_n)\in \Xs$)
независимы и одинаково распределены.
\end{df}

В дальнейшем мы будем рассматривать только модели повторных
испытаний.

\begin{ex}
Рассмотренная выше статистическая модель схемы Бернулли~--- модель
повторных испытаний. Действительно, в этом случае рассматриваются
независимые испытания с одним и тем же распределением вероятности
$\Pf(\xi=0)=p$, $\Pf(\xi=1)=1-p$, где $p\in(0,1)$~--- параметр.
\end{ex}

\begin{ex}
Рассмотрим эксперимент по измерению температуры. Мы считаем, что
измерения независимы и результаты измерений~--- значения одинаково
распределенных случайных величин $\xi_1,\ldots,\xi_n\sim\xi$. На
практике обычно результаты измерений колеблются около некоторого
постоянного значения $a$, поэтому удобно рассматривать $\xi$ в
виде $\xi=a+\Delta$, где $\xi=(\xi_1,\ldots,\xi_n)$,
$\Delta=(\Delta_1,\ldots,\Delta_n)$~--- случайная ошибка, или в
координатах: $\xi_k=a+\Delta_k$, $k=1,\ldots,n$. Случайные
величины $\Delta_1,\ldots,\Delta_n$ также независимы и одинаково
распределены; при этом $\Mf\Delta_k=0$, $\Df\Delta_k=\sigma^2$
$\fa k$. Средняя температура вычисляется как среднее
арифметическое результатов измерений:
$$\bar\xi_n=\frac{\xi_1+\ldots+\xi_n}{n}=a+\frac{\Delta_1+\ldots+\Delta_n}{n},\quad
\bar\Delta_n=\frac{\Delta_1+\ldots+\Delta_n}{n} \mbox{ --- средняя
ошибка, } \quad\Mf\bar\Delta_n=0,\quad
\Df\bar\Delta_n=\frac{\sigma^2}{n}.$$
\end{ex}

\subsection{Обоснование предельного перехода при стремлении размера
выборки к бесконечности}

В курсе математической статистики довольно часто нас интересуют
большие выборки, поэтому нам необходимо ``узаконить'' 
рассмотрение бесконечных выборок и вероятностных мер на 
$\mathbb{R}^{\infty}$.

Рассмотрим последовательность выборочных пространств
$$
(\mathbb{R}^1, \Bs(\mathbb{R}^1)), \dots, (\mathbb{R}^n, \Bs(\mathbb{R}^n)), \dots
$$
с вероятностными мерами $\Pf_1, \dots, \Pf_n, \dots$. Исследуем их
предельные свойства при $n \to \infty$.

\begin{df}
Вероятностные меры $\Pf_1, \dots, \Pf_n, \dots$ называются
\emph{согласованными}, если
$$
\forall\, n \in \mathbb{N} \quad \forall\, B \in \Bs(\mathbb{R}^n) \quad
\Pf_{n+1}(B \times \mathbb{R}) = \Pf_n(B)
$$
\end{df}

Введём пространство $(\mathbb{R}^{\infty}, \Bs(\mathbb{R}^{\infty}))$, где
$\mathbb{R}^{\infty} = \{ x = (x_1, \dots, x_n, \dots) \mid x_k \in \mathbb{R} \}$,
$\Bs(\mathbb{R}^{\infty})$~--- борелевская $\sigma$-алгебра.

\begin{df}
Пусть $B \in \Bs(\mathbb{R}^n)$. Тогда \emph{борелевским цилиндром} называется
следующее множество:
$$
Z_n(B) = \{ x = (x_1, \dots) \in \mathbb{R}^{\infty} \mid (x_1, \dots x_n) \in B \}.
$$
\end{df}

\begin{theorem}[Колмогоров]
Если меры на $\mathbb{R}^1, \dots, \mathbb{R}^n, \dots$ согласованы, то
существует единственная вероятностная мера $\Pf$ на
$(\mathbb{R}^{\infty}, \Bs(\mathbb{R}^{\infty}))$ такая, что $\Pf(Z_n(B)) =
\Pf_n(B)$ для всех $B \in \Bs(\mathbb{R}^n)$ и для всех натуральных
$n$.
\end{theorem}

Эта теорема обосновывает законность перехода к пределу при $n \to \infty$
($n$ --- размер выборки).

\subsection{Определение и свойства эмпирической
функции распределения}\label{task1}

\begin{df}
\emph{Эмпирической функцией распределения} для данной выборки
$x_1,\ldots,x_n$ называется функция
$$\hatFn(x;x_1,\ldots,x_n)=\frac{1}{n}\sum_{k=1}^n I_{(-\infty,x]}(x_k)=
\frac{1}{n}\sum_{k=1}^n I_{(x_k\le x)},$$ где
$I_A(x)=\left\{\begin{array}{cl}
  1, & x\in A; \\
  0, & x\not\in A.
\end{array}\right.$~--- индикатор множества $A$.
\end{df}

Перейдем от выборки $x_1,\ldots,x_n$ к вариационному ряду
(совокупности порядковых статистик); иными словами, упорядочим
выборку по возрастанию: $x_{(1)},\ldots,x_{(n)}$. Тогда, очевидно,
эмпирическая функция распределения может быть записана в виде
$$\hatFn(x;x_1,\ldots,x_n)=\hatFn(x;x_{(1)},\ldots,x_{(n)})=\left\{\begin{array}{cl}
  0, & x<x_{(1)}; \\
  \frac{k}{n}, & x_{(k)}\le x<x_{(k+1)}, 1\le k\le n-1; \\
  1 & x\ge x_{(n)}.
\end{array}\right.$$

\begin{center}
\begin{tikzpicture}
\begin{axis}[%
    xlabel=$x$,
    ylabel=$\text{Функция распределения $R[0, 1]$}$,
    xmin=-0.5,
    xmax=1.5,
    ymin=0,
    ymax=1,
    grid=major,
    legend pos=south east]
    \addplot[blue] coordinates {
    (-0.5,0)
    (0, 0)
    (1, 1)
    (1.5, 1)
    };
    \addplot[red] coordinates {
    (-0.5, 0)
    (0.04975858054422155,0) (0.04975858054422155,0.1)
    (0.12278047952000604,0.1) (0.12278047952000604,0.2)
    (0.15060927629193377,0.2) (0.15060927629193377,0.3)
    (0.3240913878050239,0.3) (0.3240913878050239,0.4)
    (0.4522994319056679,0.4) (0.4522994319056679,0.5)
    (0.49015499450886113,0.5) (0.49015499450886113,0.6)
    (0.5143191323458651,0.6) (0.5143191323458651,0.7)
    (0.6939590572643484,0.7) (0.6939590572643484,0.8)
    (0.9671882309097415,0.8) (0.9671882309097415,0.9)
    (0.9756251484127378,0.9) (0.9756251484127378,1)
    (1.5, 1)
    };
    \addlegendentry{Теоритическая}
    \addlegendentry{Эмпирическая}
\end{axis}
\end{tikzpicture}
\end{center}

Если выборка $x_1,\ldots,x_n$ фиксирована, то эмпирическая функция
распределения~--- это функция от переменной $x\in\R$: $\hatFn%
(x;x_1,\ldots,x_n)=\hatFn(x).$ Эта функция является функцией распределения
случайной величины, принимающей равновероятно значения $x_1, \ldots, x_n$
\footnote{Отсюда и берётся идея ЭФР. У нас есть только результаты некоторого
эксперимента, но нет никаких данных о том, что какое распределение у 
наблюдаемой случайной величины, поэтому мы полагаем, 
что эти точки могли возникать равновероятно~--- \emph{примеч. В.\,Х.}}.
Если в выборке $k$ значений совпадают, то вероятность этого значения
равна $k/n$.

Если же выборка не фиксирована, а случайные величины
$\xi_1,\ldots,\xi_n$, породившие эту выборку, независимы и
одинаково распределены с функцией распределения $F(x)$, то можно
рассматривать $\hatFn(x;\xi_1,\ldots,\xi_n)$. Для каждого
$x\in\R$ это случайная величина:
$\hatFn(x;\xi_1,\ldots,\xi_n)=\hatFn(x;\om)$. 

\begin{theorem}
1) Случайная величина $n\hatFn(x;\xi_1,\ldots,\xi_n)$ имеет
биномиальное распределение с параметрами $(n,p=F(x))$ при любом
фиксированном $x\in\R$;

2) $\hatFn(x;\xi_1,\ldots,\xi_n)$ является несмещенной
состоятельной оценкой $F(x)$;

3) $\Pf\bbr{\lim\limits_{n\to\infty}\hatFn
(x;\xi_1,\ldots,\xi_n)=F(x),\fa x\in\R}=1$;

4) $\hatFn
(x;\xi_1,\ldots,\xi_n)\stackrel{d}{\to}\Nc\br{F(x),\frac{F(x)(1-F(x))}{n}}$,
$n\to\infty$.
\end{theorem}

\begin{proof}
Найдем распределение случайной величины $\hatFn%
(x;\xi_1,\ldots,\xi_n)$ при любом фиксированном $x\in\R$. Если
$\xi_k\le x$, то $I_{(-\infty,x]}(\xi_k)=1$, а если $\xi_k>x$, то
$I_{(-\infty,x]}(\xi_k)=0$. Значит, при каждом $x\in\R$ $\hatFn%
(x;\xi_1,\ldots,\xi_n)$~--- это частота наступления события
$\{\xi_k\le x\}$, вероятность которого равна $\Pf(\xi_k\le
x)=F_{\xi_k}(x)=F(x)$. Отсюда получаем, что случайная величина
$n\hatFn(x;\xi_1,\ldots,\xi_n)$ имеет биномиальное распределение с
параметрами $(n,p=F(x))$ при любом фиксированном $x\in\R$
(утверждение 1) теоремы). Её математическое ожидание и дисперсия
соответственно равны $np$ и $np(1-p)$, поэтому получаем
\begin{equation} \label{matdisp_emp}
\Mf\hatFn(x;\xi_1,\ldots,\xi_n)=F(x), \quad \Df\hatFn%
(x;\xi_1,\ldots,\xi_n) =\frac{F(x)\br{1-F(x)}}{n}\quad \fa
x\in\R.
\end{equation}

Таким образом, эмпирическую функцию распределения $\hatFn%
(x;\xi_1,\ldots,\xi_n)$ можно рассматривать как оценку
(теоретической) функции распределения $F(x)$. Поскольку $\Mf\hatFn%
(x;\xi_1,\ldots,\xi_n)=F(x)$, то эта оценка несмещенная, а в
силу неравенства Чебышева $\Pf\br{|\hatFn%
(x;\xi_1,\ldots,\xi_n)-F(x)|\ge\ep}\le\frac{\Df\hatFn%
}{\ep^2}=\frac{p(1-p)}{n\ep^2}\to0,$ $n\to\infty$ $\fa x\in\R$,
\те $\hatFn(x;\xi_1,\ldots,\xi_n)\raP F(x)$, $n\to\infty$ $\fa
x\in\R$, значит эта оценка состоятельная. Таким образом,
утверждение 2) также доказано.

Утверждение 3) следует из УЗБЧ для схемы Бернулли (теорема
Бореля), а утверждение 4)~--- из формул (\ref{matdisp_emp}) и
центральной предельной теоремы для независимых одинаково
распределенных случайных величин, примененной к сумме $n\hatFn%
(x;\xi_1,\ldots,\xi_n)=\sum\limits_{k=1}^n
I_{(-\infty,x]}(\xi_k)$.
\end{proof}

\begin{problem}
Доказать, что $\hatFn(x;\xi_1,\ldots,\xi_n)$ является
оптимальной оценкой $F(x)$ $\fa x\in\R$.
\end{problem}

На самом деле имеет место еще более сильное утверждение, чем
утверждение 3) доказанной теоремы, а именно равномерная сходимость
$\hatFn(x;\xi_1,\ldots,\xi_n)$ к $F(x)$ с вероятностью 1, что и
составляет содержание следующей теоремы.

\subsection{Теорема Гливенко - Кантелли}\label{task2}

\begin{theorem}[Гливенко\ч Кантелли]
Пусть $\xi_1,\ldots,\xi_n$~--- взаимно независимые, одинаково
распределенные случайные величины с функцией распределения $F(x)$;
$\hatFn(x;\xi_1,\ldots,\xi_n)=\frac{1}{n}\sum\limits_{k=1}^n
I_{(-\infty,x]}(\xi_k)$~--- их эмпирическая функция распределения.
Тогда $$\Pf\bbr{\liml{n\ra\bes} \supl{x}\biggl|{\wh{F}_n(x;\xi_1,\ldots,\xi_n)-F(x)}\biggr|=0}=1$$ (\те $\hatFn(x;\xi_1,\ldots,\xi_n)$ равномерно
 сходится к $F(x)$ с вероятностью 1).
\end{theorem}

\begin{note}
Для определения эмпирической функции распределения в теореме
Гливенко\ч Кантелли не требуется понятия выборки: она определяется
для заданного (известного) набора взаимно независимых одинаково
распределенных случайных величин.
\end{note}

\begin{proof}
Для краткости будем обозначать эмпирическую функцию распределения
через $\hatFn(x)=\hatFn(x;\om)$. По условию теоремы,
$F(x)$~--- функция распределения случайных величин
$\xi_1,\ldots,\xi_n$. Рассмотрим два случая.

1) Пусть $F(x)$~--- непрерывная и строго монотонная функция.
Фиксируем произвольное $\ep>0$ и $k\in\N$, $k\neq1$:
$\frac{1}{k}\le\ep$. Поскольку функция $F(x)$ непрерывна и строго
монотонна, то для каждого $i=0,\ldots,k$ найдется $x_i$:
$F(x_i)=\frac{i}{k}$ (возможно, $x_1=-\infty$ или $x_k=+\infty$),
причем такие $x_1,\ldots,x_k$ определены однозначно. Для соседних
точек, по определению,
\begin{equation} \label{1kan}
F(x_{i+1})-F(x_i)=\frac{1}{k}\le\ep.
\end{equation}

Зафиксируем $i$ и произвольную точку $x$: $x_{i}<x<x_{i+1}$. В
силу монотонности функций $F$ и $\widehat F$ имеем:
\begin{equation}\label{2kan}
\hatFn(x_i)-F(x_{i+1})\le \hatFn(x)-F(x)\le \hatFn%
(x_{i+1})-F(x_i),
\end{equation} и используя неравенство
(\ref{1kan}), отсюда получаем:
$$\hatFn(x_i)-F(x_i)-\ep\le \hatFn(x)-F(x)\le \hatFn%
(x_{i+1})-F(x_{i+1})+\ep,$$
$$\bm{\hatFn(x)-F(x)}\le\max\limits_{0\le i\le k}\bm{\hatFn(x_i)-F(x_i)}+\ep,
\quad \fa x\in\R, \quad\Longrightarrow$$
\begin{equation} \label{3kan}
\sup\limits_{x\in\R}\bm{\hatFn(x)-F(x)}\le\max\limits_{0\le i\le
k}\bm{\hatFn(x_i)-F(x_i)}+\ep,\quad \fa\ep>0,\mbox{ где }
\frac{1}{k}\le\ep.
\end{equation}

Рассмотрим событие $A_i=\bc{\om:\enskip \hatFn(x_i;\om)\to
F(x_i), n\to\infty}.$ Так как $\hatFn(x_i;\om)$ -- среднее независимых
$\text{Bern}(F(x_i))$, то по УЗБЧ для схемы Бернулли $\Pf(A_i)=1$.
Далее, рассмотрим событие $A^{(k)}=\bigcap\limits_{i=1}^k A_i$.
$\Pf(\overline{A^{(k)}}) \leq \sum_{i=1}^k \Pf(\overline{A_i}) = 0$, поэтому
событие $A^{(k)}$ происходит почти наверное. Очевидно,
событие $A^{(k)}$ равносильно тому, что $\max\limits_{0\le i\le k}
\bm{\hatFn(x_i;\om)-F(x_i)}\to0$, $n\to\infty$. Определим
события 
$$\tilde A=\bigcap_{k=2}^\infty A^{(k)}, \quad B=\bbc{\om:
\enskip \sup_{x\in\R}\bm{\hatFn(x;\om)-F(x)}\to0,
n\to\infty}.$$

В силу неравенства (\ref{3kan}) $\tilde A\subset B$, а поскольку
$\Pf(\tilde A)=1$ (аналогично конечному пересечению), то и $\Pf(B)=1$.

2) Пусть теперь $F(x)$~--- произвольная (неубывающая) непрерывная
функция. Тогда определим $x_i$ так:
$$x_i=\inf\bc{x: \enskip F(x-0)\le\frac{i}{k}\le F(x)=F(x+0)}.$$
Далее рассуждаем аналогично первому случаю. Осталось заметить, что
при применении УЗБЧ для схемы Бернулли в данном случае нужно
представить событие $A_i$ в виде $A_i=A_i'\cap A_i''$, где
$$A_i'=\bc{\om: \enskip \hatFn(x_i;\om)\to F(x_i),n\to\infty},\quad
A_i''=\bc{\om: \enskip \hatFn(x_i-0;\om)\to
F(x_i-0),n\to\infty}.$$ Вероятность каждого из этих событий равна
$\Pf(A_i')=\Pf(A_i'')=1$, поэтому и $P(A_i)=1$; дальнейшие
рассуждения в точности такие же, как и в случае 1).
\end{proof}

\subsection{Свойства эмпирических моментов}\label{task3}

Напомним некоторые определенные в п. \ref{char} выборочные
характеристики~--- выборочное среднее и выборочную дисперсию:
$$\bar x=\frac{x_1+\ldots+x_n}{n},\quad s^2=\frac{(x_1-\bar x)^2+\ldots+(x_n-\bar x)^2}{n}.$$

\begin{stm}
$\bar x$ и $s^2$~--- соответственно математическое ожидание и
дисперсия эмпирического распределения (\те распределения,
определяемого функцией распределения
$\hatFn(x;x_1,\ldots,x_n)=\hatFn(x)$).
\end{stm}

\begin{proof}
Обозначим эмпирическое распределение $\hat\xi$, $\hatFn(x)$~---
функция распределения $\hat\xi$. Тогда доказательство следует из
соотношений
$$\Mf \hat\xi=\ints{\R}x\,d\hatFn(x)=\frac{1}{n}\sumkun\ints{\R}x\,dI_{\{x\ge x_k\}}=
\frac{1}{n}\sumkun x_k=\bar x;$$
$$\Df \hat\xi=\ints{\R}(x-\bar x)^2\,d\hatFn(x)=
\frac{1}{n}\sumkun\ints{\R}(x-\bar x)^2\,dI_{\{x\ge
x_k\}}=\frac{1}{n}\sumkun (x_k-\bar x)^2=s^2.$$ Здесь мы
воспользовались определением эмпирической функции распределения,
линейностью интеграла Стилтьеса и формулой для интеграла Стилтьеса
$\ints{X}f(x)\,dg(x)=f(\xi)c$, где $g(x)$~--- функция одного
скачка (в точке $\xi$), $c=g(\xi+0)-g(\xi-0)$~--- величина скачка.
\end{proof}

Аналогично можно показать, что выборочные моменты порядка $k$
являются моментами порядка $k$ эмпирического распределения.
Покажем, что выборочные моменты можно рассматривать как хорошие
оценки моментов теоретического распределения.

Пусть $x_1\sco x_k$~--- выборка, порожденная независимыми
одинаково распределенными случайными величинами
$\xi_1,\ldots,\xi_n\sim\xi$, $F(x)$~--- их (теоретическая) функция
распределения (неизвестная, или известно в каком классе лежит, но
неизвестно какая именно). Её $k$-тый момент равен
$$\mu_k=\intl{-\infty}{+\infty}x^k\,dF(x),\quad k\in\N.$$
($\mu_1$~--- математическое ожидание, $\mu_2$~--- второй момент,
$\sigma^2=\mu_2-\mu_1^2$~--- дисперсия, и т.д.). Рассмотрим
выборочные моменты~--- моменты эмпирического распределения:
$$\hat\mu_{k,n}=\frac{1}{n}\sum_{i=1}^k x_i^k=\frac{x_1^k+\ldots+x_n^k}{n}=
\intl{-\infty}{+\infty}x^k\,d\hatFn(x).$$

Если рассматривать $\hat\mu_{k,n}$ как оценки $\mu_k$, то легко
получаем следующие её свойства:

1) Несмещённость: $\displaystyle\Mf\hat\mu_{k,n}=\Mf\hr{\frac{\xi_1^k+\ldots+\xi_n^k}{n}}=\Mf \xi^k=\mu_k$;

2) Состоятельность: по закону больших чисел
$\displaystyle\hat\mu_{k,n}\convas\Mf\hat\mu_{k,n}=\mu_k\quad\Longrightarrow
\quad\hat\mu_{k,n}\raP \mu_k$, $n\to\infty$.

\subsection{Критерий Колмогорова}\label{task4}

\subsubsection{Статистика Колмогорова}

Пусть дана случайная выборка $\xi_1,\ldots,\xi_n$~--- независимые
одинаково распределенные случайные величины с непрерывной функцией
распределения $F(x)$.

\begin{df} Случайная величина $D_n=D_n(\xi_1,\ldots,\xi_n)=
\sup\limits_{x\in\R}\bm{\hatFn(x;\xi_1,\ldots,\xi_n)-F(x)}$ называется
статистикой Колмогорова.
\end{df}

В терминах статистики Колмогорова теорему Гливенко\ч Кантелли можно
переформулировать так: $D_n$ сходится к нулю при $n\to\infty$ с
вероятностью 1 (\те $\Pf$-п.н.).

Вид асимптотической функции распределения статистики $\sqrt{n}D_n$
дает следующая теорема.

\begin{theorem}[Колмогоров]
Если функция $F(x)$ непрерывна, то при любом $y>0$
$$\lim_{n\to\infty}\Pf(\sqrt{n}D_n\le y)=K(y):=\sum_{m=-\infty}^{\infty}(-1)^m e^{-2m^2 y^2}.$$
\end{theorem}

\begin{note}
Для $y\le 0$, очевидно, $\Pf(\sqrt{n}D_n\le y)=0$.
\end{note}

Участвующая в теореме функция
$K(y)=\sum\limits_{m=-\infty}^{\infty}(-1)^m e^{-2m^2 y^2}$, $y>0$
называется функцией Колмогорова.

Мы докажем только часть теоремы Колмогорова, а именно следующую
лемму:

\begin{lemma}
Распределение статистики Колмогорова $D_n(\xi_1,\ldots,\xi_n)$,
где $\xi_1,\ldots,\xi_n$~--- независимые одинаково распределенные
случайные величины с непрерывной функцией распределения $F(x)$, не
зависит от вида функции $F(x)$ (то есть для любых случайных н. о. р. 
$\xi_1, \ldots, \xi_n$ $D_n(\xi_1,\ldots,\xi_n)$ распределена одинаково).
\end{lemma}

\begin{proof}
Рассмотрим два случая.

1) Пусть $y=F(x)$~--- непрерывная и строго монотонная функция.
Тогда существует обратная функция: $x=F^{-1}(y)$. Рассмотрим
случайные величины $Y_1,\ldots,Y_n$, $Y_k=F(\xi_k)$,
$k=1,\ldots,n$. Они независимы и имеют одинаковое равномерное
распределение на отрезке $[0,1]$:
$$R(y)=\Pf(Y_k\le y)=\Pf(F(\xi_k)\le y)=\Pf(\xi_k\le F^{-1}(y))=F(F^{-1}(y))=y,\quad 0<y<1.$$

Эмпирическая функция распределения $Y_1,\ldots,Y_n$:

$$\hatRn(y;Y_1,\ldots,Y_n)=\frac{1}{n}\sum_{k=1}^n I_{\{Y_k\le y\}}=
\frac{1}{n}\sum_{k=1}^n I_{\{\xi_k\le F^{-1}(y)\}}=\hatFn%
(F^{-1}(y)),$$ где $\hatFn(x)=\hatFn%
(x;\xi_1,\ldots,\xi_n)$~--- эмпирическая функция распределения
случайной выборки $\xi_1,\ldots,\xi_n$.

Рассмотрим очевидное равенство $$\sup_{x:\enskip
0<F(x)<1}\bm{\hatFn(x)-F(x)}=\sup_{y:\enskip
0<y<1}\bm{\hatRn(y)-R(y)}.$$ Его левая часть с вероятностью~$1$
совпадает с $D_n(\xi_1,\ldots,\xi_n)$, а правая часть~--- с
$D_n(Y_1,\ldots,Y_n)$. Статистика $D_n(Y_1,\ldots,Y_n)$ от вида
функции $F(x)$ не зависит, поскольку от $F(x)$ не зависит
распределение случайных величин $Y_1,\ldots,Y_n$.

Для завершения доказательства осталось показать, что на множестве
$C=\bc{x:\, F(x)=0 \mbox{ или } F(x)=1}$ эмпирическая функция
распределения $\hatFn(x)$ и теоретическая $F(x)$ совпадают с
вероятностью 1. Для этого достаточно проверить, что
$\Pf\bbr{\sup\limits_{x\in C}\bm{\hatFn(x)-F(x)}=0}=1$. Проверку
этого факта мы предоставляем читателю.

2) Если функция $F(x)$~--- произвольная (неубывающая) непрерывная
функция, то рассуждения аналогичны предыдущему случаю, только в
этом случае нужно положить $F^{-1}(y)=\sup\bc{x:\, F(x)=y}$.
Читателю рекомендуется аккуратно провести рассуждения для этого
случая самостоятельно.
\end{proof}

\section{Точечные оценки}
\subsection{Выборочные характеристики} \label{char}
\subsubsection{Определение и свойства выборочных характеристик}

Пусть в некоторой статистической модели имеется выборка порядка
$n$: $x_1,\ldots,x_n$.

\begin{df}
\emph{Статистикой} называется произвольная измеримая функция
$f(x_1,\ldots,x_n)$ от элементов выборки\footnote{Статистика
зависит {\bf только от} элементов {\bf выборки}, 
и ни в коем случае {\bf не зависит от параметра}~--- 
\emph{примеч. В.\,Х.}} $x_1,\ldots,x_n$.
\end{df}

\begin{df}
Если случайная величина $\xi$ имеет распределение $F(x)$, то
\emph{медианой} распределения называется такое число $\mu$, что
$F(\mu)=\frac{1}{2}$.
\end{df}
Медиана распределения обладает тем свойством, что $\Pf(\xi\ge\mu)=\Pf(\xi\le\mu)$.

Рассмотрим примеры наиболее часто встречающихся статистик (или \emph{выборочных характеристик}):

\begin{items}{-2}
\item Выборочное среднее:
\eqn{\ol x_n := \suml{i=1}{n} x_i;}

\item Выборочная дисперсия:
\eqn{S_n^2:= \frac{1}{n}\suml{i=1}{n}(x_i-\ol x_n)^2;}

\item Выборочный момент порядка $k$:
\eqn{\wh{\mu}_k= \frac{1}{n}\suml{i=1}{n} x_i^k}

\item Выборочный центральный момент порядка $k$:
\eqn{\wh{M}_k=\frac{1}{n}\suml{i=1}{n} (x_i-\ol x_n)^k;}

\item Порядковые статистики: упорядочим элементы выборки по возрастанию, получим последовательность
\eqn{x_{(1)}\sco x_{(n)}.}
Она называется \emph{вариационным рядом выборки}, 
а её элементы\т \emph{порядковыми статистиками}.
Случайные величины $\xi_{(n)}$ со значениями $x_{(n)}$ также называются порядковыми статистиками.
Более формально,
\begin{gather}
x_{(1)}:= \min(x_1,\ldots,x_n),\\
x_{(2)}:=\max\bs{\minl{k}(x_1 \sco \wh{x_k}\sco x_n)},\\
x_{(3)}:=\max\bs{\minl{i\neq j}(x_1 \sco \wh{x_i} \sco \wh{x_j} \sco x_n)},\\
\ldots\ldots\ldots\\
x_{(n)}:=\max(x_1 \sco ,x_n),
\end{gather}
где <<крышка>>, как обычно, означает пропуск этого элемента.

\item Выборочный квантиль уровня $\alpha$:
\eqn{\wh\varkappa_{\alpha}=\case{x_{([n \alpha]+1)}, & 
n \alpha \notin \mathbb{Z}; \\ 
\frac12\hr{x_{(n \alpha)}+x_{(n \alpha+1)}}, &
n \alpha \in \mathbb{Z}.}}

\item Выборочная медиана (выборочный квантиль уровня $0.5$):
\eqn{\wh\mu=\case{x_{(m)}, & n=2m-1; \\ \frac12\hr{x_{(m)}+x_{(m+1)}}, & n=2m.}}
\end{items}

\begin{ex}
Рассмотрим равномерное распределение на $[0,\theta]$, $\theta$~--- неизвестный
параметр. Параметр $\theta$ можно оценить двумя способами:
\begin{nums}{-2}
\item $\wh\theta^1 = 2 \ol x_n$ --- несмещённая оценка
\item $\wh\theta^2 = x_{(n)}$ (оценка по крайней точке) --- смещённая,
но средне-квадратичная ошибка меньше, чем у $\wh\theta^1$.
\end{nums}
\end{ex}

\subsubsection{Распределение порядковых статистик} \label{dist_stat}

Пусть $\xi_1,\ldots,\xi_n$~--- случайная выборка с теоретической
функцией распределения $F(x)$, $\xi_{(1)},\ldots,\xi_{(n)}$~--- её
порядковые статистики.

Найдем распределение $\xi_{(k)}$, $k\in\N$. Пусть
$G_{\xi_{(k)}}(x)=\Pf(\xi_{(k)}\le x)$~--- функция распределения
$\xi_{(k)}$. При каждом фиксированном $x\in\R$ имеем:
$$G_{\xi_{(k)}}(x)=\Pf\br{\xi_{(k)}\le x}=\Pf\bbr{\hat F_n(x)\ge \frac{k}{n}}=
\suml{i=k}{n}C_n^i {\br{F(x)}}^i {\br{1-F(x)}}^{n-i}.$$

\begin{ex}
Пусть $F(x)$~--- функция распределения равномерного распределения
на отрезке $[0,1]$: $F(x)=x$, $0<x<1$. Тогда
$$G_{\xi_{(k)}}(x)=\Pf\br{\xi_{(k)}\le x}=\suml{i=k}{n}C_n^i x^i{(1-x)}^{n-i}.$$

Найдем плотность этого распределения. Для этого продифференцируем
функцию распределения:
$$\br{G_{\xi_{(k)}}(x)}_x'=\suml{i=k}{n}iC_n^i x^{i-1}{(1-x)}^{n-i}-
\suml{i=k}{n-1}(n-i)C_n^i
x^i{(1-x)}^{(n-1)-i}=$$$$=\suml{i=k}{n}nC_{n-1}^{i-1}
x^{i-1}{(1-x)}^{(n-1)-(i-1)}-\suml{i=k}{n-1}nC_{n-1}^i
x^i{(1-x)}^{(n-1)-i}=$$$$=nC_{n-1}^{k-1}x^{k-1}(1-x)^{(n-1)-(k-1)}+nC_{n-1}^k
x^k(1-x)^{n-k-1}+\ldots-nC_{n-1}^k
x^k(1-x)^{n-k-1}-\ldots=nC_{n-1}^{k-1}x^{k-1}(1-x)^{n-k}.$$ (во
втором равенстве воспользовались тождествами
$iC_n^i=nC_{n-1}^{i-1}$, $(n-i)C_n^i=nC_{n-1}^i$). Таким образом,
плотность распределения $\xi_{(k)}$ равна $\bs{\Pf(\xi_{(k)}\le x
)}'=nC_{n-1}^{k-1}x^{k-1}{(1-x)}^{n-k}=\dfrac{n!}{(k-1)!(n-k)!}x^{k-1}{(1-x)}^{n-k},$
а функция распределения~---
\begin{equation} \label{ras_ps}
G_{\xi_{(k)}}(x)=\Pf(\xi_{(k)}\le
x)=\frac{n!}{(k-1)!(n-k)!}\intl{0}{x}t^{k-1}{(1-t)}^{n-k}\,dt,\quad
0<x<1.\end{equation}
\end{ex}

\begin{df} Пусть $a>0$, $b>0$. Распределение с плотностью $$p(x)=\left\{\begin{array}{cl}
  \dfrac{1}{B(a,b)}x^{a-1}{(1-x)}^{b-1}, & 0<x<1; \\
  0, & x\le 0\mbox{ или }x\ge 1,
\end{array}\right.$$
где $B(a,b)=\intl{0}{1}t^{a-1}{(1-t)}^{b-1}\,dt$~--- бета-функция
(эйлеров интеграл I рода), называется бета-распределением с
параметрами $a>0$, $b>0$. Функция распределения бета-распределения
$I_x(a,b)=\dfrac{1}{B(a,b)}\intl{0}{x}t^{a-1}{(1-t)}^{b-1}\,dt$
($0<x<1$) называется неполной бета-функцией.
\end{df}

Из математического анализа известно, что
$B(a,b)=\dfrac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$, где
$\Gamma(\lambda)=\intl{0}{\infty}x^{\lambda-1}e^{-x}\,dx$~---
гамма-функция Эйлера (эйлеров интеграл II рода), и для $n\in\N$
$\Gamma(n+1)=n!$, поэтому формулу (\ref{ras_ps}) можно переписать
в виде

$$G_{\xi_{(k)}}(x)=\Pf(\xi_{(k)}\le
x)=\frac{1}{B(k,n-k+1)}\intl{0}{x}t^{k-1}{(1-t)}^{n-k}\,dt=I_x(k,n-k+1),\quad
0<x<1.$$

Таким образом, нами доказан следующий результат.

\begin{stm}
Распределение порядковой статистики $\xi_{(k)}$, $1\le k\le n$,
для случайной выборки $\xi_1,\ldots,\xi_n$ с равномерным
распределением на отрезке $[0,1]$ является бета-распределением с
параметрами $a=k$, $b=n-k+1$.
\end{stm}

\begin{note}
Функция распределения порядковых статистик в случае произвольной
непрерывной функции распределения $F(x)$ случайной выборки
$\xi_1,\ldots,\xi_n$ имеет вид $\Pf(\xi_{(k)}\le
x)=I_{F(x)}(k,n-k+1)$.
\end{note}

\subsection{Функция правдоподобия. Регулярные модели}

Пусть в некоторой статистической модели
$(X,\As_X,\Ps=\{\Pf_\theta,\theta\in\Theta\})$ имеется выборка
$x=(x_1,\ldots,x_n)$, порожденная случайной выборкой
$\xi_1,\ldots,\xi_n\sim\xi$, где случайные величины
$\xi_1,\ldots,\xi_n$ независимы и одинаково распределены с
функцией распределения $F_\theta(x)$, $\theta\in\Theta$~---
параметр распределения.

\begin{df} Оптимальной оценкой параметра $\theta$ называется несмещенная
оценка с минимальной дисперсией, \те такая оценка $\hattheta^*$,
для которой выполнены следующие свойства:

1) $\Mf_\theta\hattheta^*=\theta$ $\fa\theta\in\Theta$;

2)
$\Mf_\theta{(\hattheta^*-\theta)}^2=\min\limits_{\hattheta\cln\Mf_\theta\hattheta=\theta}
\Mf_\theta{(\hattheta-\theta)}^2.$

Напомним, что для несмещенных оценок среднеквадратичное отклонение совпадает с дисперсией:
$$\Mf_\theta{(\hattheta-\theta)}^2=\Mf_\theta{(\hattheta-\Mf\hattheta)}^2=\Df\hattheta.$$
\end{df}
Таким образом, оптимальная оценка~--- это наилучшая из всех
несмещенных оценок. Аналогично можно определить оптимальную оценку
в классе оценок с заданным смещением $\Delta$:
$\Mf_\theta\hattheta_1=\Mf_\theta\hattheta_2=\Delta$.

Нас будут интересовать два случая: распределение $\xi$ дискретно
(с распределением вероятностей $\Pf_\theta$) или абсолютно
непрерывно (с плотностью $p(\theta;x)$). Чтобы в дальнейшем не
рассматривать эти случаи отдельно, введем следующее удобное
обозначение:
$$f(\theta;x)=\left\{\begin{array}{cl}
  \Pf_\theta(\xi=x), & \mbox{ если модель дискретна; } \\
  p(\theta;x), & \mbox{ если модель абсолютно непрерывна. }
\end{array}\right.$$

В дальнейшем придется интегрировать по выборочному пространству,
поэтому отметим, что если модель дискретна, то интегрирование
заменяется суммированием (для краткости, мы будем проводить все
выкладки для абсолютно непрерывной модели).

\begin{df}
Функцией правдоподобия (для данной выборки $x_1,\ldots,x_n$)
называется следующая функция (параметра $\theta$):
$$L(\theta;x_1,\ldots,x_n):=\prod_{i=1}^n f(\theta;x_i),\quad \theta\in\Theta.$$
\end{df}

В дальнейшем для краткости мы будем писать
$L(\theta;x)=L(\theta;x_1,\ldots,x_n)$, $x=(x_1,\ldots,x_n)$.

\begin{df}
Статистическая модель называется \emph{регулярной} (по Рао--Крамеру),
если выполнены следующие условия регулярности:

\quad 1) $L(\theta;x)>0$ и дифференцируема по $\theta$
$\fa\theta\in\Theta$ и $\fa x\in X$;

\quad 2) Случайная величина $U(\theta;x)=\dfrac{\partial\ln
L(\theta;x)}{\partial\theta}=\sumiun\dfrac{\partial\ln
f(\theta;x_i)}{\partial\theta}$ (которая называется функцией
вклада выборки) имеет ограниченную дисперсию:
$$0<\Mf^2_\theta U(\theta;x)<\infty,\quad \fa\theta\in\Theta.$$

\quad 3) Для любой статистики $\hattheta=\hattheta(x)$ имеет
место равенство
$$\frac{\partial}{\partial\theta}\ints{X} \hattheta(x) L(\theta;x)\,dx=
\ints{X} \hattheta(x) \frac{\partial
L(\theta;x)}{\partial\theta}\,dx$$ (Это означает, что выборочное
пространство $X$ не зависит от параметра $\theta$).
\end{df}

\begin{ex} (нерегулярной модели).
Рассмотрим модель $R(0,\theta)$ (равномерное распределение на
отрезке $(0,\theta)$, $\theta>0$). Условие 3) регулярности для
этой модели не выполнено:
$$\frac{\partial}{\partial\theta}\bbr{\intl{0}{\theta}\frac{1}{\theta}\,dx}=
\frac{\partial}{\partial\theta}\bbr{\frac{1}{\theta}\theta}=\frac{\partial}{\partial\theta}(1)=0,\quad\mbox{
но
}\quad\intl{0}{\theta}\frac{\partial}{\partial\theta}\bbr{\frac{1}{\theta}}\,dx=
-\intl{0}{\theta}\frac{1}{\theta^2}dx=-\frac{1}{\theta}.$$ Таким
образом, эта модель не является регулярной.

\end{ex}

\begin{problem}
Проверить условие регулярности 3) для экспоненциального
распределения с плотностью
$$p_\theta(x)=\left\{
\begin{array}{cl}
  e^{-(x-\theta)}, & x \ge \theta; \\
  0, & x < \theta.
\end{array}\right.$$
\end{problem}

\subsection{Условное математическое ожидание} \label{task7}
\begin{df}
Пусть $(\Omega, \As, \Pf)$ --- вероятностное пространство, $\xi$ --- $\As$-измеримая функция
(случайная величина), $\Cs$ --- $\sigma$-подалгебра в $\As$. Тогда \emph{условным математическим
ожиданием (УМО; обозначение: $\Mf(\xi \mid \Cs)$)}
 $\xi$ относительно $\Cs$ (более точно, \emph{вариантом} УМО) называется случайная
величина $\tilde\xi(\omega)$, удовлетворяющая следующим условиям:
\begin{nums}{-2}
\item $\tilde\xi$ $\Cs$-измерима;
\item для всех $C\in\Cs$
$$
\int\limits_C \xi \, d\Pf = \int\limits_C \tilde\xi \, d\Pf
\text{, т.е. }  \Mf\xi\mathrm{I}_C = \Mf\tilde\xi\mathrm{I}_C.
$$
\end{nums}
\end{df}

\begin{note}
$\xi$ не обязана быть $\Cs$-измеримой (иначе определение тривиально: возьмём $\tilde\xi = \xi$).
УМО есть осреднение случайной величины, чтобы та стала измеримой относительно более грубой
$\sigma$-алгебры.
\end{note}

Следующие два утверждения обосновывают корректность определения:
\begin{stm}
Варианты УМО существуют, если $\Mf|\xi| < \infty$.
\end{stm}

\begin{proof}
Сначала рассмотрим случай неотрицательной случайной величины $\xi$.
$Q(C):= \int\limits_C \xi \, d\Pf$ определяет меру на $(\Omega, \Cs)$, абсолютно
непрерывную относительно меры $\Pf$, рассмотренной на $(\Omega, \Cs)$. По теореме
Радона\ч Никодима существует $\Cs$-измеримая функция $g(\omega) \geqslant 0$ такая,
что $\int\limits_\Omega g(\omega) \, d\Pf = 1$ и $Q(C) = \int\limits_C g(\omega) \, d\Pf$
($g$ --- производная Радона\ч Никодима). $g$ по определению является вариантом УМО.

В случае знакопеременной $\xi$ полагаем $\xi = \xi^+ - \xi^-$ ($\xi^+$ и $\xi^-$
неотрицательны) и берём в качестве варианта УМО
$\Mf(\xi\mid\Cs) = \Mf(\xi^+\mid\Cs) - \Mf(\xi^-\mid\Cs)$.
\end{proof}

\begin{note} Условие $\Mf|\xi| < \infty$ не является необходимым. \end{note}

\begin{stm}
Любые два варианта УМО совпадают почти наверное.
\end{stm}

\begin{proof}
От противного: пусть $g_1$ и $g_2$ --- два варианта УМО $\Mf(\xi \mid \Cs)$.
Пусть также $C = \{ \omega \mid g_1(\omega) \ne g_2(\omega) \}$, $\Pf(C) > 0$.
Имеем, что $C = C_< \cup C_>$ (они измеримы как прообразы борелевских), 
где $C_* = \{ \omega \mid g_1(\omega) * g_2(\omega) \}$,
$* \in \{<,>\}$. Хотя бы одно из множеств $C_<$ и $C_>$ имеет положительную меру
(иначе $\Pf(C) = 0$); без ограничения общности считаем $\Pf(C_<) > 0$.
$g_1$ и $g_2$ измеримы, поэтому $D = C_< \in \Cs$. Тогда по определению варианта УМО
$$
\int\limits_D g_1 \, d\Pf = \int\limits_D \xi \, d\Pf = \int\limits_D g_2 \, d\Pf,
$$
что противоречит тому, что $g_1 < g_2$ всюду на $D$ и $\Pf(D) > 0$.
\end{proof}

Поэтому УМО можно считать однозначно определённым с точностью до множеств $\Pf$-меры нуль.

\begin{df}
Пусть $(\Omega, \As, \Pf)$ --- вероятностное пространство, $\Cs$
--- $\sigma$-подалгебра в $\As$, $A \in \As$. Тогда \emph{условная вероятность}
 $A$ относительно $\Cs$ определяется так: $Pf(A \mid \Cs) =
 \Mf(I_A \mid \Cs)$, где $I_A$~--- характеристическая функция $A$.
\end{df}

\begin{df}
Пусть $(\Omega, \As, \Pf)$ --- вероятностное пространство, $\xi$ и
$\eta$~--- случайные величины, $\Cs_\eta$~--- $\sigma$-подалгебра
в $\As$, порождённая $\eta$ (\те прообразы всех борелевских
множеств из $\mathbb{R}$). Тогда \emph{условным матожиданием $\xi$
относительно $\eta$} называется $\Mf (\xi \mid \eta) = \Mf (\xi
\mid \Cs_\eta)$.
\end{df}

\subsubsection{Свойства УМО}\label{properties_CondE}

\begin{df} $\xi$ \emph{не зависит} от $\sigma$-алгебры $\Cs$, если для
любого события $C\in\Cs$ случайные величины $\xi$ и $\mathrm{I}_C$
независимы.
\end{df}

Обозначение: $\Cs_0 = \{\Omega, \varnothing\}$ --- тривиальная
$\sigma$-алгебра.

\begin{stm}
\begin{nums}{-2}
\item Если $\Pf(\xi = c) = 1$, то $\Mf(\xi \mid \Cs) = c$ п.н.
\item Линейность: $\Mf(\alpha_1\xi_1 + \alpha_2\xi_2 \mid \Cs) =
\alpha_1 \Mf(\xi_1 \mid \Cs) + \alpha_2 \Mf(\xi_2 \mid \Cs)$ п.н.
\item Если $\xi\leqslant\eta$ п.н., то $\Mf(\xi \mid \Cs) \leqslant \Mf(\eta\mid\Cs)$ п.н.
\item $\bigl| \Mf(\xi\mid\Cs) \bigr| \leqslant \Mf(|\xi| \mid \Cs)$ п.н.
\item Неравенство Йенсена: пусть $g(x)$ --- непрерывная выпуклая вниз функция,
$\Mf\bigl| g(\xi) \bigr| < \infty$. \\Тогда $g(\Mf(\xi\mid\Cs)) \leqslant
\Mf(g(\xi)\mid\Cs)$ п.н. Равенство имеет место тогда и только тогда, когда
$\xi$ --- $\Cs$-измерима (точнее, совпадает с некоторой $\Cs$-измеримой функцией
почти всюду).
\item $\Mf(\xi \mid \As) = \xi$ п.н.
\item Если $\xi$ не зависит от $\Cs$, то $\Mf(\xi\mid\Cs) = \Mf\xi$ п.н.
\item Пусть $\eta$ является $\Cs$-измеримой и $\Mf |\xi\eta| < \infty$. Тогда
$\Mf(\xi\eta \mid \Cs) = \eta \Mf(\xi \mid \Cs)$.
\item $\Mf(\xi \mid \Cs_0) = \Mf\xi$
\item Если $\Cs_1 \subseteq \Cs_2$, то $\Mf\bigl(\Mf(\xi\mid\Cs_2)\mid\Cs_1%
\bigr) = \Mf(\xi\mid\Cs_1)$. Если $\Cs_1 \supseteq \Cs_2$, то
$\Mf\bigl(\Mf(\xi\mid\Cs_2)\mid\Cs_1\bigr) = \Mf(\xi\mid\Cs_2)$.
\item $\Mf\bigl(\Mf(\xi\mid\Cs)\bigr) = \Mf\xi$
\item Если $\Mf\xi^2 < \infty$, то
$$
\inf\limits_{\text{$g(\omega)$ --- $\Cs$-измер.}}
\Mf(\xi - g(\omega))^2 = \Mf(\xi - \Mf(\xi\mid\Cs))^2,
$$
\те $g_{\min}(\omega) = \Mf(\xi\mid\Cs)$ имеет наименьшее
среднеквадратичное отклонение. В некотором смысмле 
это альтернативное определение УМО.
\end{nums}
\end{stm}

\begin{proof}
$1$ пункт по определению, $2 -- 5$ пункты по свойствам 
математического ожидания, $6$ пункт по определению.

Докажем $7$-е свойство. Для любого $C \in \Cs$ $\xi$ не зависит от $I_C$.
$$
\int_C \Mf(\xi\mid\Cs) d \Pf = \Mf(\xi I_C) = \Mf \xi \Mf I_C = 
\Mf \xi \int_C d \Pf = \int_C \Mf \xi d \Pf.
$$

Докажем $8$-е свойство. Рассмотрим случай, 
когда $\eta = I_B$, $B \in \Cs$. Тогда для любого $C \in \Cs$
$$
\int_C \Mf(\xi \eta \mid \Cs) d \Pf = \int_C \xi \eta d \Pf =
\int_{C \cap B} \xi d \Pf;
$$
$$
\int_C \eta \Mf(\xi \mid \Cs) d \Pf = \int_{C \cap B} \Mf(\xi \mid \Cs) 
d \Pf = \int_{C \cap B} \xi d \Pf.
$$

Докажем $10$-е свойство. Рассмотрим случай $\Cs_1 \subseteq \Cs_2$.
Пусть $C \in \Cs_1$.

$$
\int_C \Mf(\xi \mid \Cs_1) d \Pf = \int_C \xi d \Pf.
$$

$$
\int_C \Mf(\Mf(\xi \mid \Cs_2) \mid \Cs_1) d \Pf =
\int_C \Mf(\xi \mid \Cs_2) d \Pf = \int_C \xi d \Pf.
$$

Если в $10$ пункте возьмём $\Cs_1 = \Cs_0$, $\Cs_2 = \Cs$,
то по пункту $9$ получим утверждение $11$-го пункта.

Докажем $12$-е свойство. Пусть $g$ - $\Cs$ измеримая функция.
Тогда
\begin{multline*}
\Mf((\xi - g(\omega))^2 \mid \Cs) = 
\Mf((\xi - \Mf(\xi \mid \Cs) + 
\Mf(\xi \mid \Cs) - g(\omega))^2 \mid \Cs) = \\
\Mf((\xi - \Mf(\xi \mid \Cs))^2 \mid \Cs) +
\Mf((\Mf(\xi \mid \Cs) - g(\omega))^2 \mid \Cs) +
2 \Mf((\xi - \Mf(\xi \mid \Cs))(\Mf(\xi \mid \Cs) - g(\omega)) \mid \Cs).
\end{multline*}

Так как $\Mf(\xi \mid \Cs) - g(\omega)$ --- $\Cs$ - измерима,
то произведение расписывается как
$$
(\Mf(\xi \mid \Cs) - g(\omega)) \cdot 
\Mf(\xi - \Mf(\xi \mid \Cs) \mid \Cs) =
(\Mf(\xi \mid \Cs) - g(\omega)) \cdot
(\Mf(\xi \mid \Cs) - \Mf(\xi \mid \Cs)) = 0.
$$

Используя то, что второе слагаемое при раскрытии квадрата было 
неотрицательно, получаем
$$
\Mf((\xi - g(\omega))^2 \mid \Cs) \geq 
\Mf((\xi - \Mf(\xi \mid \Cs))^2 \mid \Cs).
$$
Теперь можно взять матожидание от обоих частей этого неравенство
и получить искомое.
\end{proof}

\begin{df}
Вариант условного распределения $\zeta = \Pf(\xi\mid\Cs)$ 
будем называть \emph{регулярным},
если существует такая функция $\Pf(A, \omega)$, $A \in \Cs$,
$\omega \in \Omega$, удовлетворяющая условиям:
\begin{nums}{-2}
\item $\Pf(A, \omega) = \Pf(A\mid\Cs)$ ($P$-п.н.) для любого $A \in \Cs$;
\item $\Pf(\cdot, \omega)$ --- вероятностная мера на $\Cs$ при фиксированном $\omega$.
\end{nums}
\end{df}

\subsection{Достаточные статистики} \label{task9}
$(\Xs, \As, \Ps = \{ \Pf_\theta \mid \theta\in\Theta \})$ --- параметрическая модель; $x_1, \dots,
x_n$ --- выборка.

\begin{df}
Статистика $T(x) = T(x_1, \dots, x_n)$ называется \emph{достаточной} (для данной модели), если
для всех $\theta\in\Theta$ и для всех $A\in\As$ условная вероятность
 $\Pf_\theta(A \mid T(\xi) = t)$ не зависит от
$\theta$ для всех $t$, для которых определена условная вероятность.
\end{df}

Найти достаточную статистику по определению не представляется возможным.

\begin{ex}
Схема Бернулли. $T(x) = \sum x_i$. Пусть $t$ и $x$ таковы, что $T(x) = t$. Тогда
$$
\Pf_\theta(\xi = x \mid T(\xi) = t) = \frac{\Pf_\theta(\xi = x)}{\Pf_\theta(T(\xi) = t)} =
\frac{\theta^t (1-\theta)^{n-t}}{C_n^t \theta^t (1-\theta)^{n-t}} = \frac 1 {C_n^t}
\quad \text{--- не зависит от $\theta$.}
$$
Следовательно, $T(x) = \sum x_i$~--- достаточная статистика.
\end{ex}

\begin{theorem}[критерий достаточности~--- теорема Неймана\ч Фишера~--- теорема о факторизации]
$T$~--- достаточная статистика тогда и только тогда, когда имеет
место следующее представление функции правдоподобия:
$$
L(\theta;x) = g_\theta(T(x)) h(x) \qquad g_\theta(T(x)) = g(\theta; T(x)),
$$
где $g_\theta$ и  $h$~--- неотрицательные измеримые функции
(каждая~--- в своей области).
\end{theorem}

\begin{note}
$L(\theta; x)$~--- плотность совместного распределения $\xi_1,
\dots, \xi_n$ по некоторой мере $\mu$; семейство $\{ \Pf_\theta
\}$ абсолютно непрерывно относительно меры $\mu$, плотность~---
производная Радона\ч Никодима. Нам важны два случая: а) $\mu$~---
мера Лебега, б) $\mu$~--- считающая мера.
\end{note}

\begin{proof} \emph{[Доказательство для дискретного случая]}
$x\in\Xs$ --- выборка.
$$
\Pf_\theta(\xi = x \mid T(\xi) = t) =
\begin{cases}
\frac{\Pf_\theta(\xi = x)}{\Pf_\theta(T(\xi) = t)}, & T(x) = t\\
0, & T(x) \ne t
\end{cases}
$$

\emph{Необходимость.} Пусть $T$~--- достаточная статистика. Тогда
положим
$$
h(x) =
\frac{\Pf_\theta(\xi = x)}{\Pf_\theta(T(\xi) = t)} \text{ --- не зависит от $\theta$
в силу достаточности $T$,}
$$
откуда
$ L(\theta; x) = \Pf_\theta(\xi = x) = \Pf_\theta(T(\xi) = t) \cdot h(x) = g_\theta(T(x))\cdot  h(x) $.

\emph{Достаточность.} Пусть $T(x) = t$. Тогда
$$
\Pf_\theta(\xi = x \mid T(\xi) = t) =
\frac{g_\theta(t) h(x)}{\sum\limits_{T(x) = t} g_\theta(t) h(x)} =
\frac{h(x)}{\sum h(x)} \text{ --- не зависит от $\theta$,}
$$
поэтому $T$ --- достаточная статистика.
\end{proof}

\subsection{Количество информации Фишера} \label{task6}
\subsubsection{Информация Фишера}

Пусть модель регулярна. Рассмотрим тождество
$\ints{X}L(\theta;x)\,dx\equiv1$ (оно выполнено, так как
$L(\theta;x)$~--- плотность распределения случайного вектора
$\xi_1,\ldots,\xi_n$). Продифференцируем его по $\theta$:
$$\frac{\partial}{\partial\theta}\ints{X}L(\theta;x)\,dx=
\ints{X} \frac{\partial\ln
L(\theta;x)}{\partial\theta}L(\theta;x)\,dx=0.$$

Отсюда следует, что математическое ожидание функции вклада равно
0: $\Mf_\theta U(\theta;X)=\Mf_\theta \frac{\partial \ln
L(\theta;x) }{\partial\theta}=0.$

\begin{df}
Количеством информации Фишера (или просто информацией Фишера)
называется дисперсия функции вклада:
$$I_n(\theta):=
\Df_\theta U(\theta;\xi_1,\ldots,\xi_n)=
\Mf_\theta U^2(\theta;\xi_1,\ldots,\xi_n)=
\ints{X}{\bbr{\frac{\partial \ln L(\theta;x)}{\partial\theta}}}^2
L(\theta;x)\,dx.$$

Количество информации для одного наблюдения равно $I_1(\theta)=
\ints{X}{\br{\frac{\partial \ln f(\theta;x)}{\partial\theta}}}^2
f(\theta;x)\,dx$, а поскольку наблюдения (случайные величины)
$\xi_1,\ldots,\xi_n$ независимы, то информация Фишера о выборке
размера $n$ равна $I_n(\theta)=n I_1(\theta)$.
\end{df}

\subsubsection{О свойствах информации Фишера}
$\Ps = \{ \Pf_\theta \mid \theta\in\Theta \}$, $x_1, \dots, x_s$ --- выборка.
$L(\theta; x_1, \dots, x_n)$ --- функция правдоподобия.
Информация Фишера:
$$
I_n(\theta) = \Mf_\theta \left( \frac{\partial\ln L(\theta; \xi_1, \dots, \xi_n)}{\partial\theta} \right)^2
=: I^\xi(\theta).
$$
$I^{\xi_1}(\theta) = I_1(\theta)$ ($I^\xi(\theta)$ --- информация в выборке $\xi$ --- это просто такое
обозначение). Для информации Фишера выполняются все обычные свойства информации:

\begin{stm}[свойства информации Фишера]
\begin{nums}{-2}
\item $I^\xi(\theta) = I^{\xi_1}(\theta) + \dots + I^{\xi_n}(\theta)$, если $\xi_1, \dots, \xi_n$
независимы.
\item Для повторной выборки $I^\xi(\theta) = n I^{\xi_1}(\theta)$.
\item Пусть $(\mathfrak{T}, \Cs)$ --- измеримое пространство значений статистики $T$, $\Qf_\theta$ ---
распределение статистики $T$, $\widetilde{L}(\theta; T(x))$ --- функция правдоподобия для $T(x)$
(статистика $T$ не обязательно одномерна),
$
I^{T(\xi)}(\theta) = \Mf_\theta
\left(\dfrac{\partial\ln\widetilde{L}(\theta; T(\xi))}{\partial\theta}\right)^2
$ --- информация Фишера в статистике $T$.
Тогда $I^\xi(\theta) \geqslant I^{T(\xi)}(\theta)$.
\end{nums}
\end{stm}

\begin{proof}
Докажем только пункт 3 (остальные тривиальны), да и то лишь в дискретном случае.

$L(\theta; x) = \Pf_\theta(\xi = x)$.
$$\widetilde{L}(\theta;\, T(x)) = \widetilde{L}(\theta;\, t) = \sum\limits_{x: T(x) = t} \Pf_\theta(\xi = x)$$

$$
0 \leqslant \Mf_\theta\left(\frac{\partial\ln L(\theta;
x)}{\partial\theta} - \frac{\partial\ln \widetilde{L}(\theta;
T(x))}{\partial\theta} \right)^2 = I^\xi(\theta) - 2\Mf_\theta
\left(\frac{\partial\ln L(\theta; x)}{\partial\theta} \cdot
\frac{\partial\ln \widetilde{L}(\theta; T(x))}{\partial\theta}
\right) + I^{T(\xi)}(\theta).
$$

Далее (здесь штрих означает производную по $\theta$),
$$
\widetilde L'(\theta; t) = \sum_{x:T(x)=t} \Pf_\theta'(\xi = x) =
\sum_{x:T(x) = t} L'(\theta; x),
$$
откуда
\begin{multline*}
\Mf_\theta\left( \frac{\partial \ln L}{\partial\theta} \cdot \frac{\partial \ln \widetilde{L}}%
{\partial\theta} \right) =
\sum_t \sum_{x: T(x) = t} \frac{L'(\theta;\, x)}{L(\theta;\,x)} \cdot \frac{\widetilde{L}'(\theta;\,T(x))}%
{\widetilde{L}(\theta;\,T(x))} L(\theta;\,x) = \sum_t
\frac{\widetilde{L}'(\theta;\,t)}{\widetilde{L}(\theta;\,t)} \cdot
\sum_{x:T(x)=t} L'(\theta;\,x) =\\= \sum_t \frac{\widetilde L'
(\theta;\,t)}{\widetilde L (\theta;\,t)} \widetilde L'(\theta;\,t)
= \sum_t \left( \frac{\widetilde L' (\theta;\,t)}{\widetilde
L(\theta;\,t)} \right)^2 \widetilde L(\theta;\,t) = \Mf\left(
\frac{\widetilde L' (\theta;\,T(\xi))}{\widetilde L
(\theta;\,T(\xi))} \right)^2 = I^{T(\xi)}(\theta),
\end{multline*}
поэтому $0 \leqslant I^\xi(\theta) - 2 I^{T(\xi)}(\theta) +
I^{T(\xi)}(\theta) = I^\xi(\theta) - I^{T(\xi)}(\theta)$, то есть
$I^\xi(\theta) \geqslant I^{T(\xi)}(\theta)$.
\end{proof}

\subsection{Эффективные оценки}
\subsubsection{Неравенство Рао - Крамера} \label{task11}

Пусть имеется регулярная модель с параметрическим семейством распределений
${\Ps=\{\Pf_\theta,\enskip\theta\in\Theta\}}$. Следующая теорема
дает нижнюю границу дисперсий оценок произвольной дифференцируемой
функции от параметра~$\theta$ в классе оценок с заданным
смещением.

\begin{theorem}[Неравенство Рао\ч Крам\'ера\footnote{а не Кр\'амера!}]
Пусть модель с параметрическим семейством распределений $\Ps$
регулярна, $x=(x_1,\ldots,x_n)$~--- выборка, и пусть некоторая
статистика $\hattheta(x)$ оценивает дифференцируемую функцию
$\tau(\theta)$ параметра $\theta$. Обозначим $b(\theta)=\Mf_\theta
\hattheta(x)-\tau(\theta)$~--- смещение оценки $\hattheta(x)$.
Если $b(\theta)$~--- дифференцируемая функция, то справедливо
неравенство
$$\Df_\theta\br{\hattheta(x)-\tau(\theta)-b(\theta)}\ge
\frac{[\tau'(\theta)+b'(\theta)]^2}{I_n(\theta)},$$ где
$I_n(\theta)$~--- количество информации Фишера о выборке $x$. При
этом неравенство обращается в равенство тогда и только тогда,
когда оценка $\hattheta(x)$ является линейной функцией вклада
выборки, \те
$\hattheta(x)-\tau(\theta)-b(\theta)=a(\theta)U(\theta;x)$.
\end{theorem}

В частности, если оценка $\hattheta(x)$ несмещенная,
$\Mf_\theta\hattheta(x)=\tau(\theta)$, то $b(\theta)=0$, и с
учетом $I_n(\theta)=nI_1(\theta)$ неравенство принимает вид
$\Df_\theta\hattheta(x)\ge\dfrac{\br{\tau'(\theta)}^2}{nI_1(\theta)}$.

\begin{proof}
По определению смещения оценки $\hattheta(x)$ имеем
$\tau(\theta)+b(\theta)=\Mf_\theta\hattheta(x)$.
Продифференцируем это равенство, записав $\Mf_\theta\hattheta(x)$
в виде интеграла и пользуясь условием регулярности 3):
$$\tau'(\theta)+b'(\theta)=\frac{\partial}{\partial\theta}\ints{X} \hattheta(x) L(\theta;x)\,dx=
\ints{X} \hattheta(x) \frac{\partial \ln
L(\theta;x)}{\partial\theta}L(\theta;x)\,dx=\ints{X} \hattheta(x)
U(\theta;x)L(\theta;x)\,dx=\Mf_\theta\br{\hattheta(x)U(\theta;x)}.$$

Учитывая, что $\Mf_\theta U(\theta;x)=0$, получаем:
$$\tau'(\theta)+b'(\theta)=\Mf_\theta\br{\hattheta(x)U(\theta;x)}=
\Mf_\theta\bs{\br{\hattheta(x)-\tau(\theta)-b(\theta)+\tau(\theta)+b(\theta)}U(\theta;x)}=
\Mf_\theta\bs{\br{\hattheta(x)-\tau(\theta)-b(\theta)}U(\theta;x)}+$$$$+\br{\tau(\theta)+b(\theta)}
\Mf_\theta
U(\theta;x)=\Mf_\theta\bbs{\br{\hattheta(x)-\Mf_\theta\hattheta(x)}
\br{U(\theta;x)-\Mf_\theta U(\theta;x)}}
=\cov_\theta(\hattheta(x),U(\theta;x)).$$

Применим к $\cov_\theta(\hattheta(x),U(\theta;x))$ неравенство
Коши\ч Буняковского $(\cov(\xi,\eta))^2\le \Df\xi\cdot\Df\eta$:

$$\br{\tau'(\theta)+b'(\theta)}^2=\br{\cov_\theta(\hattheta(x),U(\theta;x))}^2\le
\Df_\theta\hattheta(x)\Df_\theta
U(\theta;x)=\Df_\theta\br{\hattheta(x)-\tau(\theta)-b(\theta)}\cdot
I_n(x).$$

Отсюда следует требуемое неравенство. А так как неравенство
Коши\ч Буняковского превращается в равенство тогда и только тогда,
когда функции (в нашем случае случайные величины) линейно связаны,
то и наше неравенство превращается в равенство в том и только том
случае, когда (при каждом $\theta$) $\hattheta(x)$ и
$U(\theta;x)$ линейно связаны:
$\hattheta(x)-\tau(\theta)-b(\theta)=a(\theta)U(\theta;x)$.
Теорема доказана. \end{proof}


\begin{df}
Оценка, для которой в неравенстве Крамера-Рао имеет место равенство,
называется \emph{эффективной}.
\end{df}


\subsubsection{Многомерный случай} \label{task12}
Пусть $\theta = (\theta_1, \dots, \theta_s)^T$~---
вектор-параметр, $\Ps = \{ \Pf_\theta \mid \theta\in\Theta \}$.
Рассматривается оценка $\hattheta = (\hattheta_1, \dots,
\hattheta_s)$, $\hattheta_k = \hattheta_k(x_1, \dots, x_n)$.
Положим
$$
\lambda_j = \frac{\partial\ln L(\theta; x_1, \dots, x_n)}{\partial\theta_j},
\qquad
\Lambda = (\lambda_1, \dots, \lambda_s)^T.
$$
В многомерном случае роль дисперсии играет \emph{ковариационная матрица}
$\Sigma = \Sigma_{\hattheta(\xi)} = \Mf_\theta \bigl( (\hattheta(\xi) -
\Mf\hattheta(\xi))(\hattheta(\xi) - \Mf\hattheta(\xi))^T \bigr)$;
$\Sigma = (\sigma_{ij})$, $\sigma_{ij} = \Mf \bigl( (\hattheta_i - \Mf\hattheta_i)
(\hattheta_j - \Mf\hattheta_j) \bigr) = \cov (\hattheta_i, \hattheta_j )$;
$\sigma_{ii} = \sigma_i^2$ --- дисперсия, остальные --- попарные
ковариации. Несмещённость оценки задаётся равенством $\Mf_\theta \hattheta(\xi) = \theta$.

Аналогом количества информации Фишера является \emph{информационная матрица Фишера}
$J(\theta) = \Mf_\theta(\Lambda \Lambda^T)$. Предположим, что эта матрица обратима
(существует $J^{-1}(\theta)$). Тогда имеет место аналог теоремы Крамера\ч Рао:
матрица $\Sigma_{\hattheta(\xi)} - J^{-1}(\theta)$ является неотрицательно
определённой.

\subsubsection{Теорема Колмогорова - Блекуэлла - Рао} \label{task13}

$(\Xs, \As, \Ps = \{\Pf_\theta \mid \theta\in\Theta\})$ --- параметрическая модель.
$T(x)$ --- достаточная статистика, если существует вариант регулярной условной
вероятности $P_\theta(A\mid T(\xi)) \, \forall A\in\As$, не зависящий от параметра $\theta$.

\begin{theorem}[Колмогоров\ч Блекуэлл\ч Рао]
Пусть $T(x)$ --- достаточная статистика; $\hattheta(x)$ --- оценка параметра $\theta$ с
$\Mf_\theta \hattheta^2 < \infty$. Тогда оценка $\theta^*(\xi) = \Mf_\theta(\hattheta(\xi)\mid
T(\xi))$ обладает следующими свойствами:
\begin{nums}{-2}
\item $\hattheta^*(\xi)$ является статистикой и её распределение не зависит от $\theta$,
\item $\Mf_\theta\theta^*(\xi) = \Mf_\theta\hattheta(\xi)$,
\item $\Mf_\theta(\theta^*(\xi) - \theta)^2 \leqslant \Mf(\hattheta(\xi) - \theta)^2$.
Равенство имеет место тогда и только тогда, когда $\hattheta$ измерима относительно $\Cs_T$ (является
измеримой функцией достаточной статистики).
\end{nums}
\end{theorem}

\begin{proof}
В силу определения достаточной статистики 
$\theta^* = \Mf_\theta(\hattheta \mid T) = 
\int_{\Xs} \hattheta(X) d(\Pf(X \mid T))$ (регулярность варианта нужна для того,
чтобы тут взять интеграл по мере) не зависит от $\theta$, поэтому $\theta^*$ --- статистика.
$\theta^*$ --- $\Cs_T$-измерима (в силу определения УМО), поэтому $\theta^*$
есть функция от $T$.
$\Mf \theta^* = \Mf \bigl(\Mf(\hattheta\mid T)\bigr) = \Mf \hattheta$.
В силу неравенства Йенсена $\Mf\bigl( (\hattheta - \theta)^2 \mid T \bigr)
\geqslant \bigl( \Mf(\hattheta \mid T) - \theta \bigr)^2 =
(\theta^* - \theta)^2$. Возьмём математическое ожидание:
$\Mf (\hattheta - \theta)^2 =
\Mf \bigl( \Mf\bigl( (\hattheta - \theta)^2 \mid T \bigr) \bigr)
\geqslant \Mf (\theta^* - \theta)^2$, причём равенство в
неравенстве Йенсена достигается тогда и только тогда, когда
$\hattheta$ $\Cs_T$-измерима, т.е. есть функция от $T$.
\end{proof}

\begin{ex}
Рассмотрим случаные величины $\xi_i$ из схемы Бернулли с параметром $\theta \in (0, 1)$,
$T(\xi_1, \ldots, \xi_n) := \xi_1 + \ldots + \xi_n$. Найдём $\zeta := \Mf(\xi_1 \mid T)$.

Так как $\Mf(\xi_i \mid T) = \Mf(\xi_j \mid T)$ для любых $i, j$, то
по свойству линейности

$$
n \zeta = \Mf(\xi_1 \mid T) + \ldots + \Mf(\xi_n \mid T) = \Mf(T \mid T) = T
$$

Из чего следует, что $\zeta = T / n$.
\end{ex}

\subsubsection{Полная достаточная статистика} \label{task14}

\begin{df}
Достаточная статистика $T(x)$ называется \emph{полной} 
(для данного параметрического семейства распределений), 
если для любой измеримой функции $f(t)$ из условия
$\Mf_\theta f(T(\xi)) = 0$ ($\Pf$-п.н.) для любого $\theta \in \Theta$ 
следует, что $f(T(\xi)) = 0$ почти наверное.
\end{df}

Полная достаточная статистика $T$ обладает следующим свойством: если
$$\Mf_\theta f_1(T(\xi)) = \Mf_\theta f_2(T(\xi)),$$
то $f_1(T(\xi)) = f_2(T(\xi))$ почти наверное.
Доказательство очевидно.

\begin{theorem}[Колмогоров]
Пусть $T(x)$ --- полная достаточная статистика и $f$ и $g$ таковы, 
что $\Mf_\theta f(T(\xi)) = g(\theta)$. 
Тогда $f(T)$ является оптимальной оценкой для $g(\theta)$.
\end{theorem}

\begin{proof}
Докажем теорему в частном случае $g(\theta) = \theta$.

Дано, что $\Mf_\theta f(T) = \theta$, т.е. $\hattheta(\xi) := f(T(\xi))$ --- несмещённая оценка
параметра $\theta$. Докажем, что эта оценка имеет наименьшую дисперсию среди всех возможных
несмещённых оценок. От противного. Пусть существует такая оценка $\widetilde\theta$, что
$\Mf \widetilde\theta(\xi) = \theta$ ($\widetilde\theta$ --- несмещённая оценка) и
$\Df_{\theta_0} \widetilde\theta(\xi) < \Df_{\theta_0} \hattheta(\xi)$ хотя бы для одного
$\theta_0 \in \Theta$. Улучшим оценку $\widetilde\theta$ при помощи теоремы
Колмогорова\ч Блекуэлла\ч Рао: $\theta^* := \Mf_\theta(\widetilde\theta \mid T(\xi))$.
Такая оценка обладает следующими свойствами:
\begin{nums}{-2}
\item $\theta^* = \theta^*(T(x))$;
\item $\Mf_\theta \theta^* = \theta$;
\item $\Df_\theta \theta^* \leqslant \Df_\theta \widetilde\theta \,\, \forall\theta\in\Theta$.
\end{nums}
Имеем: $\Df_{\theta_0} \theta^* \leqslant \Df_{\theta_0} \widetilde\theta < \Df_{\theta_0}
\hattheta$. Но $\theta^*$ и $\hattheta$ суть функции полной достаточной статистики $T$,
$\Mf_\theta \theta^* = \theta = \Mf_\theta \hattheta$, откуда $\theta^*(\xi) = \hattheta(\xi)$
почти наверное, а поэтому $\Df_{\theta_0} \theta^* = \Df_{\theta_0} \hattheta$. Противоречие.
\end{proof}

\begin{note}
Полная достаточная статистика существует не всегда.
\end{note}

\subsubsection{Замечания о минимальной достаточной статистике}
\begin{nums}{-2}
\item Сама выборка является достаточной статистикой (например, по критерию факторизации).
\item Любая измеримая взаимно-однозначная функция от достаточной статистики сама является
достаточной статистикой.
\item Если для любой достаточной статистики $T$ существует такая измеримая 
функция $\psi$, что $T_{\min}(x) = \psi(T(x))$ ($T_{\min}$ \emph{подчинена} $T$), 
то $T_{\min}$ называется \emph{минимальной} достаточной
статистикой. 
Для любой биективной измеримой функции $\phi$ статистика
$\phi \circ T_{\min}$ тоже является минимальной достаточной.
Минимальная достаточная статистика существует всегда (для любой параметрической модели).
Можно проводить редукцию (без потери информации) от выборки к минимальной достаточной статистике.
Дальнейшая редукция невозможна.
\item Как правило, в разложении, которое даёт теорема Неймана\ч Фишера, появляется минимальная
достаточная статистика.
\item Полная достаточная статистика (если она существует) является минимальной.
\end{nums}

\subsection{Асимптотические свойства оценок} \label{task10}
\begin{nums}{-2}
\item \emph{Состоятельность оценки}. $\{ \Pf_\theta \mid \theta \in \Theta \}$~--- параметрическая
модель, ${\hattheta}_n$~--- оценка по выборке длины $n$,
$\theta_0$~--- истинное значение параметра. Оценка состоятельна,
если ${\hattheta}_n \raPnrai \theta_0$.

\item \emph{Асимптотическая несмещённость}. $\Mf_\theta {\hattheta}_n \ranrai \theta$
(\те смещение $b_n(\theta) = \Mf_\theta {\hattheta}_n  - \theta
\ranrai 0$).

\item \emph{Асимптотическая нормальность}. ${\hattheta}_n$ асимптотически нормальна, если
существует монотонно сходящаяся к нулю последовательность
положительных чисел $\left\{ c_n \right\}_{n=1}^{\infty}$ такая, что
$$
\frac{{\hattheta}_n - \theta}{c_n} \xrightarrow{\mathrm{d}} Z \sim \Norm(0,1)
$$
(сходимость по распределению к некоторой случайной величине со стандартным нормальным
распределением). Говорят, что $\frac{\hattheta_n - \theta}{c_n}$ асимптотически
нормальна с $\Norm(0,1)$, а $\hattheta_n$ асимптотически нормальна с $\Norm(\theta,c_n^2)$;
$\theta$ называется асимптотическим средним, а $c_n^2$ --- асимптотической дисперсией оценки
$\hattheta_n$. Асимптотически нормальная оценка автоматически является асимптотически
несмещённой. В качестве $c_n$ обычно берут $c_n^2 = \frac{\sigma^2(\theta)}{n}$ ($\sigma^2$
не зависит от выборки).
\begin{ex}
Схема Бернулли с параметром $p$ (вероятность успеха).
$\xi_1, \dots, \xi_n$ --- выборка.
$$
\widehat{p} = \frac{\xi_1 + \dots + \xi_n} n,
\qquad \Mf \widehat{p} = p, \qquad \Df \widehat{p} =
\frac{p(1-p)}{n} = \frac{\sigma^2(p)}{n}.
$$
$\widehat{p}$ асимптотически нормальна в силу теоремы Муавра --
Лапласа (ЦПТ). (Обычно через эту теорему асимптотическую
нормальность и доказывают.)
\end{ex}

\begin{ex}
$\xi_1, \dots, \xi_n$, $\xi_i \sim \Norm(a, \sigma^2)$.
$\Df \overline{\xi} = \frac{\sigma^2}{n}$.
\end{ex}

\item \emph{Асимптотическая эффективность}. Рассмотрим семейство распределений, подчинённых
условиям регулярности, для оценок параметров которого имеет место
неравенство Крамера\ч Рао. $1/{I_n(\theta)}$~--- нижняя граница
дисперсий всех оценок. Коэффициент эффективности оценки: $
e_n(\hattheta_n) = \frac{1/{I_n(\theta)}}{\Df\hattheta_n}. $ $0 <
e_n(\hattheta_n) \leqslant 1$. Если $e_n(\hattheta_n) = 1$, то
оценка называется эффективной. $e_\infty(\hattheta_n) =
\lim\limits_{\nrai} e_n(\hattheta_n)$. Если $e_\infty(\hattheta_n)
= 1$, то оценка называется асимптотически эффективной.
\par\emph{Асимптотическая эффективность в рамках асимптотической нормальности.}
Пусть $\hattheta_n$ асимптотически нормальна с $\Norm(\theta,
\frac{\sigma^2(\theta)}{n})$. Она
 называется асимптотически эффективной (в рамках
асимптотической нормальности), если
$$
\frac{1/I_n(\theta)}{\sigma^2(\theta)/n} \ranrai 1.
$$
\end{nums}

\begin{lemma}
Пусть $\xi_n := (\xi_n^{(1)}, \ldots, \xi_n^{(s)})$ - случайный вектор, 
$c: = (c_1, \ldots, c_s)$.
Если $\xi_n^{(i)} \xrightarrow{P} c_i$ для любого $i \in \{1, \ldots, s\}$ и 
$f$ - непрерывна в точке $c$, то $f(\xi_n) \xrightarrow{P} f(c)$.
\end{lemma}

\begin{proof}
Надо доказать, что для любого $\varepsilon > 0$

$$
\Pf(\omega \mid |f(\xi_n(\omega)) - f(c)| > \varepsilon) 
\xrightarrow{n \to \infty} 0
$$

Зафиксируем $\varepsilon > 0$. Положим
$A_{n, \varepsilon} := \{\omega \mid |f(\xi_n(\omega)) - f(c)| > \varepsilon \}$,
$B_{n, \delta}^{(i)} := \{\omega \mid |\xi_n^{(i)}(\omega) - c| > \delta \}$,
$B_{n, \delta} := \bigcap_{i=1}^s B_{n, \delta}^{(i)}$.
В силу непрерывности $f$ в точке $c$ существует $\delta > 0$ такое, 
что из того, что $|\xi_n(\omega) - c| \leq \delta$ следует
$|f(\xi_n(\omega)) - f(c)| \leq \varepsilon$,
то есть $\overline{B}_{n, \delta} \subset \overline{A}_{n, \varepsilon}$
или $A_{n, \varepsilon} \subset B_{n, \delta}$. Используя тот факт,
что 

$$
\Pf(B_{n, \delta}) \leq \sum_{i=1}^s \Pf(B_{n, \delta}^{(i)}) 
\xrightarrow{n \to \infty} 0
$$

получаем, что 

$$
\Pf(A_{n, \varepsilon}) \leq \Pf(B_{n, \delta}) \xrightarrow{n \to \infty} 0
$$
\end{proof}

\begin{lemma}[Слуцкого]\label{L:OMP1}
\begin{nums}{-4}
\item $X_n \radnrai X$, $Y_n \radnrai 0$ $\Rightarrow$ $X_n + Y_n \radnrai X$.
\item $X_n \radnrai X$, $Z_n \radnrai 1$ $\Rightarrow$ $X_n Z_n \radnrai X$, $X_n/Z_n \radnrai X$.
\end{nums}
\end{lemma}

\begin{proof}
Если $Y_n \radnrai 0$, $Z_n \radnrai 1$, то
$Y_n \raPnrai 0$, $Z_n \raPnrai 1$.
\end{proof}

\begin{lemma}
$\hattheta_n$ - оценка $\theta$,
имеет асимптотически нормальное распределение 
$\mathcal{N}(\theta, \sigma(\theta)^2 / n)$.
Пусть $f(\theta)$ --- дифференцируемая , $f'(\theta) \ne 0$. 
\end{lemma}


\begin{proof}
Запишем определение дифференцируемости $f$ в точке $\theta$
$$
f(t) = f(\theta) + (t - \theta) (f'(\theta) + \gamma(t, \theta)).
$$

Преобразуем выражение к
$$
\frac{f(t) - f(\theta)}{f'(\theta)} = 
(t - \theta) \left(1 + \frac{\gamma(t, \theta)}{
f'(\theta)}\right)
$$

Заменим $t$ на $\hattheta_n$ и умножим выражения на $\sqrt{n} / \sigma(\theta)$
$$
\sqrt{n} \frac{f(\hattheta_n) - f(\theta)}{f'(\theta) \sigma(\theta)} =
\sqrt{n} \frac{\hattheta_n - \theta}{f'(\theta) \sigma(\theta)}
\left(1 + \frac{\gamma(\hattheta_n, \theta)}{f'(\theta)}\right)
$$

Так как $\sqrt{n} (\hattheta_n - \theta) / \sigma(\theta) \radnrai 
\mathcal{N}(0, 1)$, а $\gamma(\hattheta, \theta) \raPnrai 0$,
то по свойствам сходимости $f(\hattheta_n)$ является 
асимптотически нормальной оценкой $f(\theta)$ 
с асимптотической СКО $|f'(\theta)| \sigma(\theta)$.

\end{proof}

\subsection{Методы получения оценок}

\begin{ex} \label{task15}
Схема Бернулли. $p = \Mf \xi_i$ --- параметр (вероятность удачи);
$\xi_i$~--- 1 или 0. $\overline{x} = \frac 1n (x_1 + \dots +
x_n)$~--- первый выборочный момент. $\widehat{p} = \overline{x}$
--- оценка $p$ по методу моментов. 
Эта оценка является несмещенной, так как
$$
\Mf \overline{\xi} = \frac{1}{n}(\Mf \xi_1 + \ldots + \Mf \xi_n) =
\frac{1}{n}(\underbrace{p + \ldots + p}_{n}) = p.
$$
По закону больших чисел $\overline{\xi}$ сходится к $p$ по вероятности.
Информация Фишера одного испытания равна
$$
I_1(p) = \Mf_p \left(\frac{\partial \ln 
\left(p^{\xi} (1-p)^{1-\xi}\right)}{\partial p}\right)^2 =
\Mf_p \left(\frac{\xi}{p} - \frac{1-\xi}{1-p}\right)^2 =
\frac{1}{p} + \frac{1}{1-p} = \frac{1}{p(1-p)},
$$
Поэтому $\Df_p \xi = 1 / I_1(p)$ и оценка эффективная.
\end{ex}

\subsubsection{Метод моментов} \label{task16}
Рассматривается статистическая модель с $s$-мерным параметром $\theta = (\theta_1, \dots, \theta_s)$.
$m_k(\theta) = \Mf \xi_i^k$ ($i = 1,\dots,n$, выборка повторная) --- $k$-й момент (истинный);
$\widehat{m}_k(\theta) = \frac{\xi_1^k + \dots + \xi_n^k}{n}$ --- эмпирический момент. Предположим, что
$\Mf \xi_i^s = m_s(\theta) < \infty$. Эмпирические моменты являются оценками для истинных. Запишем
систему \emph{моментных уравнений}:
$$
\begin{cases}
m_k(\theta_1, \dots, \theta_s) = \widehat{m}_k\\
1\leqslant k\leqslant s
\end{cases}
$$
Рассмотрим полученную систему относительно переменных $\theta_1, \dots, \theta_s$. Пусть существует
единственное решение ${\hattheta_j} \bw= f_j(\widehat{m}_1, \dots, \widehat{m}_s)$
($1\leqslant j\leqslant s$).
Мы получили некоторую оценку для $\theta$. Следующая теорема сообщает, что оценка не совсем
плохая.

\begin{theorem}[О~состоятельности статистических оценок, 
полученных методом моментов]

Пусть ${\hattheta}_1, \dots, {\hattheta}_s$ есть решение системы
моментных уравнений и пусть функции $f_j$ непрерывны. Тогда оценки
${\hattheta}_j = f_j(\widehat{m}_1, \dots, \widehat{m}_s)$ для
всех $j$ являются состоятельными оценками параметров $\theta_j$
(т.е. $\hattheta_j \raPnrai \theta_j$ для всех $j$).
\end{theorem}

\begin{proof}
В силу асимптотического свойства моментов имеет 
$\widehat{m}_k(\theta) \raPnrai m_k(\theta)$.
Так как $f_j$ непрерывны, то они сохраняют сходимость по вероятности и
$f_j(\widehat{m}_1(\theta), \ldots, \widehat{m}_s(\theta)) \raPnrai \theta_j$.
\end{proof}

\begin{ex}
$\xi_1, \dots, \xi_n \sim \Norm(a, \sigma^2)$, $\Mf \xi_i = a$,
$\Df \xi_i = \sigma^2$. $\overline{x} = \widehat{a}$~--- хорошая
оценка. $\widehat{\sigma^2} = s^2 = \frac 1n ((x_1 -
\overline{x})^2 + \dots + (x_n - \overline{x})^2)$~--- выборочная
дисперсия\т смещённая оценка $\sigma^2$. Это оценки, полученные
по методу моментов. А вот такая оценка:
$\widehat{\widehat{\sigma^2}} = s^2 \frac{n}{n-1}$ является
несмещённой.
\end{ex}

\subsubsection{Метод максимального (наибольшего) правдоподобия} \label{task17}
\paragraph{Принцип МП в простейшем случае}
Принцип МП был рассмотрен ещё Гауссом в следующей форме: найдём такие значения параметров, чтобы вероятность
получить данную выборку была максимальной (берём $\max\limits_{\theta\in\Theta} \Pf_\theta(\xi_1 = x_1,
\dots, \xi_n = x_n) = \Pf_{\theta^{*}}(\xi_1 = x_1, \dots, \xi_n = x_n)$). $\theta^{*}$ --- оценка МП
(наиболее правдоподобное значение $\theta$).

\paragraph{Общая ситуация}
$x_1, \dots, x_n$ --- выборка в $(\Xs\!\!, \As, \Ps)$, $\Ps = \{ \Pf_\theta \mid \theta\in\Theta \}$.
$L(\theta; x_1, \dots, x_n)$ --- функция правдоподобия. Возьмём $\theta^{*}$ такое, что
$$
\sup\limits_{\theta\in\Theta} L(\theta; x_1,\dots,x_n) = L(\theta^{*}; x_1, \dots, x_n)
$$
(если точная верхняя грань достигается). $\theta^{*}$ называется \emph{оценкой максимального правдоподобия
(ОМП)}.

\begin{ex}
$\xi_1, \dots, \xi_n$ взаимно независимы и равномерно распределены на
$[0, \theta]$; $\theta \in \Theta = (0, +\infty)$.\\
$L(\theta; x_1, \dots, x_n) = f_\theta(x_1)\cdot \dots\cdot f_\theta(x_n)$,
где $f_\theta(x) = \frac 1\theta \mathrm{I}_{[0, \theta]} (x)$ --- плотность
равномерного распределения. $L \ne 0$ тогда и только тогда, когда
для всех $i$ $x_i \in [0, \theta]$.
$$
L = \begin{cases}
\frac 1 {\theta^n} & x_{(n)} \leqslant \theta \\
0 & \text{в ином случае}
\end{cases}
$$
Ясно, что максимум $L$ достигается при $\theta = \theta^* = x_{(n)} =
\max\limits_i x_i$. $\theta^*$ --- ОМП.
\end{ex}

Предположим, что функция $L(\theta; x_1, \dots, x_n)$ дифференцируема по параметру $\theta$
($\theta\in\Theta\subset\mathbb{R}$). Тогда ОМП
можно найти, решая \emph{уравнение правдоподобия}:
$
\frac{\partial L(\theta)}{\partial \theta} = 0
$
(или переходим к логарифмам: $\frac{\partial \ln L}{\partial \theta} = 0$, если это возможно).
В случае $\Theta\subset\mathbb{R}^k$ получаем систему уравнений.

{\bf Условия регулярности} (\textasteriskcentered):
\begin{nums}{-2}
\item $\Theta$ --- невырожденный замкнутый интервал на $\mathbb R$; существуют
$\frac{\partial \ln f_{\theta}}{\partial \theta}$, 
$\frac{\partial^2 \ln f_{\theta}}{\partial \theta^2}$,
$\frac{\partial^3 \ln f_{\theta}}{\partial \theta^3}$ 
для любого $\theta\in\Theta$.

\item Для всех $\theta\in\Theta$:
$$
\left| \frac{\partial \ln f_{\theta}}{\partial \theta} \right| 
\leqslant g_1(x), \qquad
\left| \frac{\partial^2 \ln f_{\theta}}{\partial \theta^2} \right| 
\leqslant g_2(x), \qquad
\left| \frac{\partial^3 \ln f_{\theta}}{\partial \theta^3} \right| 
\leqslant H(x),
$$
где $g_1$ и $g_2$ интегрируемы на $\mathbb R$, а $H$ обладает следующим свойством:
$$
\Mf_\theta H = \int\limits_{-\infty}^{+\infty} H(x) f_\theta(x) \, dx < M
$$
($M$ не зависит от $\theta$).

\item
$$
0 < I_1(\theta) = \int\limits_{-\infty}^{+\infty}
\left(\frac{\partial \ln f_{\theta}}{\partial \theta}\right)^2
f_\theta(x) \, dx < \infty.
$$
\end{nums}
Эти условия содержат условия Крамера\ч Рао.

\begin{theorem}[о свойствах ОМП --- теорема Дюге] \label{task18}
Пусть выполнены 
условия регулярности {\rm (\textasteriskcentered)}.
Зафиксируем $\theta_0 \in \Theta$.
Пусть для любых $\varepsilon > 0, \delta > 0$ 
существует такой $N$, что для любых $n > N$ существует и единственно
решение $\theta^*$ уравнение правдоподобия, принадлежащее интервалу
$(\theta_0 - \delta, \theta_0 + \delta)$ с вероятностью
не меньшей чем $1 - \varepsilon$.
Тогда ОМП обладает следующими асимптотическими свойствами:
\begin{nums}{-4}
\item состоятельность;
\item асимптотическая нормальность;
\item асимптотическая эффективность.
\end{nums}
\end{theorem}

\begin{proof}
$$
\frac{\partial \ln f_\theta(x)}{\partial\theta} = \{ \mbox{разложение Тейлора} \} =
\left. \frac{\partial \ln f_\theta}{\partial\theta} \right|_{\theta=\theta_0} +
(\theta-\theta_0) \left. \frac{\partial^2 \ln f_\theta}{\partial \theta^2}\right|_{\theta=\theta_0} +
\frac 12 \tau (\theta-\theta_0)^2 H(x)
$$

($x\in\mathbb R$, $|\tau| < 1$). Вспоминаем, что $L(\theta; x_1, \dots, x_n) = f_\theta(x_1)\cdot\dots
\cdot f_\theta(x_n)$, и переписываем уравнение правдоподобия

$$
\frac 1n \frac{\partial \ln L(\theta; x_1, \dots, x_n)}{\partial \theta} = 0
$$

в виде:

$$
\mbox{F}(\theta) = B_0 + (\theta-\theta_0) B_1 + \frac 12 \tau (\theta-\theta_0)^2 B_2 = 0,
$$

где

$$
B_0 = \frac 1n \sum_{i=1}^{n} \left.\left(\frac{\partial\ln f_\theta(\xi_i)}{\partial\theta}%
\right)\right|_{\theta=\theta_0} \qquad
B_1 = \frac 1n \sum_{i=1}^{n} \left.\left(\frac{\partial^2\ln f_\theta(\xi_i)}{\partial\theta^2}%
\right)\right|_{\theta=\theta_0} \qquad
B_2 = \frac 1n \sum_{i=1}^{n} H(\xi_i)
$$

По ЗБЧ Хинчина (нужны н.о.р. сл. вел. с конечным матожиданием) 
$B_i \raPnrai \Mf_{\theta} B_i$, поэтому найдём $\Mf_{\theta} B_i$.

В условиях регулярности меняем дифференцирование по $\theta$ и интегрирование:
$$\Mf_{\theta} \frac{\partial\ln f_\theta(\xi)}
{\partial\theta} =
\iii \frac{\partial\ln f_\theta(x)}
{\partial\theta} f_\theta(x) \, dx =
\iii \frac{1}{f_\theta(x)} \cdot
\frac{\partial f_\theta(x)}{\partial\theta} \cdot f_\theta(x) \, dx =
\frac\partial{\partial\theta} \underbrace{\iii
f_\theta(x) \, dx}_{= 1} = 0
$$

Отсюда $\Mf_{\theta} B_0 = \frac 1n \sum\limits_{i=1}^{n}
 \Mf_{\theta} \frac{\partial\ln f_\theta(\xi_i)}
{\partial\theta} = 0$.

Утверждение $\Mf_{\theta} B_1 = -I_1(\theta)$ 
вытекает из следующей выкладки\footnote%
{Лектор на неё забил, но я всё же приведу её. --- \emph{примеч. С.\,К.}}:

\begin{multline*}
\Mf_{\theta} \frac{\partial^2 \ln f_\theta(\xi)}{\partial\theta^2} =
\iii \frac\partial{\partial\theta}
\frac{\partial \ln f_\theta(x)}{\partial\theta}
f_\theta(x) \, dx =
\iii \frac\partial{\partial\theta}
\left(\frac 1 {f_\theta(x)} \cdot \frac{\partial f_\theta(x)}{\partial\theta}
\right) f_\theta(x) \, dx ={}\\=
\iii \frac{-1}{f^2_\theta(x)} \cdot \frac{\partial f_\theta(x)}{\partial\theta}
\cdot \frac{\partial f_\theta(x)}{\partial\theta} f_\theta(x) \, dx +
\iii \frac{\partial^2 f_\theta(x)}{\partial\theta^2} \cdot \frac 1
{f_\theta(x)} f_\theta(x) \, dx =
- \iii \left( \frac{\partial f_\theta(x)}{\partial\theta} \right)^2
\frac 1 {f_\theta(x)} \, dx +{}\\+
\underbrace{\frac{\partial^2}{\partial\theta^2}
\iii f_\theta(x) \, dx}_{=0} =
- \iii \left( \frac{\partial \ln f_\theta(x)}{\partial\theta} f_\theta(x)
\right)^2 \frac 1 {f_\theta(x)} \, dx =
- \iii \left( \frac{\partial \ln f_\theta(x)}{\partial\theta} \right)^2
f_\theta(x) \, dx ={}\\=
- \Mf \left(\frac{\partial\ln f_\theta(\xi)}{\partial\theta}
\right)^2 = - I_1(\theta).
\end{multline*}

Про $B_2$ нам известно из условий регулярности,
что $\Mf_{\theta} B_2 = \Mf_{\theta} H(\xi_i) < M$.

Докажем асимптотическую нормальность и асимптотическую эффективность. 
Имеем $\mbox{F}(\theta^{*}) = 0$.
Отсюда
$$
\theta^{*} - \theta_0 = \frac{B_0}{-B_1 - \frac 12 \tau (\theta^{*} - \theta_0)B_2}.
$$
Далее ($k^2 = I_1(\theta)$),
$$
\frac{\theta^{*} - \theta_0}{\frac{1}{\sqrt{nk^2}}} = \frac{\frac{1}{\sqrt{nk^2}}
\sum\limits_{i=1}^{n} \frac{\partial \ln f(\xi_i)}{\partial\theta}}%
{-\frac{B_1}{k^2} - \frac 12 \tau \frac{\theta^{*} - \theta_0}{k^2}B_2}.
$$
$Y_i = \frac{\partial \ln f(\xi_i)}{\partial\theta}$
 --- независимые и одинаково распределённые с $\Mf_{\theta} Y_i = 0$ и 
 $\Df_{\theta} Y_i = \Mf_{\theta} Y_i^2 = k^2$. В силу
ЦПТ числитель
$$
\frac 1 {\sqrt{nk^2}} \sum_{i=1}^n 
\frac{\partial \ln f(\xi_i)}{\partial \theta} \radnrai
Z \sim \Norm(0,1).
$$
$B_1 \raP -k^2$, $B_2 \raP \Mf H(\xi_1) < M$, откуда
знаменатель
$$
-\frac{B_1}{k^2} - \frac 12 \tau \frac{\theta^{*} - \theta_0}{k^2} B_2
\raP 1
$$
(второе слагаемое оценивается через $\frac 12 \frac M {k^2}
(\theta^* - \theta_0)$, а $\theta^* - \theta_0 < \delta \to 0$).
Тогда (в силу леммы Слуцкого \ref{L:OMP1}) $\sqrt{nk^2}(\theta^{*} -
\theta_0)$ асимптотически нормальна с $\Norm(0,1)$. Отсюда
$\theta^{*}$ асимптотически нормальна с $\Norm(\theta_0,
\frac{1}{nk^2})$, а т.\,к. асимптотическая дисперсия $nk^2 =
I_n(\theta)$, то $\theta^{*}$ асимптотически эффективна (в рамках
асимптотической нормальности).
\end{proof}

\begin{problem}
Пусть $\Ps = \{ \Pf_\theta \mid \theta\in\Theta \}$ --- регулярное параметрическое семейство
оценка $\hattheta$, то $\hattheta$ есть ОМП.
\end{problem}

\section{Байесовские статистические оценки}\label{bayes_est}
\subsection{Байесовские точечные оценки}
\subsubsection{Функция риска} \label{tasks19}
Рассмотрим функцию $u$, называемую функцией штрафа (потерь), обладающую следующими
свойствами:
\begin{nums}{-2}
\item $u$ чётна;
\item $u$ возрастает на $(0; +\infty)$;
\item $u(0) = 0$.
\end{nums}
\emph{Функцией риска} оценки $\delta$ параметра $\theta$
называется функция $R(\theta; \delta) = \Mf\bigl(u(\delta - \theta)\bigr)$. В частном
случае $u(x) = x^2$ (обычно рассматриваемом на практике) функция риска
$R(\theta; \delta) = \Mf(\delta - \theta)^2$ называется
\emph{квадратичной функцией риска}. В теории эффективных оценок статистики сравниваются
в смысле равномерной минимизации риска.

\begin{ex}
В схеме Бернулли оцениваем параметр $p$. Эффективная оценка --- частота
$\hat p = \frac 1n (x_1 + \dots + x_n)$. Для неё функция риска (квадратичная)
равна $R_1 = R(p; \hat p) = \frac{p(1-p)}{n}$. Пусть теперь $n=2$ (выборка состоит
из двух элементов $x_1, x_2$). Рассмотрим странную оценку $\hat{\hat p} = \frac 12$
для всех значений $x_i$. Её функция риска есть $R_2 = R(p, \hat{\hat p}) = (\frac 12 -
p)^2$. Если нам откуда-то известно (имеется априорная информация), что $\frac 13 \leqslant
p \leqslant \frac 23$, то странная оценка $\hat{\hat p}$ оказывается лучше.
\end{ex}

\subsubsection{Байесовский подход}
Параметр $\theta$ рассматривается как случайная величина $\btheta$ со
значениями $\theta\in\Theta$ и распределением вероятностей $\Pi$ на измеримом пространстве
$(\Theta, \Bs_\Theta)$. Распределение $\Pi$ называется \emph{априорным распределением} параметра.

Усредним функцию риска $R(\theta;\delta)$ по распределению параметра:
$$
R(\Pi; \delta) = \int\limits_\Theta R(\theta;\delta) \, \Pi(d\theta).
$$
$R(\Pi; \delta)$ называется \emph{априорным риском}. Оценка $\delta^*$, минимизирующая
априорный риск, называется \emph{байесовской оценкой} параметра $\theta$:
$R(\Pi; \delta^*) = \inf\limits_{\delta\in\Delta} R(\Pi; \delta)$ ($\Delta$ ---
множество оценок). Можно схитрить: взять априорное распределение так, чтобы
получить лучшую оценку.

\subsection{Теорема о байесовской оценке для квадратичной функции риска}
Рассмотрим функцию $\Pf_\theta(A)$, зависящую от $A\in \As$ и $\theta \in \Theta$.
При каждом фиксированном $A$ $\Pf_\theta(A)$ есть $\Bs_\Theta$-измеримая функция от $\theta$,
а при каждом фиксированном $\theta$ --- задаёт распределение вероятностей на $(\Xs, \As)$.
Рассмотрим на декартовом произведении $\Theta \times \Xs$ $\sigma$-алгебру, порождённую
прямоугольниками $B \times A$, где $B \in \Bs_\Theta$, $A \in \As$. На этих прямоугольниках введём
меру $Q$:
$$
Q(B \times A) = \int\limits_B \Pf_\theta(A) \, \Pi(d\theta).
$$
Эта мера $\sigma$-аддитивна на полукольце прямоугольников.
Значит, она продолжается по Лебегу на всю $\sigma$-алгебру, которую мы назовём $\Bs_\Theta \times
\As$. Будем рассматривать квадратичную функцию потерь и предполагать, что $R(\Pi; \delta) =
\Mf_Q(\btheta - \delta(\xi))^2 < \infty$.

\begin{theorem}[о байесовской оценке для квадратичной функции риска]
Пусть априорное распределение $\Pi$ и оценка $\delta(x)$ таковы, что $R(\Pi; \delta) =
\Mf_Q(\btheta - \delta(\xi))^2 < \infty$. Тогда байесовской оценкой параметра $\theta$
является $\delta^*(\xi) = \Mf_Q(\btheta \mid \xi)$.
\end{theorem}

\begin{proof}
\begin{nums}{-2}
\item $\delta^*$ является статистикой, т.к. она зависит от $\btheta$ посредством
распределения, а оно фиксировано.
\item $\Mf_Q\bigl((\btheta - \delta(\xi))^2 \mid \xi\bigr)$, $\Mf_Q(\btheta^2\mid\xi)$ и
$\Mf_Q(\btheta\mid\xi)$ существуют.
\item $\Mf_Q(\btheta - \delta^*(\xi))^2 = 
\Mf_Q \bigl(\Mf_Q((\btheta - \delta^*(\xi))^2 \mid \theta) \bigr) \leq
\Mf_Q \bigl(\Mf_Q((\btheta - \delta(\xi))^2 \mid \theta) \bigr) =
\Mf_Q(\btheta - \delta(\xi))^2$ в силу свойств
УМО ($\delta^* = \Mf_Q(\btheta\mid\xi)$). Равенство~--- при
$\delta^*(\xi) = \Mf_Q(\btheta\mid\xi)$.
\end{nums}
\end{proof}

\begin{imp}[единственность байесовской оценки]
Пусть $\delta_1^*, \delta_2^*$ --- две байесовские оценки (относительно
квадратичной функции потерь). Тогда $Q(\delta_1^* \ne \delta_2^*) = 0$.
\end{imp} 

\begin{proof}
Положим $C := \{(\theta, x) \mid \delta_1^{*}(x) \ne \delta_2^{*}(x)\}$.
Так как значение $\delta_i^{*}$ не зависят от параметра, 
то $C = \Theta \times A$, где $A$ --- некоторое подмножество $\As$. 
Так как УМО определено с точностью до меры нуль, то 
$$
Q(C) = \int_{\Theta} \Pf_{\theta}(A) \, \Pi(d \theta) = 
\int_{\Theta} 0 \, \Pi(d \theta) = 0.
$$
\end{proof}

\subsection{Апостериорный риск}

Пусть семейство $\Ps$ абсолютно непрерывно относительно лебеговой меры. Тогда для любого
события $A \in \As$
$$\Pf_\theta(A) = \int\limits_A p_\theta(x) \, dx.$$

\begin{stm}
Пусть $\delta^*$~--- байесовская оценка относительно квадратичной
функции потерь. Тогда $\Pf_\theta$-п.\,н. имеем
$$
\delta^* = \frac{\int\limits_\Theta \theta p_\theta(x) \, \Pi(d\theta)}
{\int\limits_\Theta p_\theta(x) \, \Pi(d\theta)}.
$$
\end{stm}

\begin{proof}
Пусть $A \in \As$ --- произвольное событие. Положим $C = \Theta \times A \in \Bs_\Theta \times \As$.
По определению УМО и теореме о байесовской оценке
$$
\int\limits_C \delta^* \, dQ = 
\Mf_Q (I_{\xi \in A} \Mf_Q(\theta \mid \xi)) =
\Mf_Q (\Mf_Q(I_{\xi \in A} \theta \mid \xi)) =
\Mf_Q(I_{\xi \in A} \theta) =
\int\limits_C \btheta \, dQ.
$$
Вспоминая определение меры $Q$ и теорему Фубини, получаем:
$$
\int\limits_A \delta^*(x) \left( \int\limits_\Theta p_\theta(x) \, \Pi(d\theta) \right) \, dx =
\int\limits_C \delta^* \, dQ = \int\limits_C \btheta \, dQ =
\int\limits_A \left( \int\limits_\Theta \theta p_\theta(x) \, \Pi(d\theta) \right) \, dx,
$$
откуда в силу произвольности выбора $A$ получаем, что
$$
\delta^*(x) \int\limits_\Theta p_\theta(x) \, \Pi(d\theta) =
\int\limits_\Theta \theta p_\theta(x) \, \Pi(d\theta) \quad \text{$\Pf_\theta$-п.н.}
$$
\end{proof}

Если априорное распределение $\Pi$ имеет плотность $\pi(\theta)$, то
$$
\delta^*(x) = \frac{\int\limits_\Theta \theta p_\theta(x) \pi(\theta) \, d\theta}
{\int\limits_\Theta p_\theta(x) \pi(\theta) \, d\theta} =
\int\limits_\Theta \theta q(\theta \mid x) \, d\theta,
$$
где
$$
q(\theta \mid x) = \frac{p_\theta(x) \pi(\theta)}{\int\limits_\Theta p_\theta(x) \pi(\theta) \, d\theta}
$$
называется \emph{апостериорной плотностью}\footnote{
Для многих распределений существуют априорные и 
соответствующие им апостериорные распределения, входящие в одни и те же
семейства. Такие априорные распределения называют \emph{сопряженными}.
Таблицу сопряженных распределений можно посмотреть на \href{https://en.wikipedia.org/wiki/Conjugate_prior}{википедии}.
--- \emph{примеч. В.\,Х.}} (плотностью апостериорного распределения параметра).
Это условное распределение $\btheta$ при условии $\xi$. Последняя формула называется
формулой Байеса для плотностей.

\begin{note} Байесовская оценка минимизирует апостериорный риск $\Mf_Q((\btheta - \delta(\xi))^2 \mid
\xi)$ (без доказательства).
\end{note}

\begin{ex} \label{task20}
Рассмотрим биномиальное распределение $\xi_i \sim \text{Binom}(m_i, p)$.
Оценим параметр $p$ для априорного распределения 
$\pi \sim \text{Beta}(\alpha, \beta)$ с квадратичной функцией потери.
Из выше доказанной теоремы следует, что
$$
\delta^{*}(x) = \int_0^1 p q(p \mid x) dp, \,
q(p \mid x) = \frac{f_p(x) \pi(p)}{\int_0^1 f_p(x) \pi(p) dp} =
\frac{f_{1, p}(x_1) \cdots f_{n, p}(x_n) \pi(p)}{
\int_0^1 f_{1, p}(x_1) \cdots f_{n, p}(x_n) \pi(p) dp}.
$$
Пусть $k := \sum_{i=1}^n x_i$, тогда
\begin{multline*}
\int_0^1 f_{1, p}(x_1) \cdots f_{n, p}(x_n) \pi(p) dp = 
\int_0^1 C_{m_1}^{x_1} p^{x_1} (1-p)^{1-x_1} \cdots 
C_{m_n}^{x_n} p^{x_n} (1-p)^{1-x_n} 
\frac{p^{\alpha-1} (1-p)^{\beta-1}}{B(\alpha, \beta)} dp =\\
C_{m_1}^{x_1} \cdots C_{m_n}^{x_n} \int_0^1 p^k (1-p)^{n-k} 
\frac{p^{\alpha-1} (1-p)^{\beta-1}}{B(\alpha, \beta)} dp =
C_{m_1}^{x_1} \cdots C_{m_n}^{x_n}
\frac{B(k+\alpha, n-k+\beta)}{B(\alpha, \beta)},
\end{multline*}
откуда апостериорное распределение равно
$$
q(p \mid x) = \frac{C_{m_1}^{x_1} \cdots C_{m_n}^{x_n} 
p^{k+\alpha-1} (1-p)^{n-k+\beta-1} \text{Beta}(\alpha, \beta)}{
\text{Beta}(\alpha, \beta) C_{m_1}^{x_1} \cdots C_{m_n}^{x_n} B(k+\alpha, n-k+\beta)} =
\frac{p^{k+\alpha-1} (1-p)^{n-k+\beta-1}}{B(k+\alpha, n-k+\beta)}.
$$
Заключаем
\begin{multline*}
\delta^{*}(x) = \int_0^1 p \frac{p^{k+\alpha-1} (1-p)^{n-k+\beta-1}}{
B(k+\alpha, n-k+\beta)} dp = \frac{B(k+\alpha+1, n-k+\beta)}{B(k+\alpha, n-k+\beta)} = 
\frac{\Gamma(n+\alpha+\beta) \Gamma(k+\alpha+1) \Gamma(n-k+\beta)}{
\Gamma(k+\alpha) \Gamma(n-k+\beta) \Gamma(n+\alpha+\beta+1)} = \\
\frac{k+\alpha}{n+\alpha+\beta} = 
\frac{\sum_{i=1}^n x_i + \alpha}{n+\alpha+\beta}.
\end{multline*}
\end{ex}

\begin{ex}
Рассмотрим нормальное распределение $\xi_i \sim \Norm(\mu, \sigma^2)$, 
где $\sigma$ известно. Оценим параметр $\mu$ для априорного распределения
$\pi \sim \Norm(\mu_0, \sigma_0^2)$ с квадратичной функцией потери.
Из выше доказанной теоремы следует, что
$$
\delta^{*}(x) = \int_{-\infty}^{\infty} \mu q(\mu \mid x) d \mu, \,
q(\mu \mid x) = \frac{f_{\mu}(x) \pi(\mu)}{
\int_{-\infty}^{+\infty} f_{\mu}(x) \pi(\mu) d \mu} =
\frac{f_{1, \mu}(x_1) \cdots f_{n, \mu}(x_n) \pi(\mu)}{
\int_{-\infty}^{+\infty} f_{1, \mu}(x_1) \cdots f_{n, \mu}(x_n) 
\pi(\mu) d \mu}.
$$
Распишем знаменатель:
$$
\int_{-\infty}^{+\infty} f_{1, \mu}(x_1) \cdots f_{n, \mu}(x_n) 
\pi(\mu) d \mu =
\int_{-\infty}^{+\infty} \frac{1}{(\sigma \sqrt{2 \pi})^n}
e^{-\sum_{i=1}^n (x_i - \mu)^2 / (2 \sigma^2)}
\frac{1}{\sigma_0 \sqrt{2 \pi}} e^{-(\mu - \mu_0)^2 / (2 \sigma_0^2)} d \mu.
$$
Под экспонентой стоит многочлен от $\mu$. Выделим полный квадрат:
\begin{multline*}
-\frac{\sum_{i=1}^n (x_i - \mu)^2}{2 \sigma^2} -
\frac{(\mu - \mu_0)^2}{2 \sigma_0^2} = 
-\frac{1}{2 \sigma^2 \sigma_0^2} \left(
(n \sigma_0^2 + \sigma^2) \mu^2 - 2 \left(\sigma_0^2 \sum_{i=1}^n x_i +
\sigma^2 \mu_0\right) \mu + \sigma_0^2 \sum_{i=1}^n x_i^2 + 
\sigma^2 \mu_0^2\right) = \\
-\frac{n \sigma_0^2 + \sigma^2}{2 \sigma^2 \sigma_0^2}
\left(\left(\mu - \frac{\sigma_0^2 \sum_{i=1}^n x_i + \sigma^2 \mu_0}{
n \sigma_0^2 + \sigma^2}\right)^2 - b\right) =
C \left(-(\mu - a)^2 / 2 + b\right).
\end{multline*}
Подставляя в интеграл и делая замену переменных, получаем
$$
D \int_{-\infty}^{+\infty} \exp 
\left\{C \left(-(\mu - a)^2 / 2 + b\right)\right\} d \mu =
D e^{C b} \int_{-\infty}^{+\infty} e^{C -\widetilde{\mu}^2 / 2} 
d \widetilde{\mu} = D e^{C b} \sqrt{2 \pi / C}.
$$
Апостериорная плотность равна
$$
q(\mu \mid x) = \frac{D \exp \left\{C (-(\mu - a)^2 / 2 + b)\right\}}{
D e^{C b} \sqrt{2 \pi C}} = \frac{1}{\sqrt{2 \pi / C}} 
e^{- C (\mu - a)^2 / 2},
$$
где 
$$
a = \frac{\sigma_0^2 \sum_{i=1}^n x_i + \sigma^2 \mu_0}{
n \sigma_0^2 + \sigma^2}, \, 
C = \frac{n \sigma_0^2 + \sigma^2}{2 \sigma^2 \sigma_0^2}.
$$
По формуле плотности видно, что апостериорная плотность суть плотность
нормального распределения с параметрами $a, C^{-1}$ ($C^{-1}$ - дисперсия),
поэтому байесовская оценка как матожидание случайной величины
с апостериорной плотностью равна 
$$
\delta^{*}(x) = a = \frac{\sigma_0^2 \sum_{i=1}^n x_i + \sigma^2 \mu_0}{
n \sigma_0^2 + \sigma^2}.
$$
\end{ex}


\subsection{Свойства байесовских оценок} \label{task21}
\begin{nums}{-2}
\item Байесовская оценка смещена для любого априорного распределения
$\Pi$, то есть $\Mf_{\theta} \delta^{*}(\xi) \ne \theta$ 
для любого $C$ такого, что $\Pi(C) > 0$.

\item Апостериорное распределение параметра относительно любого априорного распределения является
функцией достаточной статистики. Действительно, пусть $T(x)$ --- достаточная статистика. Тогда
по теореме Неймана\ч Пирсона $p_\theta(x) = g(\theta; T(x)) h(x)$. При условии существования плотности $\pi$
априорного распределения для плотности $q(\theta\mid x)$ условного распределения имеет место следующее:
$$
q(\theta \mid x) = \frac{p_\theta(x) \pi(\theta)}{\int\limits_\Theta p_\theta(x) \pi(\theta) \, d\theta} =
\frac{g(\theta; T(x)) \pi(\theta)}{\int\limits_\Theta g(\theta; T(x)) \pi(\theta) \, d\theta} =
f(T(x)),
$$
т.е. плотность апостериорного распределения есть функция достаточной статистики.

\item При некоторых дополнительных условиях на $\Pi$ (например, если $\pi(\theta) \ne 0$ при всех
$\theta \in \Theta$) верно и обратное: если апостериорное распределение
$\btheta$ зависит от $x$ посредством статистики $T(x)$, то $T(x)$ является достаточной статистикой.
Таким образом мы получили байесовский критерий достаточности статистики.
\end{nums}

\subsection{Минимаксные оценки}
Пусть теперь нет ни оптимальной оценки, ни априорной информации.
$R_M(\delta) = \sup\limits_{\theta\in\Theta} R(\theta; \delta)$ --- наихудшее значение функции
риска.
\begin{df}
$\delta^*$ называется \emph{минимаксной} оценкой $\theta$, если
$R_M(\delta^*) = \inf\limits_{\delta\in\Delta} R_M(\delta)$ (т.е. она
минимизирует максимальное значение функции риска).
\end{df}

\begin{theorem}
Пусть задано априорное распределение $\Pi$.
Если $\delta^{*}$ является байесовской и $R(\Pi, \delta^{*}) = \const$,
то $\delta^{*}$ --- минимаксна.
\end{theorem}

\begin{proof}
Если $\delta^{*}$ такая, как в теореме, но не является минимаксной,
то найдется такая оценка $\delta$, что 
$$
\sup\limits_{\theta \in \Theta} R(\theta, \delta) < 
\sup\limits_{\theta \in \Theta} R(\theta, \delta^{*}) =
R(\theta, \delta^{*}).
$$
Тогда при всех $\theta \in \Theta$ справедливо неравенство
$$
R(\theta, \delta) < R(\theta, \delta^{*}).
$$
Но поскольку $\delta^{*}$ -- байесовская, то
$$
\int_{\Theta} R(\theta, \delta^{*}) \pi(\theta) d \theta \leq
\int_{\Theta} R(\theta, \delta) \pi(\theta) d \theta.
$$
Эти два неравенства противоречат друг другу, поэтому предположение
было неверно и $\delta^{*}$ -- минимаксная.
\end{proof}

\begin{ex} \label{task46}
Рассмотрим $\xi_i \sim \text{Bern}(p)$, $p \in (0, 1)$.
Найдём ОМП в данной модели.
$$
L(p; x_1, \ldots, x_n) = p^{m} (1-p)^{n-m} I_{x \in \{0, 1\}^n},
$$
где $m = \sum_{i=1}^n x_i$.
Уравнение ОМП
$$
\frac{\partial L(p; x)}{\partial p} = (m (1-p) + (n-m) p) p^{m-1} (1-p)^{n-m-1} 
I_{x \in \{0, 1\}^n} = 0,
$$
откуда $\hat{p} = m / n$. По уже доказанному частота является эффективной оценкой
параметра $p$. Но эта оценка не является минимаксной. 

Минимаксной оценкой для схемы Бернулли будет оценка Ходжеса-Лемана
$$
\widetilde{p} = \overline{x} + \frac{1/2 - \overline{x}}{1+\sqrt{n}}.
$$

\begin{proof}
Посчитаем риск оценки
\begin{multline*}
R(p, \hat{p}) = \Ef_p (p - \hat{p})^2 = p^2 - 2 p 
\Ef_p \left(\frac{1/2 + \sqrt{n} \overline{X}}{1 + \sqrt{n}}\right) +
\Ef_p \left(\frac{1/2 + \sqrt{n} \overline{X}}{1 + \sqrt{n}}\right)^2 = \\
p^2 - \frac{p + 2 \sqrt{n} p^2}{1 + \sqrt{n}} + 
\frac{1/4 + \sqrt{n} p + p (1-p) + n p^2}{(1 + \sqrt{n})^2} = 
\frac{1}{4 (1 + \sqrt{n})^2}. 
\end{multline*}
Как мы видим, риск не зависит от параметра $p$.
Для квадратичной функции потери мы знаем вид байесовской оценки.
Подберём априорное распределение $\pi$ так, чтобы оценка $\widetilde{p}$
стала байесовской.
$$
\frac{m + \sqrt{n}/2}{n + \sqrt{n}} = \widetilde{p} =
\frac{\int_0^1 p^{1+m} (1-p)^{n-m} \pi(p) d p}{
\int_0^1 p^{m} (1-p)^{n-m} \pi(p) d p},
$$
где $m = \sum_{i=1}^n x_i$. Как можно заметить, стоит подбирать 
априорную плотность $\pi$ как плотность случайной величины, имеющей распределение
$\text{Beta}(\alpha, \beta)$. Тогда последнее равенство преобразуется к
$$
\frac{m + \sqrt{n}/2}{n + \sqrt{n}} =
\frac{B(1+m+\alpha, n-m+\beta)}{B(m+\alpha, n-m+\beta)} = 
\frac{\Gamma(1+m+\alpha) \Gamma(n-m+\beta) \Gamma(n+\alpha+\beta)}{
\Gamma(1+n+\alpha+\beta) \Gamma(m+\alpha) \Gamma(n-m+\beta)} =
\frac{m+\alpha}{n+\alpha+\beta}.
$$
Тогда для $\alpha = \beta = \sqrt{n}/2$ оценка Ходжеса-Лемана является
байесовской. Так как риск постоянной, то оценка является минимаксной.
\end{proof}

\end{ex}


\subsection{Байесовские интервальные оценки}
Предположим, что распределения $\Pf_\theta$ и $\Pi$ абсолютно непрерывны относительно лебеговой меры,
т.е. существуют плотности $p_\theta(x)$ и $\pi(\theta)$. $q(\theta\mid x)$ --- апостериорная плотность.
Возьмём произвольное $\alpha \in (0,1)$ и произвольным образом разобьём его на две части:
$\alpha = \alpha_1 + \alpha_2$, $\alpha_{1,2} \in (0,1)$. Найдём $\alpha_1$- и $(1-\alpha_2)$-квантили
апостериорного распределения и обозначим их $\utheta = \utheta(\alpha_1, x)$ и $\otheta =
\otheta(\alpha_2, x)$ соответственно:
$$
\int\limits_{-\infty}^{\utheta} q(\theta \mid x) \, d\theta = \alpha_1, \qquad
\int\limits_{-\infty}^{\otheta} q(\theta \mid x) \, d\theta = 1 - \alpha_2.
$$
$$
\Pf(\utheta \leqslant \btheta \leqslant \otheta) =
\int\limits_{\utheta}^{\otheta} q(\theta\mid x) \, d\theta = 1 - \alpha_2 - \alpha_1 = 1 - \alpha,
$$
где вероятность рассматривается в смысле меры $Q$ (и по априорному распределению $\btheta$,
и по распределениям из семейства $\Ps$).
Получили интервал $[\utheta, \otheta]$ для параметра --- это и есть
\emph{байесовская интервальная оценка} (этот интервал определяется по $\alpha$ неоднозначно).
$\alpha$ --- вероятность того, что $\btheta$ не попадёт в интервал (вероятность ошибки при
использовании этого интервала).

\section{Многомерное нормальное распределение}
\subsection{Эквивалентные определения} \label{task22}
Пусть $\xi = (\xi_1, \dots, \xi_n)^T$~--- случайный
вектор\footnote{Здесь и далее буква $T$ обозначает
транспонирование. Векторы-столбцы записаны как транспонированные
строки для экономии места. --- \emph{примеч. С. К.}}, $a = (a_1,
\dots, a_n)^T$ --- некоторый постоянный вектор, $x = (x_1, \dots,
x_n)^T \in {\mathbb R}^n$, $A$~--- положительно определённая
матрица $n \times n$.

{\bf Первое определение.} Если плотность (совместного) распределения
вектора $\xi$ имеет вид
$$
p_\xi(x, a, A) = \frac{1}{\left(\sqrt{2\pi}\right)^n} (\det A)^{\frac 12}
\exp \left( - \frac 12 (x-a)^T A (x-a) \right),
$$
то говорят, что $\xi$ имеет \emph{многомерное} ($n$-мерное)
\emph{нормальное распределение}. Вектор $a = \Mf \xi$ ($a_i = \Mf
\xi_i$) называется средним значением, а матрица $\Sigma =
\Sigma_\xi = A^{-1} = \Mf(\xi - a)(\xi - a)^T$~---
\emph{ковариационной матрицей}. $\Sigma = (\sigma_{ij})$,
$\sigma_{ij} = \Mf(\xi_i - a_i)(\xi_j - a_j)$. Ковариационная
матрица является квадратной, симметричной и положительно
определённой. Распределение, заданной такой плотностью,
обозначается $\Norm(a, \Sigma)$.

{\bf Второе определение.} Определим многомерное нормальное
распределение через характеристические функции. $t = (t_1, \dots,
t_n)^T \in {\mathbb R}^n$. $\ph_\xi(t) = \Mf e^{it^T\xi}$~---
\emph{характеристическая функция} случайного вектора. Будем
считать, что $\xi \sim \Norm(a, \Sigma)$, если
$$
\ph_\xi(t) = \exp \left( it^Ta - \frac 12 t^T\Sigma t \right).
$$
В отличие от предыдущего определения, сюда включается случай вырожденной
ковариационной матрицы.

\begin{note} Характеристическая функция вычисляется по плотности, а по
многомерной формуле обращения плотность однозначно восстанавливается по
характеристической функции. Поэтому первое и второе определения равносильны (по теореме единственности).
\end{note}

{\bf Третье определение.} Если любая линейная комбинация компонент вектора
$\xi$ имеет (одномерное) нормальное распределение, то $\xi$ имеет многомерное нормальное распределение.

Третье определение будут использоваться в теории случайных процессов.

\subsection{Свойства многомерного нормального распределения}
\begin{nums}{-2}
\item $\xi \sim \Norm(a, \Sigma)$, $\eta = A \xi$. Тогда
$\eta \sim \Norm(A a, A \Sigma A^T)$.

\begin{proof}
Найдём характеристическую функцию $\eta$:

$$
\phi_{\eta}(t) = \Mf e^{i t^T \eta} = 
\Mf e^{i t^T A \xi} =
\phi_{\xi}(A^T t) = \exp\{i (A^T t)^T a - ((A^T t)^T \Sigma (A^T t)\} =
\exp\{i t^T (A a) - t^T (A \Sigma A^T) t\}.
$$

В силу теоремы единственности получаем искомое.
\end{proof}

\item $\xi_1, \ldots, \xi_n$ --- нормальные, 
$\xi := (\xi_1, \ldots, \xi_n)$. Тогда
$\xi_1, \ldots, \xi_n$ --- независимые тогда и только тогда, когда
$\Sigma$ --- диагональная.

\begin{proof}
Слева направо очевидно так как из независимости следует некоррелированность.
Справа налево пишем явно характеристическую функцию, 
видим, что переменные разделяются в сумму квадратов, 
по теореме единственности получаем, что это многомерное нормальное
с диагональной матрицей.
\end{proof}


\item $\xi \sim \Norm(a, \Sigma)$. Положим 
$Q := (\xi-a)^T \Sigma^{-1} (\xi-a)$. Тогда $Q \sim \chi_n^2$, где 
$\chi_n^2 \sim Z_1^2 + \ldots + Z_n^2$, 
$Z_i^2 \sim \Norm(0, 1)$ --- независимы.

\begin{proof}
Так как $\Sigma$ --- положительная симметричная матрица, то существует
такая матрица $C$, что $\Sigma = C^T C$. Тогда
$$
Q := (\xi-a)^T \Sigma^{-1} (\xi-a) = 
(C (\xi-a))^T (C (\xi-a)) = \eta^T \eta
$$
Заметим, что 
$$
\eta \sim \Norm(C (a-a), C^T \Sigma C) = \Norm(0, C^T (C^{-1})^T C^{-1} C)
= \Norm(0, E).
$$
Поэтому $\eta$ имеет стандартное многомерное нормальное распределение,
а $Q$ --- распределение хи-квадрат.
\end{proof}
\end{nums}

\subsection{Лемма Фишера} \label{task23}
\begin{lemma}[Фишер]
Пусть $\eta_1, \dots, \eta_n$ --- независимые (одномерные) случайные
величины с $\Norm(a, \sigma^2)$. Тогда:
\begin{nums}{-2}
\item существуют ортогональная матрица $C$ и случайный вектор $\xi$
такие, что $\xi = C\eta$, $\Mf \xi_1 = \sqrt n a$, $\Mf \xi_i = 0$
при $i>1$ и $\Df \xi_i = \sigma^2$ при всех $i$;
\item $\overline \eta = \dfrac{\eta_1 + \dots + \eta_n}{n}$ и
$\sum\limits_{i=1}^{n} (\eta_i - \overline\eta)^2$ независимы;
\end{nums}
\end{lemma}

\begin{proof}
\begin{nums}{-2}
\item Положим $c_{11} = c_{12} = \dots = c_{1n} := \dfrac{1}{\sqrt n}$.
$\sum\limits_{j=1}^n c_{1j}^2 = 1$. $\xi_1 = \sum\limits_{j=1}^n c_{1j}
\eta_j = \sqrt n \overline\eta$, $\Mf \xi_1 = \sqrt n a$, $\Df \xi_1 =
n \frac{\sigma^2}{n} = \sigma^2$. Остальные $c_{ij}$ ($2 \leqslant i
\leqslant n$, $1 \leqslant j \leqslant n$) подбираются из условий:
$\sum\limits_{j=1}^n c_{ij}^2 = 1$ (при всех $i > 1$) и
$\frac{1}{\sqrt n} \sum\limits_{j=1}^{n} c_{ij} = 0$. Из этих условий
получаем, что матрица $C = (c_{ij})$ ортогональна, и при $i>1$
$\Mf \xi_i = 0$ и $\Df \xi_i = \sigma^2$.

\item В силу ортогональности $C$ имеем 
$$
\sum\limits_{i=1}^n \xi_i^2 =
\xi^T \xi = \eta^T C^T C \eta = \eta^T \eta = \sum\limits_{i=1}^n \eta_i^2.
$$
Более того, так как $C$ -- ортогональная матрица, то
$$\cov \eta = C \cov \xi C^T =
C \sigma^2 E C^T = \sigma^2 E,
$$
поэтому $\xi_1, \ldots, \xi_n$ -- независимы.
Отсюда 
$$\sum\limits_{i=2}^n \xi_i^2 =
\sum\limits_{i=1}^n \eta_i^2 - \xi_1^2 = 
\sum\limits_{i=1}^n \eta_i^2 -
n {\overline\eta}^2 = 
\sum\limits_{i=1}^n (\eta_i - \overline\eta)^2,
$$
то есть $\sum\limits_{i=1}^n
(\eta_i - \overline\eta)^2 = \sum\limits_{i=2}^n \xi_i^2$ --- не зависит
от $\overline\eta = \xi_1 / \sqrt n$.
\end{nums}
\end{proof}

\begin{note} Если $\eta_1, \dots, \eta_n$ независимы и имеют одинаковое
нормальное распределение, то $\overline\eta$ и $\frac 1n \sum (\eta_i -
\overline\eta)^2$ независимы. На самом деле это характеристическое
свойство нормальных распределений.
\end{note}

\subsection{Связанные с нормальным распределения} \label{task24}
\subsubsection{Хи-квадрат распределение}
Пусть случайные величины $Z_1, \dots, Z_n$ взаимно независимы и
имеют стандартное нормальное распределение. Тогда распределение
случайной величины $Z_1^2 + \dots + Z_n^2$ называется \emph{хи-квадрат}
распределением с $n$ степенями свободы и обозначается $\chi^2_n$.
При $\chi^2_2$ имеет показательное распределение с параметром
$\lambda = \frac 12$. Плотность хи-квадрат распределения даётся
формулой:
$$
p_{\chi^2_n}(x) = \frac{1}{2^{\frac n2} 
\Gamma(\frac n2)} x^{\frac n2 - 1}
e^{- \frac x2}, \qquad
\text{где } \Gamma(\lambda) = \int\limits_0^\infty x^{\lambda - 1}
e^{-x} \, dx \text{ --- гамма-функция Эйлера.}
$$
$\Mf \chi^2_n = n$, $\Df \chi^2_n = 2n$. Характеристическая
функция: $\ph_{\chi^2_n} = (1-2it)^{- \frac n2}$.
\begin{proof}
Выведем формулу плотности. Очевидно, что 
$$
p_{\chi_1^2}(x) = \frac{1}{\sqrt{2 \pi x}} e^{-\frac{x}{2}}.
$$
Для характеристической функции верно 
$\ph_{\chi_1^2}(t) = (1 - 2 i t)^{-1/2}$.
Тогда пользуясь независимостью слагаемых для хи-квадрат случайной величины,
получаем
$$
\ph_{\chi_n^2}(t) = \frac{1}{(1 - 2 i t)^{n/2}}.
$$
Заметим, что это есть характеристическая функция гамма распределения
с параметрами $n/2, 2$. В силу теоремы единственности хи-квадрат распределена
как $\Gamma(n/2, 2)$.
\end{proof}

% FIXME: Написать вывод плотностей для Фишера

\subsubsection{Распределение Стьюдента}
Если $Z_0, Z_1, \dots, Z_n$ взаимно независимы и имеют стандартное нормальное
распределение, то распределение случайной величины
$$
\frac{Z_0}{\sqrt{\dfrac{Z_1^2 + \dots + Z_n^2}{n}}} =
\frac{Z_0}{\sqrt{\chi^2_n / n}}
$$
называется \emph{распределением Стьюдента} ($t$-распределением) и
обозначается $t_n$. Плотность распределения Стьюдента
даётся формулой
$$
p_{t_n}(x) = \frac{C_n}{\left( 1 + \frac{x^2}{n} \right)^{\frac{n+1}{2}}},
\qquad \text{где } C_n = \frac{\Gamma(\frac{n+1}{2})}{\Gamma(\frac n2)
\sqrt{\pi n}}.
$$
При $n=1$ распределение Стьюдента совпадает с распределением Коши,
имеющим плотность $\dfrac{1}{\pi (1+x^2)}$.
\begin{proof}
Рассмотрим случайную величину ${\xi = (Z_0, \chi_n^2)^T}$, 
где $Z_0$ имеет стандартное нормальное распределение,
$\chi_n^2$ имеет хи-квадрат распределение
с~$n$ степенями свободы, $Z_0$ и $\chi_n^2$ -- независимы.
Пусть
$$
\eta 
= \left(
\begin{matrix}
\frac{Z_0}{\sqrt{\chi_n^2 / n}} \\
\chi_n^2
\end{matrix}
\right)
= g\left(
\begin{matrix}
Z_0 \\
\chi_n^2
\end{matrix}
\right)
= g(\xi).
$$
Плотность случайного вектора~$\xi$ нам известна.
Найдём плотность вектора~$\eta$ по~формуле
замены плотности. 
Заметим, что
$$
J_g(z, x)
= \left(
\begin{matrix}
\sqrt{n / x} & * \\
0            & 1
\end{matrix}
\right),
$$
откуда при $z = t \sqrt{x / n}$
$$
f_{\eta}(t, x)
= \frac{f_{\xi}(z, x)}{|J_g(z, x)|}
= \sqrt{\frac{x}{n}} f_{Z_0}(t \sqrt{x / n}) f_{\chi_n^2}(x)
= \sqrt{\frac{x}{n}} \frac{1}{\sqrt{2 \pi}} e^{-x t^2 / (2 n)} 
\frac{1}{2^{n/2} \Gamma(n/2)} x^{n/2-1} e^{-x/2}.
$$
Положим $D_n := (2^{n/2} \Gamma(n/2) \sqrt{2 \pi n})^{-1}$.
Первая координата вектора $\eta$ обладает 
искомым распределением Стьюдента с $n$ степенями свободы,
и её плотность находится как
\begin{multline*}
f_{t_n}(t) = \int_0^{\infty} f_{\eta}(t, x) dx
= D_n \int_0^{\infty} x^{(n-1) / 2} e^{-(t^2/n+1) x / 2} dx
= D_n \frac{2^{(n+1) / 2}}{(1 + t^2/n)^{(n+1)/2}} 
\int_0^{\infty} y^{(n-1)/2} e^{-y} dy = \\
= \frac{\Gamma((n+1)/2) 2^{(n+1)/2}}{2^{n/2} \Gamma(n/2) \sqrt{2 \pi n}}
\frac{1}{(1 + t^2/n)^{(n+1)/2}}
= \frac{\Gamma((n+1)/2)}{\Gamma(n/2) \sqrt{\pi n}} \frac{1}{(1 + t^2/n)^{(n+1)/2}}.
\end{multline*}
\end{proof}

\subsubsection{Распределение Фишера}

Распределение определяется как
$$
F_{n, m} := \frac{\chi_n^2 / n}{\chi_m^2 / m},
$$
где $\chi_n^2, \chi_m^2$ --- хи-квадрат распределённые случайные величины.
Говорят, что распределение Фишера имеет $n$ и $m$ степени свободы.
Плотность:

$$
p_{n,m}(x) = \begin{cases}
\frac{1}{B(n/2,m/2)}(\frac{n}{m})^{n/2} x^{n/2-1} (1 + \frac{n}{m}x)^{-\frac{m+n}{2}},& x \ge 0\\
0,& x \leq 0
\end{cases}
$$

Функция распределения:
$$
P(F_{m,n} \leq x) = \frac{I_{\frac{m}{n}x}(m/2,n/2)}{1 + \frac{m}{n}x}, \quad
\text{где $I_x(a,b)$ - неполная $\beta$-функция с параметрами $a$ и $b$.}
$$
\begin{proof}
Рассмотрим случайную величину ${\xi = (\chi_n^2, \chi_m^2)^T}$, 
где $\chi_n^2$ имеет хи-квадрат распределение
с~$n$ степенями свободы,
$\chi_m^2$ имеет хи-квадрат распределение
с~$m$ степенями свободы, $\chi_n^2$ и $\chi_m^2$ -- независимы.
Пусть
$$
\eta 
= \left(
\begin{matrix}
\frac{\chi_n^2 / n}{\chi_m^2 / m} \\
\chi_m^2
\end{matrix}
\right)
= g\left(
\begin{matrix}
\chi_n^2 \\
\chi_m^2
\end{matrix}
\right)
= g(\xi).
$$
Плотность случайного вектора~$\xi$ нам известна.
Найдём плотность вектора~$\eta$ по~формуле
замены плотности. 
Заметим, что
$$
J_g(x_1, x_2)
= \left(
\begin{matrix}
m / (n x_2) & * \\
0            & 1
\end{matrix}
\right),
$$
откуда при $x_1 = n x_2 y / m > 0$, $y > 0$
\begin{multline*}
f_{\eta}(y, x_2)
= \frac{f_{\xi}(x_1, x_2)}{|J_g(x_1, x_2)|}
= \frac{n x_2}{m} f_{\chi_n^2}(n x_2 y / m) f_{\chi_m^2}(x_2) = \\
= \frac{n x_2}{m}
\frac{1}{2^{n/2} \Gamma(n/2)} (n x_2 y / m)^{n/2-1} e^{-n x_2 y / (2m)} 
\frac{1}{2^{m/2} \Gamma(m/2)} x_2^{m/2-1} e^{-x_2/2} = \\
= \frac{n^{n/2}}{m^{n/2} 2^{(n+m)/2} \Gamma(n/2) \Gamma(m/2)}
y^{n/2-1} x_2^{(n+m)/2-1} e^{-(ny/m+1) x_2/2}.
\end{multline*}
Положим ${D_n := n^{n/2} / (m^{n/2} 2^{(n+m)/2} \Gamma(n/2) \Gamma(m/2))}$.
Первая координата вектора $\eta$ обладает 
искомым распределением Фишера с $n$ и $m$ степенями свободы,
и её плотность находится как
\begin{multline*}
f_{F_{n, m}}(y) = \int_0^{\infty} f_{\eta}(y, x_2) dx_2
= D_n y^{n/2-1} \int_0^{\infty} x_2^{(n+m)/2-1} e^{-(ny/m+1) x_2/2} dx_2 = \\
= D_n y^{n/2-1} \frac{2^{(n+m)/2}}{(1+ny/m)^{(n+m)/2}}
\int_0^{\infty} z^{(n+m)/2 - 1} e^{-z} dz
= \frac{1}{B(n/2, m/2)} \left(\frac{n}{m}\right)^{n/2} y^{n/2-1} \frac{1}{(1+ny/m)^{(n+m)/2}}.
\end{multline*}
\end{proof}

\subsection{Статистики для нормальных выборок}
\subsubsection{Распределение статистик для нормальных выборок}
\begin{stm}
Пусть $\eta_1, \dots, \eta_n$ --- независимые (одномерные) случайные
величины с $\Norm(a, \sigma^2)$. Тогда:
$$
\frac{1}{\sigma^2} \sum_{i=1}^n (\eta_i - \overline\eta)^2 \sim \chi^2_{n-1},
\qquad
\frac{\sqrt n (\overline\eta - a)}{s} \sim t_{n-1},
\quad
\text{где } s^2 = \frac{1}{n-1} \sum_{i=1}^n (\eta_i - \overline\eta)^2.
$$
\end{stm}

\begin{proof} \label{task25}
$Z_i = \dfrac{\xi_i}{\sigma} \sim \Norm(0,1)$ при $i>1$, откуда
$$
\frac 1 {\sigma^2} \sum_{i=1}^n (\eta_i - \overline\eta)^2 =
\sum_{i=2}^n \frac{\xi_i^2}{\sigma^2} = Z_2^2 + \dots + Z_n^2
\sim \chi^2_{n-1},
\quad
s^2 = \frac{1}{n-1} \sum_{i=1}^n (\eta_i - \overline\eta)^2,
$$
откуда $\dfrac{(n-1)s^2}{\sigma^2} \sim \chi^2_{n-1}$.

$\dfrac{\overline\eta - a}{\sigma/{\sqrt n}} \sim \Norm(0,1)$,
$\dfrac{(n-1)s^2}{\sigma^2} \sim \chi^2_{n-1}$ и эти случайные
величины независимы по лемме Фишера, поэтому
$$
\frac{\sqrt n (\overline\eta - a)}{s} =
\frac{\frac{\overline\eta - a}{\sqrt{\sigma^2/n}}}{\sqrt{s^2/\sigma^2}}
\sim t_{n-1}.
$$
\end{proof}
$x_1, \dots, x_n$ --- повторная выборка из $\Norm(a, \sigma^2)$.

\subsubsection{Оценка дисперсии при неизвестном среднем}
$$
\frac{(n-1)s^2}{\sigma^2} = \frac{\sum_{i=1}^n (x_i - \overline{x})}{\sigma^2}
$$

Пусть $\eta_i$ как в доказательстве леммы Фишера, тогда
$$
\frac{\sum_{i=1}^n (\xi_i - \overline{\xi})}{
\sigma^2} = \frac{\sum_{i=2}^n \eta_i}{\sigma^2} \sim \chi_{n-1}^2.
$$

\subsubsection{Оценка среднего при неизвестной дисперсии}
$\Mf \overline\xi = a$, $\Df \overline\xi = \sigma^2/n$.
$$
\frac{\overline \xi - a}{\sqrt{s^2/n}} \sim 
\frac{\sqrt{n} (\overline \xi - a)}{\sigma \sqrt{\chi_{n-1}^2 / (n-1)}} \sim
t_{n-1}.
$$

\section{Доверительные интервалы} \label{task26}
$(\Xs, \As, \Ps = \{\Pf_\theta \mid \theta\in\Theta\})$ --- параметрическая
модель, $\theta$ --- скалярный параметр (иначе интервалы заменяются
более сложными областями). $(\xi_1, \dots, \xi_n)$ --- случайная выборка,
$x = (x_1, \dots, x_n)$ --- её реализация. По заданному $\alpha \in (0,1)$ и
по выборке находим такие статистики $\utheta(\alpha; x) <
\otheta(\alpha; x)$, что $\Pf_\theta([\utheta, \otheta] \ni
\theta) \geqslant 1 - \alpha$. В этом случае интервал $[\utheta,
\otheta]$ называется (точным) \emph{доверительным интервалом} для $\theta$
с \emph{доверительной вероятностью} (\emph{доверительным уровнем})
$1 - \alpha$. $\alpha$ --- вероятность ошибки при использовании
данного интервала. $\utheta$ и $\otheta$ называются \emph{доверительными
границами}.

Доверительный интервал --- частный случай интервальной оценки. В
отличие от байесовского подхода здесь $\theta$\т не случайная
величина; зато у Байеса границы постоянные, а у нас\т
случайные (зависят от выборки).

\emph{Сравнение} двух доверительных интервалов производится по
двум характеристикам:
\begin{nums}{-2}
\item по $\alpha$ (или по доверительной вероятности $1 - \alpha$);
\item по средней длине доверительного интервала.
\end{nums}

\subsection{Общие способы получения доверительных интервалов}
\subsubsection{Метод центральной статистики}
Пусть $\xi_1, \dots, \xi_n$ взаимно независимы и распределены с
абсолютно непрерывным распределением вероятностей $\Pf_\theta$.
Найдём функцию $S(\xi;\theta)$~--- так называемую
\emph{центральную статистику}, хотя на самом деле она статистикой
не является,~--- со следующими свойствами:
\begin{nums}{-1}
\item распределение $S(\xi;\theta)$ не зависит от $\theta$ и
имеет плотность $f_S(y)$,
\item при каждом фиксированном $x\in \Xs$ функция $S(x;\theta)$
(как функция аргумента $\theta$) является непрерывной и строго монотонной.
\end{nums}
Зададим $\alpha\in(0,1)$ и найдём $u_1 < u_2$ так, чтобы
$$
\Pf(u_1 \leqslant S(\xi; \theta) \leqslant u_2) =
\int\limits_{u_1}^{u_2} f_S(y) \, dy = 1-\alpha.
$$
($u_1$ и $u_2$ находятся неоднозначно.) Заметим, что распределение
вероятностей в правой части не зависит от параметра $\theta$.
Обратим неравенство относительно $\theta$ и получим, что
$\Pf(T_1(\xi) \leqslant \theta \leqslant T_2(\xi)) =  1 - \alpha$,
\те $[T_1(x), T_2(x)]$ есть доверительный интервал для $\theta$ с
доверительной вероятностью $1-\alpha$.

\subsubsection{Метод монотонного преобразования}
Пусть $T(x)$ --- некоторая статистика, $G_\theta(t) =
\Pf_\theta(T(\xi) \leqslant t)$~--- её функция распределения.
Пусть $G_\theta(t)$ при любом фиксированном $\theta$ непрерывна и
строго монотонна относительно $t$ и при любом фиксированном $t$
является непрерывной строго убывающей функцией от $\theta$. Тогда
$\Pf_\theta(G_\theta(t_1) \leqslant G_\theta(T(\xi)) \leqslant
G_\theta(t_2)) = \Pf_\theta(t_1 \leqslant T(\xi) \leqslant t_2) =
G_\theta(t_2) - G_\theta(t_1)$ при любых $t_1 < t_2$. Если
$G_\theta(t_1) = \alpha_1$, $G_\theta(t_2) = 1 - \alpha_2$,
$\alpha_1 + \alpha_2 = \alpha$, то $\Pf_\theta(\alpha_1 \leqslant
G_\theta(T) \leqslant 1 - \alpha_2) = 1 - \alpha_2 - \alpha_1 = 1
- \alpha$. Положим $\otheta$: $G_\otheta(T(x)) = \alpha_1$;
$\utheta$: $G_\utheta(T(x)) = \alpha_2$. Тогда $\Pf(\utheta
\leqslant \theta \leqslant \otheta) = 1-\alpha$, \те
$[\utheta,\otheta]$~--- доверительный интервал с вероятностью
ошибки $\alpha$.

\subsection{Точный доверительный интервал для параметра биномиального
распределения} \label{task27}
Пусть дана случайная выборка $(\xi_1, \dots, \xi_n)$ со значениями
$x_1, \dots, x_n$, $\xi_k$ взаимно независимы и распределены
следующим образом:
$$
\xi_k = \begin{cases}
0 & \text{с вероятностью $1-p$},\\
1 & \text{с вероятностью $p$},
\end{cases}
$$
где $p$ --- неизвестный параметр.
$S_n = \sum\limits_{i=1}^{n} \xi_i$, $\Pf(S_n = m) = C^m_n p^m (1-p)^{n-m}$.

Положим
$$f_m(p):= \sum_{k=0}^{m} C_n^k p^k (1-p)^{n-k} = 
1 - \sum_{k=m+1}^{n} C^k_n p^k (1-p)^{n-k}.
$$

Вычитаемое нам знакомо: \hyperref[dist_stat]{раньше} мы нашли,
что для $\eta_1, \dots, \eta_n$ -- взаимно
независимых и равномерно распределенных на $[0,1]$ верно
$$
\Pf(\eta_{(m)} \leqslant x) = \sum_{k\geqslant m} C_n^k x^k (1-x)^{n-k}.
$$

При фиксированном $n > m = S_n$ это функция $f_m(p)$ от аргумента $p \in [0,1]$
является непрерывной и строго убывающей функцией 
(так как вычитаемое из $1$ это $\Pf(\eta_{(m+1)} \leqslant p)$,
которое очевидно строго возрастает по $p$), причём $f_m(0) = 1$, $f_m(1) = 0$. 
Поэтому для каждого
$\beta \in (0,1)$ уравнение $f_m(p) = \beta$ имеет единственный
корень $p = \olp = \olp_m(\beta)$. $\sum\limits_{k=m}^{n} C^k_n
p^k (1-p)^{n-k} = 1 - f_{m-1}(p)$. Для каждого $\beta \in (0,1)$
уравнение $1 - f_{m-1}(p) = \beta$ имеет единственный корень $p =
\ulp = \ulp_{m-1}(\beta)$.

Полагаем $\beta = \alpha/2$ и получаем, что
\begin{multline*}
\Pf_p (p \geqslant \olp) = \Pf_p(f_m(p) \leqslant \beta) = 
\Pf_p \left(\sum_{k=0}^{S_n} C_n^k p^k (1-p)^{n-k} \leqslant \beta \right) = 
\Pf_p \left(\sum_{k=0}^{S_n} \Pf_p(\widetilde{S}_n = k) \leqslant \beta 
\right) = \\
\sum_{j=0}^n \Pf_p\left(\sum_{k=0}^{S_n} \Pf_p(\widetilde{S}_n = k)
\leqslant \beta, \Pf_p(S_n = j)\right) = 
\sum_{j \, : \, \sum\limits_{k\leqslant j} \Pf_p(S_n = k) \leqslant \beta}
\Pf(S_n = j) \leqslant \beta, 
\end{multline*}
то есть $\Pf(p > \olp) \leqslant \beta$. Аналогично
$\Pf(p < \ulp) \leqslant \beta$. Окончательно,
$$
\Pf_p ( \ulp \leqslant p \leqslant \olp ) \geqslant 1 - 2\beta =
1 - \alpha,
$$
то есть интервал $[\ulp,\olp]$ есть доверительный интервал для $p$
с доверительной вероятностью $1 - \alpha$.

\subsection{Доверительные интервалы для параметров нормального распределения}
\label{task28}
\subsubsection{Доверительный интервал для среднего при известной дисперсии}
$\xi_1, \dots, \xi_n$ --- повторная выборка,
$\xi \sim \Norm(a, \sigma^2)$, $a = \Mf \xi_i$, $\sigma^2 = \Df \xi_i$;
$\sigma^2$ известно. $\overline{\xi}$ --- лучшая точечная оценка для $a$.


$\overline{\xi} \sim \Norm(a, \dfrac{\sigma^2}{n})$. Отсюда
$
\dfrac{\overline{\xi} - a}{\sqrt{{\sigma^2}/{n}}} \sim
\Norm(0,1)
$ --- распределение не зависит от неизвестного параметра
(заметим, что это не статистика, т.к. она зависит от параметра $a$).
Поэтому
$$
\Pf_a \left( \left| \frac{\overline\xi - a}{\sqrt{{\sigma^2}/n}} \right|
\leqslant u \right) = \Phi(u) - \Phi(-u) = 2\Phi(u) - 1,
\qquad
\text{где }
\Phi(x) = \int\limits_{-\infty}^x \frac{1}{\sqrt{2\pi}} e^{-{u^2}/2} \, du
\text{ --- ф. р. $\Norm(0,1)$.}
$$

Пусть задано $\alpha \in (0,1)$. По таблицам стандартного нормального
распределения находим $(1 - \frac{\alpha}{2})$-квантиль $\Norm(0,1)$.
Обозначим эту квантиль $u_{1 - \frac{\alpha}{2}}$. Тогда
$\Phi(u_{1-\frac\alpha 2}) = 1 - \frac\alpha 2$, откуда
$2\Phi(u_{1-\frac\alpha 2}) - 1 = 1 - \alpha$. В силу предыдущего
$$
\Pf_a \left( \left| \frac{\overline\xi - a}{\sigma / \sqrt{n}} \right|
\leqslant u_{1 - \frac\alpha 2} \right) = 1 - \alpha,
$$
поэтому $\left[\overline x - u_{1 - \frac\alpha 2} \dfrac{\sigma}{\sqrt n},
\overline x + u_{1 - \frac\alpha 2} \dfrac{\sigma}{\sqrt n}\right]$ ---
доверительный интервал для $a$ с доверительной вероятностью $1-\alpha$.


% FIXME: про распределение Стьюдента (интервал для a при неизв. \sigma^2)

\subsubsection{Доверительный интервал для дисперсии при известном среднем}
$s_0^2 = \dfrac 1n \sum\limits_{k=1}^{n} (x_k - a)^2$ --- несмещённая
($\Mf s_0^2 = \sigma^2$) оценка для $\sigma^2$.
$$
Z_k = \frac{\xi_k - a}{\sigma} \sim \Norm(0,1),
\qquad
\frac{n s_0^2}{\sigma^2} = \sum_{k=1}^{n} \left(
\frac{\xi_k - a}{\sigma} \right)^2
= Z_1^2 + \dots + Z_n^2.
$$
$Z_k$ взаимно независимы и одинаково распределены с $\Norm(0,1)$.
Распределение случайной величины $Z_1^2 + \dots + Z_n^2$ называется
\emph{хи-квадрат} распределением с $n$ степенями свободы и обозначается
$\chi^2_n$. Для квантилей этого распределения составлены таблицы.
Используя эти таблицы, находим квантили $g_1 = g_1(\frac\alpha 2)$ ---
$\frac \alpha 2$-квантиль и $g_2 = g_2(\frac\alpha 2)$ ---
$(1 - \frac\alpha 2)$-квантиль $\chi^2_n$. Получаем, что
$$
\Pf_{\sigma^2} \left( g_1 \leqslant \frac{n s_0^2}{\sigma^2} \leqslant
g_2 \right) = \left( 1 - \frac \alpha 2 \right) - \frac \alpha 2 =
1 - \alpha,
$$
то есть $\left[ \dfrac{n s_0^2}{g_2}, \dfrac{n s_0^2}{g_1} \right]$
является доверительным интервалом для $\sigma^2$ с доверительной
вероятностью $1-\alpha$.

\subsection{Асимптотические доверительные интервалы} \label{task29}
Как правило, они основаны на асимптотической нормальности некоторых
статистик.

\begin{ex} В схеме Бернулли с параметром $p\in[0,1]$ рассматривается
оценка 
$\widehat{p}_n = \frac 1n (x_1 + \dots + x_n)$. $\Mf \widehat{p}_n
= p, \Df \widehat{p}_n = \dfrac{p(1-p)}{n}$. В силу ЦПТ
$\widehat{p}_n$ асимптотически нормальна с $\Norm\left(p,
\dfrac{p(1-p)}{n}\right)$, \те
$$
\frac{\widehat{p}_n - p}{\sqrt{\dfrac{p(1-p)}{n}}}
\xrightarrow[\nrai]{\mathrm d} Z \sim \Norm(0,1).
$$

Положим $u = u_{1-\frac\alpha 2}$~--- $(1-\frac\alpha 2)$-квантиль
стандартного нормального распределения. Получаем, что
$$
\Pf \left( \left| \frac{\widehat{p}_n - p}{\sqrt{\frac{p(1-p)}{n}}}
\right| \leqslant u \right) \approx \Phi(u) - \Phi(-u) = 1 - \alpha,
$$
где $\Phi$~--- функция стандартного нормального распределения.
Точность вычисляется из оценок скорости сходимости в ЦПТ. Решая
неравенство
$$
n (\widehat{p}_n - p)^2 \leqslant p(1-p) u^2,
$$
получаем интервал для $p$:
$\Pf(\ulp_n \leqslant p \leqslant \olp_n) \approx 1 - \alpha$
($\ulp_n$ и $\olp_n$ --- оценки).

\end{ex}

\begin{lemma} Пусть оценка $\hattheta_n(x)$ параметра $\theta$ асимптотически
нормальна с $\Norm ( \theta; \sigma^2(\theta) / n )$. Пусть
$f(\theta)$ дифференцируема и $f'(\theta) \ne 0$ в рассматриваемой
области. Тогда $f(\hattheta_n)$ асимптотически нормальна с распределением
$$\Norm \left(f(\theta); \dfrac{(f'(\theta))^2 \sigma^2(\theta)}{n}\right).$$
\end{lemma}

\begin{note}
Функцию $f(\theta)$ можно выбрать так, чтобы $f'(\theta) \sigma(\theta) =
\const$, и таким образом избавится от зависимости дисперсии от
параметра.
\end{note}

\begin{proof}
Разложим $f$ по формуле Тейлора: $f(t) = f(\theta) + (t-\theta)
(f'(\theta) + \gamma(t; \theta))$, где $\gamma(t; \theta) \to 0$
при $t \to 0$. Подставим $t = \hattheta_n$:
$$
f(\hattheta_n) - f(\theta) = (\hattheta_n - \theta) f'(\theta)
\left( 1 + \frac{\gamma(\hattheta_n; \theta)}{f'(\theta)} \right).
$$
Положим
$$
Y_n:= \frac{\hattheta_n - \theta}{\sigma(\theta) / {\sqrt n}},
\qquad
Z_n:= \frac{\gamma(\hattheta_n; \theta)}{f'(\theta)},
\qquad
W_n:= \frac{f(\hattheta_n) - f(\theta)}{f'(\theta) \sigma(\theta) /
{\sqrt n}}
$$
и получим, что $W_n = Y_n (1 + Z_n)$. Но из асимптотической
нормальности $\hattheta_n$ $Y_n \xrightarrow{\mathrm d} Y \sim
\Norm(0,1)$, а $\gamma(\hattheta_n; \theta) \xrightarrow{\Pf} 0$
в силу состоятельности, поэтому $W_n \xrightarrow{\mathrm d} Y \sim
\Norm(0,1)$, что и даёт асимптотическую нормальность $f(\hattheta_n)$.
\end{proof}

\begin{ex} \label{task30}
В схеме Бернулли оценка $\widehat{p}_n = \frac 1n (x_1 + \dots + x_n)$
асимптотически нормальна с $\Norm(p; p(1-p)/n)$. Положим
$f(p) = \arcsin \sqrt p$, $p \in (0,1)$. $f'(p) =
\dfrac{1}{2\sqrt{p(1-p)}}$, $f'(p) \sigma(p) =
\dfrac{\sqrt{p(1-p)}}{2\sqrt{p(1-p)}} = \dfrac 12 = \const$.
В силу предыдущей леммы оценка
$\arcsin \sqrt {\widehat{p}_n}$ асимптотически
нормальна с $\Norm(\arcsin \sqrt p, \frac 1 {4n})$. Пользуясь
монотонностью, получаем асимптотический доверительный
интервал для $p$.
\end{ex}

\begin{note}
Для любого фиксированного $x \in \mathbb{R}$ 
можно построить доверительный интервал 
для функции распределения $F(x)$,
используя эмпирическую функцию распределения 
$F_n(x; x_1, \ldots, x_n))$.
\end{note}

\section{Проверка статистических гипотез}
\subsection{Задача различения двух статистических гипотез} \label{task31}
$(\Xs, \As, \Ps)$ --- статистическая модель, $x = (x_1, \dots, x_n)
\in \Xs$ --- выборка. Пусть семейство распределений разбито на
два подсемейства: $\Ps = \Ps_1 \sqcup \Ps_2$. Хотим построить
критерий для выбора одной из двух гипотез: $H_0$: $\Pf \in \Ps_0$,
$H_1$: $\Pf \in \Ps_1$. Рассмотрим произвольное множество
$S \in \As$ и назовём его \emph{критическим множеством}
(\emph{критической областью}) критерия.
Сам критерий в терминах
$S$ формулируется так: если $x \in S$, то $H_0$ \emph{отклоняется}
($H_1$ принимается); если $x \notin S$, то $H_0$ принимается
($H_1$ отклоняется). Критерий с критическим множеством $S$
называется $S$-критерием.

\emph{Ошибка первого рода} --- отклонить
$H_0$, когда она верна. \emph{Ошибка второго рода} --- принять
$H_0$, когда верна $H_1$. Нужно принять решение так, чтобы
вероятности ошибок были минимальными. (Пока это не вполне
понятно: как считать вероятность ошибки, если в $\Ps_0$ и
$\Ps_1$ много разных распределений?)

\begin{ex}
$\Ps = \{ \Pf_0, \Pf_1 \}$ --- два разных распределения,
$\Ps_i = \{ \Pf_i \}, i \in \{ 0, 1 \}$. Пусть заданы
малые числа $\alpha, \beta \in (0, 1)$. Потребуем
$\Pf_0(S) \leqslant \alpha$ и $\Pf_1(\bar S) \leqslant \beta$
(или, что то же самое, $\Pf_0(\bar S) \geqslant 1 - \alpha$ и
$\Pf_1(S) \geqslant 1 - \beta$), то есть числа $\alpha$
и $\beta$ ограничивают сверху ошибки первого и второго рода
соответственно. Нужно найти такое множество $S$, чтобы различить
распределения $\Pf_0$ и $\Pf_1$.
\end{ex}

Перейдём к параметрической модели: $\Ps = \{ \Pf_\theta \mid
\theta\in\Theta \}$, причём распределения попарно различны
($\theta' \ne \theta \, \Rightarrow \, \Pf_{\theta'} \ne \Pf_\theta$).
$\Theta = \Theta_0 \sqcup \Theta_1$; $H_0$: $\theta\in\Theta_0$,
$H_1$: $\theta\in\Theta_1$.

Пусть дан критерий, заданный критическим множеством $S$ ($S$-критерий).
Тогда функция $g_S(\theta) := \Pf_\theta(S)$ от переменной $\theta$ называется
\emph{функцией мощности} $S$-критерия. При $\theta\in\Theta_0$
$g_S(\theta) = \Pf_\theta(S)$ есть вероятность ошибки первого рода,
а для $\theta\in\Theta_1$ вероятность ошибки второго рода выражается
так: $\Pf_\theta(\bar S) = 1 - \Pf_\theta(S) = 1 - g_S(\theta)$.
$\sup\limits_{\theta\in\Theta_0} g_S(\theta)$ --- \emph{размер критерия}.
Мы требуем, чтобы $\sup\limits_{\theta\in\Theta_0} g_S(\theta) \leqslant
\alpha$; в этом случае $\alpha$ есть \emph{уровень значимости} критерия.
$\inf\limits_{\theta\in\Theta_1} g_S(\theta)$ --- \emph{мощность
критерия} (мощность есть наименьшая вероятность принять альтернативную
гипотезу, когда она верна). Идеальной (недостижимой) функцией мощности
является индикатор множества $\Theta_1$.

Критерий можно определить также при помощи \emph{критической функции}
$\ph_S$ --- индикатора множества $S$. Тогда
$g_S(\theta) = \Pf_\theta(S) = \Mf_\theta \ph_S(\xi)$.

Статистическая гипотеза называется \emph{простой}, если она имеет
вид $\Pf = \Pf_0$, где $\Pf_0$ --- заданное распределение (или, что
то же самое, $F(x) = F_0(x)$ или $\theta = \theta_0$; $F_0$ и
$\theta_0$ известны), т.е. речь идёт о конкретном распределении
вероятностей. Остальные гипотезы --- сложные.

\subsection{Разделение гипотез}

Пусть теперь $\Theta = \Theta_0 \cup \Theta_1, 
\Theta_0 \cap \Theta_1 = \varnothing$.
$\sup\limits_{\theta \in \Theta_0} \Pf_{\theta}(S)$ -- 
\emph{размер критерия} $S$.
Если $\sup\limits_{\theta \in \Theta_0} \leq \alpha$,
то $\alpha$ -- уровень значимости $S$.
\emph{Мощность критерия} $S$: $\inf\limits_{\theta \in \Theta_1}
g_s(\theta)$, где $g_s(\theta) = \Pf_{\theta}(S)$ -- \emph{функция
мощности критерия} $S$.
Положим
$$
\phi(x) =
\begin{cases}
1, & x \in S, \\
0, & x \notin S.
\end{cases}
$$
Такая функция называется \emph{критической функцией} критерия $S$.

\subsubsection{Сравнение двух простых гипотез. Теорема Неймана - Пирсона}
Пусть $\Theta = \{ \theta_0, \theta_1 \} \subset \mathbb{R}$, $\theta_0
< \theta_1$. $H_0$: $\theta = \theta_0$, $H_1$: $\theta = \theta_1$.
$\Pf_{\theta_0} (S)$ --- вероятность ошибки первого рода 
(\emph{размер критерия}).
$\Pf_{\theta_1} (\bar S)$ --- вероятность ошибки второго рода;
$1 - \Pf_{\theta_1} (\bar S) = \Pf_{\theta_1} (S)$ --- мощность критерия.
Критерий $S$ с критическим значением $\alpha$
называется \emph{несмещённым}, если 
$g_{S}(\theta_1) \geq \alpha$.

{\bf Сравнение критериев.} Зададим $\alpha \in (0,1)$ --- уровень
значимости (ограничитель размера критерия). Рассмотрим все критерии
(критические функции) $\ph_S(x)$ с $\Mf_{\theta_0} \ph_S(\xi) = \alpha$
(если $\Pf_{\theta_0}$ --- дискретное распределение, то такого $S$
может и не найтись --- тогда требуем $\leqslant \alpha$ или
применяем рандомизацию --- см. ниже).
Среди этих критериев выделяем $\ph_{S^*} = \ph_S^*$ --- такой, что
$\Mf_{\theta_1} \ph_S^*(\xi) = \sup\limits_{\ph_S\, :\, \alpha = \Mf_{\theta_0}
\ph_S(\xi)} \Mf_{\theta_1} \ph_S(\xi)$ (в общем случае берём $\sup$
по $\theta\in\Theta_1$) --- если такой критерий существует, то
это \emph{наиболее мощный критерий} уровня значимости $\alpha$.

{\bf Рандомизированные критерии.} Расширим множество рассматриваемых
критериев. Пусть $\ph(x)$ --- произвольная функция, принимающая значения
из $[0,1]$ (не обязательно индикатор). Если $\ph(x) = 1$, то $H_0$
отклоняется, если $\ph(x) = 0$, то $H_0$ принимается, а вот если
$0 < \ph(x) < 1$, то бросается жульническая монетка и $H_0$ отклоняется
в вероятностью $\ph(x)$. Такая процедура называется \emph{рандомизацией},
а сам критерий --- \emph{рандомизированным}. (Бросание монетки ---
вспомогательный эксперимент.) В этом случае функция мощности критерия
задаётся формулой: $g(\theta) = \Mf_\theta \ph(\xi)$.

\begin{theorem}[теорема Неймана\ч Пирсона; фундаментальная лемма]
Пусть $\Ps = \{ \Pf_\theta \mid \theta\in\Theta \}$,
$\Theta = \{ \theta_0, \theta_1 \} \subset \mathbb{R}$,
$\theta_0 < \theta_1$; семейство $\Ps$ абсолютно непрерывно
относительно некоторой меры (например, меры Лебега в $\mathbb{R}^n$),
т.е. существует плотность $p_\theta(x) = L(\theta; x)$ --- функция
правдоподобия; $L_i(x) = L(\theta_i; x), i \in \{ 0, 1 \}$. Пусть
$p_\theta(x) > 0$ (для всех $x \in \mathbb{R}, \theta\in\Theta$).
$H_0$: $\theta = \theta_0$, $H_1$: $\theta = \theta_1$. Задано
$\alpha \in (0,1)$ --- уровень значимости (вероятность ошибки
первого рода).\par Тогда критерий с критической функцией
$$
\ph^*(x) = \begin{cases}
1,          & L_1(x) > c_\alpha L_0(x), \\
\ep_\alpha, & L_1(x) = c_\alpha L_0(x), \\
0,          & L_1(x) < c_\alpha L_0(x),
\end{cases}
$$
где $c_\alpha$ и $\ep_\alpha$ определяются из условия
$\Mf_{\theta_0} \ph^*(\xi) = \alpha$, таков, что для любой
критической функции $\ph$ с $\Mf_{\theta_0} \ph(\xi) = \alpha$ имеет
место неравенство $\Mf_{\theta_1} \ph^*(\xi) \geqslant
\Mf_{\theta_1} \ph(\xi)$ (т.е. этот критерий обладает максимальной
мощностью среди критериев уровня значимости $\alpha$).
\end{theorem}

\begin{proof}
Сначала найдём $c_\alpha$ и $\ep_\alpha$.
Рассмотрим функцию $f(c) = \Pf_{\theta_0} \{ L_1(\xi) > c L_0(\xi) \}$,
$c \in [0, +\infty)$. $f$ --- невозрастающая функция $f(0) = 1$,
$f(+\infty) = 0$ (последняя запись понимается как предел).
$1 - f(c) = \Pf_{\theta_0} \left\{
\dfrac {L_1(\xi)}{L_0(\xi)} \leqslant c \right\}$ ---
функция распределения отношения значений функций правдоподобия
(\emph{отношения правдоподобия}) --- неубывающая непрерывная слева
функция.

Положим $c_\alpha = \min \{ c \mid f(c) \leqslant \alpha < f(c - 0) \}$
(если $c_\alpha$ --- точка непрерывности $f$, то $f(c_\alpha) = \alpha$).
Определим $\ep_\alpha$:
$$
\ep_\alpha = \begin{cases}
0, & \text{если $c_\alpha$ --- точка непрерывности $f$ ($f(c_\alpha) =
f(c_\alpha - 0)$)}, \\
\dfrac{\alpha - f(c_\alpha)}{f(c_\alpha - 0) - f(c_\alpha)}
& \text{в ином случае.}
\end{cases}
$$

Критерий определён. Будем доказывать его свойства.

\begin{multline*}
\Mf_{\theta_0} \ph^*(\xi) = \int\limits_{\Xs} \ph^*(x) p_{\theta_0}(x) \, dx =
\int\limits_{\{x \mid L_1(x) > c_\alpha L_0(x)\}} p_{\theta_0}(x) \, dx +
\ep_\alpha
\int\limits_{\{x \mid L_1(x) = c_\alpha L_0(x) \}} p_{\theta_0}(x) \, dx =\\=
\begin{cases}
f(c_\alpha) = \alpha, & \text{если $c_\alpha$ --- точка непрерывности $f$}, \\
f(c_\alpha) +
\dfrac{\alpha - f(c_\alpha)}{f(c_\alpha - 0) - f(c_\alpha)}
(f(c_\alpha - 0) - f(c_\alpha)) = \alpha & \text{в ином случае.}
\end{cases}
\end{multline*}
Итак, ошибка первого рода равна $\alpha$.

Теперь возьмём любой другой критерий с критической функцией $\ph$
с условием $\Mf_{\theta_0} \ph(\xi) = \alpha$. Покажем, что
$\Mf_{\theta_1} \ph^*(\xi) \geqslant \Mf_{\theta_1} \ph(\xi)$.
$\Xs^+:= \{ x\in\Xs \mid \ph^*(x) - \ph(x) \geqslant 0 \}$,
$\Xs^-:= \{ x\in\Xs \mid \ph^*(x) - \ph(x) < 0 \}$. $\Xs =
\Xs^+ \sqcup \Xs^-$. Для всех $x\in\Xs^+$ имеем, что
$\ph^*(x) - \ph(x) \geqslant 0$ $\Rightarrow$ $\ph^*(x) \geqslant
\ph(x) \geqslant 0$ $\Rightarrow$ $\ph^*(x) \geqslant 0$
$\Rightarrow$ $L_1(x) \geqslant c_\alpha L_0(x)$, откуда
$(\ph^*(x) - \ph(x))(p_{\theta_1}(x) - c_\alpha p_{\theta_0}(x))
\geqslant 0$. Если же $x \in \Xs^-$, то
$\ph^*(x) - \ph(x) < 0$, откуда $\ph^*(x) < 1$, то есть
$L_1(x) \leqslant c_\alpha L_0(x)$, и опять
$(\ph^*(x) - \ph(x))(p_{\theta_1}(x) - c_\alpha p_{\theta_0}(x))
\geqslant 0$. Поэтому
$$
\Mf_{\theta_1} (\ph^*(\xi) - \ph(\xi)) =
\int\limits_\Xs (\ph^*(x) - \ph(x)) p_{\theta_1}(x) \, dx =
\int\limits_\Xs (\ph^* - \ph)(p_{\theta_1}(x) -
\underbrace{c_\alpha p_{\theta_0}(x)}_{\text{\hspace*{-25pt}вычли нулевой интеграл\hspace*{-25pt}}}
) \, dx \geqslant 0.
$$
\end{proof}

Если $\Pf_{\theta_0}(S) \leqslant \Pf_{\theta_1}(S)$, то $S$-критерий
называется \emph{несмещённым}. 

\begin{theorem}
Критерий Неймана-Пирсона является несмещённым.
\end{theorem}

\begin{proof}
Положим 
$$
D_{c}^{*} := \{L_1 * c L_0\},
$$
где $* \in \{<, \leq, =, \geq, >\}$.
По определению ошибки второго рода 
$$
\beta = \Ef_{\theta_1} (1-\phi^{*}) =
\Pf_{\theta_1}(X \in D_{c_{\alpha}}^{\leq}) -
\varepsilon_{\alpha} \Pf_{\theta_1}(X \in D_{c_{\alpha}}^{=}).
$$
Пользуясь тем, что 
$
\Pf_{\theta_1}(X \in D_{c_{\alpha}}^{=}) =
c_{\alpha} \Pf_{\theta_0}(X \in D_{c_{\alpha}}^{=}),
$
получаем
$$
\beta = \Pf_{\theta_1}(X \in D_{c_{\alpha}}^{\leq}) -
\varepsilon_{\alpha} c_{\alpha} 
\Pf_{\theta_0}(X \in D_{c_{\alpha}}^{=}).
$$
По определению ошибки первого рода
$$
\alpha = \Ef_{\theta_0} \phi^{*} = 
\Pf_{\theta_0}(X \in D_{c_{\alpha}}^{>}) +
\varepsilon_{\alpha} \Pf_{\theta_0}(X \in D_{c_{\alpha}}^{=}).
$$
Выражая последнее слагаемое в предыдущее выражение получаем
\begin{equation} \label{test_n_p}
\beta = 1 - \Pf_{\theta_1}(X \in D_{c_{\alpha}}^{>}) -
c_{\alpha} (\alpha - \Pf_{\theta_0}(X \in D_{c_{\alpha}}^{>})).
\end{equation}
Для непрерывных функций правдоподобий имеем
$$
c_{\alpha} \Pf_{\theta_0}(X \in D_{c_{\alpha}}^{>}) =
\int_{L_1(x) > c_{\alpha} L_0(x)} c_{\alpha} L_0(x) dx \leq
\int_{L_1(x) > c_{\alpha} L_0(x)} L_1(x) dx =
\Pf_{\theta_1}(X \in D_{c_{\alpha}}^{>}).
$$
$$
\Pf_{\theta_1}(X \in D_{c_{\alpha}}^{\leq}) =
\int_{L_1(x) \leq c_{\alpha} L_0(x)} L_1(x) dx \leq
\int_{L_1(x) \leq c_{\alpha} L_0(x)} c_{\alpha} L_0(x) dx =
c_{\alpha} \Pf_{\theta_0}(X \in D_{c_{\alpha}}^{\leq}).
$$
Для дискретных интегралы заменяются на суммы.

Используя $2$ оценки, получаем
$$
\frac{1 - \Pf_{\theta_1}(X \in D_{c_{\alpha}}^{>})}{
1 - \Pf_{\theta_0}(X \in D_{c_{\alpha}}^{>})} \leq
c_{\alpha} \leq
\frac{\Pf_{\theta_1}(X \in D_{c_{\alpha}}^{>})}{
\Pf_{\theta_0}(X \in D_{c_{\alpha}}^{>})}.
$$
Преобразуя эти неравенства, приходим к
$$
\Pf_{\theta_0}(X \in D_{c_{\alpha}}^{>}) \leq
\Pf_{\theta_1}(X \in D_{c_{\alpha}}^{>}).
$$
Подставим эту оценку в \ref{test_n_p}
$$
\beta = 1 - c_{\alpha} \alpha + \Pf_{\theta_0}(X \in D_{c_{\alpha}}^{>})
(c_{\alpha} - 1) \leq 1 - c_{\alpha} \alpha + \alpha (c_{\alpha} - 1) =
1 - \alpha.
$$
Несмещённость доказана.
\end{proof}

\subsubsection{Равномерно наибольший критерий} \label{task32}
\begin{df}
Если существует критерий с критическим множеством $D$, такой что
его уровень значимости не превосходит $\alpha$ и для любого другого
критерия $D^{*}$ с уровнем значимости не более $\alpha$
при всех $\theta \in \Theta_1$ справедливо соотношение
$$
\Pf_{\theta}(D) > \Pf_{\theta}(D^{*}), \, \theta \in \Theta_1,
$$
то критерий $D$ называют \emph{равномерно наиболее мощным} критерием
для проверки $H_0$ против $H_1$.
Если $H_1$ простая, то критерий называют \emph{наиболее мощным}.
\end{df}

\begin{df}
Говорят, что для дискретных или абсолютно-непрерывных функций распределения
$F_{\theta}$ выполнено свойство \emph{монотонности отношения правдоподобий},
если все плотности имеют одинаковый носитель и существует статистика
$T(X_1, \ldots, X_n)$ такая, что при любых $\theta_0 < \theta_1$
$$
\frac{f_{\theta_1}(x_1) \cdots f_{\theta_1}(x_n)}{
f_{\theta_0}(x_0) \cdots f_{\theta_0}(x_n)} = 
g_{\theta_0, \theta_1}(T(x_1, \ldots, x_n)),
$$
где $g_{\theta_0, \theta_1}$ -- некоторая монотонная функция.
\end{df}

\begin{theorem}
Пусть $F_{\theta}$ -- семейство распределений с монотонным отношением
правдоподобий, причём упомянутая в определении функция монотонно возрастает 
по $T$. Тогда РНМ критерий уровня $\alpha$ для проверки гипотезы
$H_0: \theta \leq \theta_0$ с альтернативой $H_1: \theta > \theta_0$
задаётся критической функцией вида
$$
\psi(x_1, \ldots, x_n) = 
\begin{cases}
1, & T(x_1, \ldots, x_n) > c_{\alpha}, \\
\varepsilon_{\alpha}, & T(x_1, \ldots, x_n) = c_{\alpha}, \\
0, & T(x_1, \ldots, x_n) < c_{\alpha},
\end{cases}
$$
где $C_{\alpha}, \varepsilon_{\alpha}$ определяются соотношением
$\Ef_{\theta_0} \psi(X_1, \ldots, X_n) = \alpha$.
\end{theorem}

\begin{proof}
Положим $\beta_{\psi}(\theta) := \Mf_{\theta} \psi(X)$.
Докажем, что $\beta_{\psi}(\theta)$ монотонна по $\theta$.
Рассмотрим критерий $H_0': \theta = \theta_1$ с альтернативой 
$H_1': \theta = \theta_2 > \theta_1$. По теореме Неймана-Пирсона
существует наиболее мощный критерий 
$$
\phi_{\theta_1, \theta_2}(x) =
\begin{cases}
1,          & L_2(x) > d_\alpha L_1(x), \\
\delta_\alpha, & L_2(x) = d_\alpha L_1(x), \\
0,          & L_2(x) < d_\alpha L_1(x),
\end{cases} =
\begin{cases}
1,          & g_{\theta_1, \theta_2}(T(x)) > d_\alpha, \\
\delta_\alpha, & g_{\theta_1, \theta_2}(T(x)) = d_\alpha, \\
0,          & g_{\theta_1, \theta_2}(T(x)) < d_\alpha,
\end{cases} =
\begin{cases}
1,                      & T(x) > \widetilde{c}_\alpha, \\
\widetilde{\ep}_\alpha, & T(x) = \widetilde{c}_\alpha, \\
0,                      & T(x) < \widetilde{c}_\alpha,
\end{cases}.
$$
Вообще говоря, $\widetilde{\ep}_\alpha$ и $\widetilde{c}_\alpha$
могли зависеть от $\theta_1$ и $\theta_2$, но так как 
эти параметры определяются исходя из того, что 
$\Mf \phi_{\theta_1, \theta_2}(X_1, \ldots, X_n) = \alpha$ -- функции,
не зависящей от $\theta_1$ и $\theta_2$, то 
$\ep_\alpha := \widetilde{\ep}_\alpha$ и $c_\alpha := \widetilde{c}_\alpha$,
$\psi := \ph_{\theta_1, \theta_2}$.

Рассмотрим критерий $\widetilde{\ph} \equiv 
\beta_{\psi}(\theta_1)$ --
критерий, вне зависимости от выборки отклоняющий гипотезу с вероятностью
$\beta_{\psi}(\theta_1)$. Так как 
$\beta_{\psi}(\theta_1) = \beta_{\widetilde{\ph}}(\theta_1)$,
то в силу НМ критерий $\psi$ имеем
$$
\beta_{\psi}(\theta_2) \geq 
\beta_{\widetilde{\ph}}(\theta_2) = 
\beta_{\psi}(\theta_1),
$$
откуда $\beta_{\psi}(\theta)$ -- монотонна по $\theta$.

В силу монотонности
$$
\sup_{\theta \leq \theta_0} \beta_{\psi}(\theta) = 
\beta_{\psi}(\theta_0) = \alpha.
$$
Для любого критерия $\psi_1$ такого, что уровень значимости $\psi_1$ не более
$\alpha$ верно, что $\beta_{\psi_1}(\theta_0) \leq \alpha$,
поэтому в силу НМ $\psi$ для $H_0': \theta = \theta_0$,
$H_1': \theta = \theta_1 > \theta_0$
$$
\beta_{\psi_1}(\theta_1) \geq \beta_{\psi}(\theta_1),
$$
то есть критерий является РНМ.
\end{proof}


\begin{ex} \label{task33}
Проверим гипотезу о среднем нормального распределения $H_0: \theta=\theta_0$
с альтернативой $H_1: \theta=\theta_1$ при известной дисперсии
$\sigma^2$.
Выпишем функцию правдоподобия
$$
L(\theta, \sigma^2; x) =
L(\theta, \sigma^2; x_1, \ldots, x_n) =
\frac{1}{(\sigma \sqrt{2 \pi})^n} 
\exp\left\{-\frac{\sum_{i=1}^n (x_i - \theta)^2}{2 \sigma^2}\right\}.
$$

Тогда
\begin{multline*}
D =
\left\{\frac{L(\theta_0, \sigma^2; x)}{L(\theta_1, \sigma^2; x)} > c\right\} =
\left\{\exp\left\{-\frac{
\sum_{i=1}^n ((x_i-\theta_0)^2 - (x_i-\theta_1)^2)}{
2 \sigma^2}\right\} > c\right\} = \\
\left\{(\theta_0 - \theta_1) \frac{
\sum_{i=1}^n (2 x_i - \theta_0 - \theta_1)}{
\sigma^2} > 2 \ln c\right\} = 
\left\{2(\bar x - \theta_0) >
\frac{2 \sigma^2 \ln c}{(\theta_0 - \theta_1) n} +
\theta_1 - \theta_0\right\}.
\end{multline*}
Обозначим выражение справа в неравенстве за $\widetilde{c}$.
Пусть верна гипотеза и $X_i \sim \Norm(\theta_0, \sigma^2)$.
тогда $\Bar X \sim \Norm(\theta_0, \sigma^2/n)$,
$Z := \sqrt{n} (\Bar X - \theta_0) / \sigma \sim \Norm(0, 1)$ и
$$
D =
\left\{\sqrt{n} \frac{\bar x - \theta_0}{\sigma} > 
\frac{\sqrt{n} \widetilde{c}}{2 \sigma}\right\} =
\left\{Z > \hat{c}\right\}
$$
По теореме Неймана-Пирсона $P_{\theta_0}(D) = \alpha$,
поэтому $\hat{c}$ -- квантиль уровня значимости $1-\alpha$ 
стандартного нормального распределения. Критерий будет выглядеть следующим образом:
если $\bar x \leq \theta_0 + \sigma \hat{c} / \sqrt{n}$, тогда принимаем гипотезу $H_0$,
иначе отклоняем. Такой критерий по теореме будет наиболее мощным и несмещённым.
\end{ex}

\subsection{Проверка гипотез о параметрах нормального распределения} 
\label{task34}
Пусть дана выборка  $(x_1, \ldots, x_n)$ из $\Norm(a, \sigma^2)$,
$H_0$: $a = a_0$ ($a_0$ фиксировано); $H_1$: $a \ne a_0$,
$\sigma$ неизвестно. Тогда по следствию из леммы Фишера
$\sqrt{n} (\overline{\eta} - a) / s \sim t_{n-1}$.
Рассмотрим критическое множество
$$
S = \left\{\left|\frac{\overline{x} - a_0}{s / \sqrt{n}}\right| >
t_{1 - \alpha/2}\right\},
$$
где $t_{1-\alpha/2}$ -- $1-\alpha/2$ квантиль $t_{n-1}$.

Критерий:
если $x$ не принадлежит нашему критическому множеству, 
то $H_0$ принимается, иначе отклоняется. Вероятность ошибки первого рода
(отклонить $H_0$, когда она верна) равна $\alpha$, так как
при гипотезе $a = a_0$.

Пусть теперь $H_0: \sigma = \sigma_0$ ($\sigma_0$ фиксировано),
$H_1: \sigma \ne \sigma_0$, $a$ неизвестно. При гипотезе имеем
$(n-1) s / \sigma_0^2 \sim \chi_{n-1}^2$, 
поэтому рассмотрим критическое множество
$$
S = \left\{\frac{(n-1) s}{\sigma_0^2} > \varkappa_{1-\alpha}\right\} =
\left\{\frac{\sum_{i=1}^n (x_i - \overline{x})^2}{\sigma_0^2} > 
\varkappa_{1-\alpha}\right\},
$$
где $\varkappa_{1-\alpha}$ -- $1-\alpha$ квантиль $\chi_{n-1}^2$.

Критерий:
если $x$ не принадлежит нашему критическому множеству, 
то $H_0$ принимается, иначе отклоняется. Вероятность ошибки первого рода
(отклонить $H_0$, когда она верна) равна $\alpha$.

\subsection{Проверка гипотез о параметрах нормального распределения с
помощью доверительного интервала} 
\label{task35}
Пусть дана выборка  $(x_1, \ldots, x_n)$ из $\Norm(a, \sigma^2)$,
$H_0$: $a = a_0$ ($a_0$ фиксировано); $H_1$: $a \ne a_0$,
$\sigma$ неизвестно. Тогда по следствию из леммы Фишера
$\sqrt{n} (\overline{\eta} - a) / s \sim t_{n-1}$.
Можем построить доверительный интервал для параметра как
$$
a_0 \in \left(\overline{x} - \frac{t_{1 - \alpha/2} s}{\sqrt{n}},
\overline{x} + \frac{t_{1 - \alpha/2} s}{\sqrt{n}}\right),
$$
где $t_{1-\alpha/2}$ -- $1-\alpha/2$ квантиль $t_{n-1}$.

Критерий:
если $x$ принадлежит нашему доверительному интервалу, 
то $H_0$ принимается, иначе отклоняется. Вероятность ошибки первого рода
(отклонить $H_0$, когда она верна) равна $\alpha$, так как
при гипотезе $a = a_0$.

Пусть теперь $H_0: \sigma = \sigma_0$ ($\sigma_0$ фиксировано),
$H_1: \sigma \ne \sigma_0$, $a$ неизвестно. При гипотезе имеем
$(n-1) s / \sigma_0^2 \sim \chi_{n-1}^2$, 
поэтому построим доверительный интервал для параметра как
$$
\sigma_0^2 \in \left(\frac{(n-1) s}{\varkappa_{1-\alpha/2}}, 
\frac{(n-1) s}{\varkappa_{\alpha/2}}\right),
$$
где $\varkappa_{\beta}$ -- $\beta$ квантиль $\chi_{n-1}^2$.

Критерий:
если $x$ принадлежит нашему доверительному интервалу, 
то $H_0$ принимается, иначе отклоняется. Вероятность ошибки первого рода
(отклонить $H_0$, когда она верна) равна $\alpha$.

% Пусть дана выборка  $(x_1, \ldots, x_n)$ из $\Norm(a, \sigma^2)$. 
% $H_0$: $a = a_0$ ($a_0$ фиксировано); $H_1$: $a \ne a_0$. 
% Пусть $\sigma^2$ известно и равно $\sigma^2_0$.
% Известно, что $(\overline{\xi} - a) / \sqrt{\sigma^2/n} \sim 
% \Norm(0, 1)$. По лемме Фишера, т.к. $\overline{\xi}$ и $S^2$ независимы,
% то $(\overline{\xi} - a) / \sqrt{S^2/n} \sim t_{n-1}$.
% Тогда $\left[ \overline x - t_{1-\frac\alpha 2}
% \sigma^2_0 / \sqrt{n}, \overline x + t_{1-\frac\alpha 2}
% \sigma^2_0 / \sqrt{n} \right]$ --- доверительный интервал для
% $a$ с доверительной вероятностью $1 - \alpha$. Соответственно,
% критическим множеством является 
% $$
% S = \left\{\left|\frac{\overline{x} - a_0}{s/\sqrt{n}}\right| > 
% t_{1 - \alpha/2}\right\}.
% $$
% Критерий:
% если $a_0$ принадлежит нашему доверительному интервалу, то $H_0$
% принимается, иначе отклоняется. Вероятность ошибки первого рода
% (отклонить $H_0$, когда она верна) равна $\alpha$. Вероятность
% ошибки второго рода явно не вычисляется (гипотеза $H_1$ --- не конкретная).
% 
% Пусть теперь $\sigma$ неизвестна, тогда
% 
% Если альтернативная гипотеза имеет вид $H_1$: $a > a_0$, то вместо
% рассмотренного ранее двустороннего используем \emph{односторонний критерий}:
% оставляем только верхнюю критическую границу.
% 
% Аналогично при помощи доверительных интервалов строятся критерии для
% $a$ при неизвестном $\sigma^2$ и для $\sigma^2$ (при известном или
% неизвестном $a$).
% Критерии, в которых $H_0$ задаёт конкретное распределение вероятностей,
% а $H_1 = \lnot H_0$, называются \emph{критериями согласия}.

%%%%%%%
%%%%%%%
%% Lect. 13 by Alexander V. Kharitonov
%%%%%%%
%%%%%%%

% FIXME: remove these comments

%\pagebreak
%{\Huge Лекция \textnumero~13}

% 25-Nov-2006
%\documentclass{article}
%\usepackage{amssymb,amsthm,amsmath}
%\usepackage[T2A]{fontenc}
%\usepackage[utf-8]{inputenc}
%\usepackage[russian]{babel}
%\newcommand{\eps}{\varepsilon}

%\newtheorem{theorem}{Теорема}
%\newtheorem{lemma}{Лемма}

% FIXME: переместить в преамбулу
\let\le=\leqslant
\let\leq=\leqslant
\let\ge=\geqslant
\let\geq=\geqslant
%\let\preceq=\preccurlyeq
%\let\succeq=\succcurlyeq

\newcommand{\perems}[2]{#1_1,\dots,#1_{#2}}
\newcommand{\perem}[3]{#1_{#2},\dots,#1_{#3}}

%\begin{document}
%%В случае одностороннего критерия можно использовать не доверительный интервал, а доверительные границы.

\subsection{Проверка гипотезы однородности нормальных выборок}
Пусть имеется две выборки $\perems xm; \perems yn$, порожденные соответственно независимыми в совокупности случайными величинами
$\perems \xi m, \perems \eta n$, причём $\perems \xi m \thicksim \Norm(a_1,\sigma_1^2), \perems \eta n \thicksim \Norm(a_2,\sigma_2^2)$ и все параметры $a_1, \sigma_1^2, a_2, \sigma_2^2$ неизвестны.

$H_0: a_1 = a_2, \sigma_1 = \sigma_2$ --- \emph{гипотеза однородности}.
$H_1 = \lnot H_0$. Разобьём задачу проверки гипотезы однородности на две задачи: \par\noindent
I. $H'_0: \sigma_1^2 = \sigma_2^2$ (при любых $a$), $H'_1: \sigma_1^2 \neq \sigma_2^2$ (если $H'_0$
отклоняется, то отклоняем $H_0$; в противном случае движемся дальше).
 \par\noindent
II. Если $H'_0$ принимается, то $H''_0: a_1 = a_2, H''_1: a_1 \neq a_2$. Теперь $H_0$ принимается
тогда и только тогда, когда принимается $H''_0$.

{\bf Критерий Фишера.} Если $f_{\frac{\alpha}{2}} \leq \hat
F_{m,n} \leq f_{1-\frac{\alpha}{2}}$, то $H'_0$ принимается, иначе
отвергается. Вероятность ошибки первого рода в точности равна
$\alpha$. (Можно использовать и односторонний критерий)


\subsubsection{Критерий Стьюдента равенства средних значений при условии равенства дисперсий} \label{equality_expectancy}

%\subsection*{II.(о равенстве средних значений при условии равенства дисперсий)}

Если $H'_0$ отклоняется, то отклоняется и гипотеза однородности $H_0$. Пусть теперь $H'_0$
принята (т.е. $\sigma_1^2 = \sigma_2^2 = \sigma^2$). Будем проверять гипотезу
$H''_0: a_1 = a_2$ ($H''_1 = \lnot H''_0$).

%\subsubsection*{Критерий Стьюдента}

Мы знаем, что $\Mf (\bar x - \bar y) = a_1 - a_2$ и $\Df (\bar x - \bar y) = \sigma^2(1/m + 1/n)$
($\bar x$ и $\bar y$ независимы и $\bar x - \bar y \sim \Norm(a_1 - a_2, \sigma^2(1/m + 1/n))$.)

$$
\frac{(\bar x - \bar y) - (a_1 - a_2)}{\sqrt{\sigma^2(1/m + 1/n)}} \sim \Norm(0,1),
$$
но $\sigma^2$ неизвестно. Подставим для неё несмещённую оценку
$$
\hat \sigma^2 = \frac{(m-1)s_1^2 + (n-1)s_2^2}{m+n-2}
$$

При условии $H''_0$: $a_1 - a_2 = 0$ статистика $\hat t_{m+n-2} = \frac{\bar x - \bar y}{\sqrt{\sigma^2(1/m+1/n)}}$ имеет распределение Стьюдента
с $m+n-2$ степенями свободы (следует из леммы Фишера).

Зададим $\alpha \in (0,1)$. $t_{\alpha/2} = - t_{1 - \alpha/2}$ --- соответствующие квантили
распределения Стьюдента (равенство имеет место в силу симметричности распределения). Критическим множеством будет
$$
S = \left\{\left|\frac{(\bar x - \bar y) - (a_1 - a_2)}{
\sqrt{\sigma^2(1/m + 1/n)}}\right| > t_{1-\alpha/2}\right\}
$$

{\bf Критерий Стьюдента.} Если $|\hat t_{m+n-2}| > t_{1-\alpha/2}$, то $H''_0$ отклоняется,
иначе~--- принимается. Вероятность ошибки первого рода равна $\alpha$.
Если при этом $H''_0$ принимается, то принимается и исходная гипотеза однородности $H_0$.

\subsubsection{Критерий Фишера равенства дисперсий} \label{task37}

Рассмотрим две статистики:
$$
s_1^2 = \frac{1}{m-1}\sum_{k=1}^m (x_k - \bar x)^2, \qquad
s_2^2 = \frac{1}{n-1}\sum_{j=1}^n (y_j - \bar y)^2.$$

Отношение $\hat F_{m,n} = \dfrac{s_1^2 / \sigma_1^2}{s_2^2 / \sigma_2^2}$ при условии $H'_0$
($\sigma_1^2 = \sigma_2^2$) равно $s_1^2 / s_2^2$ и
%
%отношение при условии $H'_0$($\sigma_1^2 = \sigma_2^2 = \sigma^2$)
%$$
%\frac{\frac{s_1^2}{\sigma_1^2}}{\frac{s_2^2}{\sigma_2^2}} = \frac{s_1^2}{s_2^2}
%$$
%
%$\hat F_{m,n} = s_1^2/s_2^2$ при $H'_0$
имеет $F$-распределение с $(m-1)$ и $(n-1)$ степенями свободы:

$$
\frac{s_1^2}{s_2^2} = \frac{\frac{(m-1)s_1^2}{\sigma^2}\cdot\frac{1}{m-1}}{\frac{(n-1)s_2^2}{\sigma^2}\cdot\frac{1}{n-1}} = \frac{\chi^2_{m-1} \cdot \frac{1}{m-1}}{\chi^2_{n-1} \cdot \frac{1}{n-1}} = F_{m-1,n-1},
$$
где предпоследнее равенство справедливо в силу леммы Фишера.

Для заданного $\alpha$ находим квантили $f_{\frac{\alpha}{2}}(m-1,n-1)$ и $f_{1-\frac{\alpha}{2}}(m-1,n-1)$ $F$-распределения с $(m-1)$ и $(n-1)$ степенями свободы и берем их в качестве критических значений.
Критическим множеством будет
$$
S = \left\{\frac{1}{f_{1-\alpha/2}} < 
\frac{S_1^2}{S_2^2} < f_{\alpha/2}\right\}
$$

\subsection{Дисперсионный анализ однофакторной модели} \label{task39}

Пусть $\perems xn$~--- выборка, где все элементы получены независимо. %Выделяем $k, k \geq 3$ подвыборок (a priori).
A priori разобьём её на $k, k \geq 3$ подвыборок: $x_{11}, \dots, x_{n_11}; \dots; x_{1k}, \dots,
x_{n_kk}$, $n = n_1 + \dots + n_k$. $\bar x_i = (x_{1i} + \dots + x_{n_ii}) / {n_i}$ --- средние
значения по подвыборкам,
$\bar{\bar x} = (x_{11} + \dots + x_{n_kk}) / n$ --- среднее значение.

Эти данные удобно представлять в виде \emph{таблицы дисперсионного анализа}:

\begin{tabular}{|c|c|c|c|c|}
\cline{1-4}
1 & 2 & $\ldots$ & $k$ \\\cline{1-4}
$x_{11}$   & $x_{12}$   & $\ldots$ & $x_{1k}$ \\
$x_{21}$   & $x_{22}$   & $\ldots$ & $x_{2k}$ \\
$\vdots$   & $\vdots$   &          & $\vdots$ \\
\hline
$n_1$      & $n_2$      & $\ldots$ & $n_k$      & $n$            \\\hline
$a_1$      & $a_2$      & $\ldots$ & $a_k$      & $\bar a$       \\\hline
$\bar x_1$ & $\bar x_2$ & $\ldots$ & $\bar x_k$ & $\bar{\bar x}$ \\\hline 
\end{tabular}

Предполагается, что существуют постоянные (но неизвестные нам)
$\perems ak$ такие, что $x_{ij} = a_j + e_{ij}$ при всех $i$ и
$j$, где $e_{ij}$~--- случайные ошибки, взаимно независимые при
разных $i$ и $j$, $\Mf e_{ij} = 0$, $\Df e_{i,j} = \sigma^2$
(считаем измерения равноточными), $\sigma^2$ неизвестно.

Эта модель называется \emph{однофакторной}. Считается, что на эксперименты, порождающие
данные в столбцах, отличаются влиянием некоторого фактора;
$1,2,\ldots,k$~--- номера уровней фактора, $a_j$~--- характеристики уровня фактора.
Различия между элементами одних столбцов ,,чисто случайны''.
Существуют и многофакторные модели.

$H_0$: гипотеза об отсутствии фактора ($a_1 = \ldots = a_k$) (гипотеза однородности
подвыборок), $H_1 = \lnot H_0$.
Если $e_{ij} \sim \Norm(0,\sigma^2) \Leftrightarrow x_{ij} \sim \Norm(a_j, \sigma^2),$
то $H_0$ есть гипотеза о равенстве средних значений $k$ нормальных выборок.
[Случай, когда $x_{ij}$ имеют непрерывные распределения, существенно отличающиеся от нормального,
относится к непараметрическому анализу (ранговые критерии, etc). Этим мы заниматься не будем.]

\emph{Изменчивость данных}~--- отклонение от среднего (измеряется как выборочная дисперсия)
$$
\sum_{i,j} (x_{ij} - \Bar{\Bar x})^2 = \sum_{i,j} (x_{ij} - \bar
x_j)^2 + \sum_{j} n_j (\bar x_j - \Bar{\Bar x})^2$$
%где $\Bar{\Bar x} = 1/n \sum_{i,j} x_{ij}$~--- полное среднее, $\bar x_j = 1/n_j \sum_{i=1}^{n_j} x_{ij}$ - среднее по $j$-му столбцу.

$$
\sum_{i,j} x_{ij} = n \Bar{\Bar x} = \sum_{j=1}^k n_j \bar x_j
$$

Найдём распределение первой суммы. По лемме Фишера
$$
\frac{\sum\limits_{i=1}^{n_j} (x_{ij} - \bar x_j)^2}{\sigma^2} = \chi^2_{n_j - 1},%\quad \text{(по лемме Фишера)}
$$
откуда
$$
\frac{1}{\sigma^2}\sum_{i,j} (x_{ij} - \bar x_j)^2 = \sum_{j=1}^k \chi^2_{n_j - 1} = \chi^2_{\sum_j (n_j - 1)} = \chi^2_{n-k},
$$
т.к. все $\chi^2_{n_j - 1}$ у нас независимы.

$\bar x_j$ не зависит от $\sum(x_{ij} - \bar x_j)^2$ по лемме Фишера, следовательно, $\sum_j(x_{ij} - \bar x_j)^2$ и
$\sum_j (\bar x_j - \Bar{\Bar x})^2$ независимы.

Если $H_0$ верна, то $\frac 1{\sigma^2} \sum (x_{ij} - \Bar{\Bar x})^2 = \chi^2_{n-1}$:
$$
\frac 1{\sigma^2} \sum_j n_j(\bar x_j - \Bar{\Bar x})^2 = \chi^2_{k-1} \text{(по лемме Фишера)}
$$
Так как $\bar \xi_j$ не зависит от 
$\sum_{i=1}^{n_j} (\xi_{i, j} - \bar \xi_j)^2$,
$\bar{\bar \xi}$ не зависит от 
$\sum_{i=1}^k (\bar \xi_i - \bar{\bar \xi})^2$, то
$$
\chi^2_{n-1} = \chi^2_{n-k} + \chi^2_{k-1}
$$
$\hat \sigma_1^2 = \frac{1}{n-k} \sum_{i,j}(x_{ij} - \bar x_j)^2$~--- оценка, не зависящая от гипотезы $H_0$,
$\hat \sigma_2^2 = \frac{1}{k-1} \sum_{j} n_j (\bar x_j - \Bar\Bar x)^2$~--- реагирует на $H_0$\\
%$$\frac{\hat \sigma_2^2}{\hat \sigma_1^2} = \hat F_{k-1,n-k}\quad\text{--- дисперсионное отношение}$$
Дисперсионное отношение $\dfrac{\hat \sigma_2^2}{\hat \sigma_1^2}$
имеет распределение Фишера\ч Снедекора с $k-1,\,n-k$ степенями
свободы.

{\bf Правосторонний критерий Фишера.} Пусть $\alpha:\quad
f_{1-\alpha}(k-1,n-k)$~--- $(1-\alpha)$-квантиль $F$-распределения
с $k-1,n-k$ степенями свободы. Если $\hat F_{k-1,n-k} >
f_{1-\alpha}$, то $H_0$ отклоняется (фактор существует); иначе~---
принимается.

\subsection{Парные и множественные сравнения} \label{task40}
Рассмотрим случай, когда в дисперсионном анализе однофакторной модели гипотеза отсутствия фактора
($H_0$) отвергнута. Проведём более тонкое исследование.
\subsubsection{Парное сравнение}
Проверяем гипотезу $H_{0(j,l)}$: $a_j = a_l \Leftrightarrow a_j - a_l = 0$;  $H_1 = \lnot H_0$.
(однородности двух подвыборок).

\hyperref[equality_expectancy]{Строим} доверительный интервал для разности $a_j - a_l$. $\bar x_j, \bar x_l$~--- оценки для $a_j, a_l$.

$$
\frac{(\bar x_j - \bar x_l) - (a_j - a_l)}{\sqrt{\hat \sigma_1^2 \left( \frac{1}{n_j} + \frac{1}{n_l} \right)}} = \hat t_{n-k}
$$
$\hat t_{n-k}$ имеет распределение Стьюдента с $(n-k)$ степенями свободы, фиксируем $\alpha$, строим доверительный интервал для $a_j - a_l$
стандартным способом: $$\left[ (\bar x_j - \bar x_l) - t_{1-\frac\alpha 2}(n-k)
\sqrt{\hat\sigma_1^2 \left(\frac 1 {n_j}  + \frac 1 {n_l}\right)},\:
(\bar x_j - \bar x_l) + t_{1-\frac\alpha 2}(n-k)
\sqrt{\hat\sigma_1^2 \left(\frac 1 {n_j}  + \frac 1 {n_l}\right)} \, \right].$$


{\bf Критерий:} если $0$ лежит внутри этого интервала, то
принимаем $H_{0(j,l)}$, иначе~---  отклоняем. Если левая граница больше нуля,
то делаем вывод, что $a_j > a_l$ с вероятностью как минимум $1-\al$.
Аналогично, если правая граница меньше нуля,
то делаем вывод, что $a_j < a_l$ с вероятностью как минимум $1-\al$.

\subsubsection{Множественные сравнения}
$$
\Psi = \sum_{j=1}^k c_j a_j,\quad \sum_{j-1}^k c_j = 0
$$

$\Psi$~--- ,,\emph{сравнение}'' $\perems ak$ (,,контраст''),
парное сравнение~--- частный случай такого. Оценим: $\hat \Psi =
\sum_{j=1}^k c_j$
$$
\Mf \hat\Psi = \Psi, \qquad \Df \hat\Psi = \sigma^2 \sum_{j=1}^k c^2_j/n_j
$$
$$
\frac{\hat \Psi - \Psi}{\sqrt{\hat \sigma^2_1 \sum_{j}
\frac{c^2_j}{n_j}}} = \hat t_{n-k}
$$

Далее стандартным образом строим доверительный интервал для $\Psi$ ($\hat t_{n-k}$ имеет распределение Стьюдента). Проводя множественные сравнения между
разными параметрами, мы можем определить, на какие однородные подгруппы можно
разбить имеющуюся выбору.

\subsection{Критерий Пирсона \texorpdfstring{$\chi^2$}{хи-квадрат}}
%\subsubsection{Схема Бернулли с параметром $p \in (0,1)$}
\subsubsection{Биномиальный критерий} \label{task41}
Рассмотрим схему Бернулли с параметром $p\in(0,1)$.
Проверяем гипотезу $H_0: p = p_0,\quad H_1: p \neq p_0$.
$T(x) = x_1 + \ldots + x_n$~--- достаточная статистика, $\alpha$~--- уровень значимости (вероятность
ошибки первого рода), $m^*_\alpha$~---
 некоторая критическая граница.\par\noindent
{\bf Критерий:} если $T(x) > m^*_\alpha$, то $H_0$ отклоняем, иначе принимаем.
Запишем вероятность ошибки первого рода:
$
\sum\limits_{m > m^*_\alpha} \Cb_n^m p_0^m (1-p_0)^{n-m} \leq \alpha
$

Заменим точный критерий на приближенный с помощью ЦПТ.
$$
\frac{T(\xi) - np}{\sqrt{np(1-p)}} \radnrai \Norm(0,1).
$$
Вероятность ошибки первого рода:
$$
\Pf_{p_0}\left( \frac{T(\xi) - np_0}{\sqrt{np_0(1-p_0)}} > \frac{m^*_\alpha - np_0}{\sqrt{np_0(1-p_0)}} \right) = \alpha
$$
$$
1 - \Phi\left(\frac{m^*_\alpha - np_0}{\sqrt{np_0(1-p_0)}}\right) = \alpha
$$
$$
u_{1-\alpha} = \frac{m^*_\alpha - np_0}{\sqrt{np_0(1-p_0)}} \text{~--- $(1-\alpha)$-квантиль $\Norm(0,1)$}
$$
Из последнего условия находится граница $m^*_\alpha = np_0 + u_{1-\alpha} \sqrt{np_0(1-p_0)}$.

%%%
%%% Lect. 14 by A.V.Kharitonov
%%%

%\subsection*{О проверке гипотез о параметре биномиального распределения}

%Рассмотрим схему Бернулли и статистические гипотезы о значении вероятности успеха в ней. $H_0: p = p_0$~--- простая гипотеза (задаёт конкретное распределение вероятностей), $H_1 = \lnot H_0$~--- сложная гипотеза.
%
%\subsubsection*{Биномиальный критерий}
% Если $\sum_{i=1}^n x_i > m^{*}_\alpha$, то $H_0$ отклоняется. При этом $m^*_\alpha$ вычисляется исходя из заданного уровня
%значимости $\alpha$.
%С помощью ЦПТ можно получить приближённое значение $m^*_\alpha = n p_0 + u_{1-\alpha} \sqrt{np_0(1-p_0)}$, где $u_{1-\alpha}$~--- $(1-\alpha)$-квантиль
%$\Norm(0,1)$. С таким значением $m^*_\alpha$ вероятность ошибки первого рода $\to \alpha (n \to \infty)$.

\subsubsection{Критерий \texorpdfstring{$\chi^2$}{хи-квадрат} для схемы Бернулли (предисловие к критерию Пирсона)}
Есть и другой критерий. Пусть $m = x_1 + \ldots + x_n$. $\Pf(S_n = m) = \Cb^m_n p^m (1-p)^{n-m}$.
Рассмотрим статистику
$$
\widehat X^2_1 = \frac{(m-np_0)^2}{np_0} + \frac{(n-m-n(1-p_0))^2}{n(1-p_0)}
$$

$\widehat X^2_1$ есть сумма относительных квадратичных отклонений эмпирических результатов от ожидаемых. В условиях $H_0$ $\Mf S_n = np_0$.
$$
\widehat X^2_1 = \frac{(m-np_0)^2}{np_0} + \frac{(m-np_0)^2}{n(1-p_0)} = \frac{(m-np_0)^2}{np_0(1-p_0)} = \left( \frac{m-np_0}{\sqrt{np_0(1-p_0)}} \right)^2
$$

Следовательно, так как в силу ЦПТ $\frac{S_n - np_0}{\sqrt{np_0(1-p_0)}} \rad \Norm(0,1)$,
то $\widehat X^2_1$, как квадрат этой величины,
сходится по распределению к $\chi^2_1$ при условии $H_0$.
$\widehat X^2_1$ называется \emph{статистикой Пирсона (хи-квадрат)}.
Критерий строится стандартным образом при помощи квантилей хи-квадрат распределения.

\subsubsection{Критерий знаков}
$(x_1, y_1), \dots, (x_n, y_n)$ --- набор парных наблюдений, порождённый
парой многомерных случайных величин $(\xi, \xi')$ ($\xi$ и $\xi'$ могут
быть зависимыми, а вот $\xi_1, \dots, \xi_n$ независимы в совокупности
и одинаково распределены, равно как и $\xi_1', \dots, \xi_n'$). Пусть
$\xi_1, \dots, \xi_n$ распределены с функцией распределения $F(x)$, а
$\xi_1', \dots, \xi_n'$ --- с функцией распределения $G(x) =
F(x - \theta)$, $\theta\in\mathbb{R}$ --- параметр.
Гипотеза $H_0$ состоит в однородности выборок (т.е. $\theta = 0$);
$H_1 = \lnot H_0$.

Перейдём к разностям $z_i = x_i - y_i$ (это значения случайных величин
$\eta_i = \xi_i - \xi_i'$; $\eta_1, \dots, \eta_n$ независимы в
совокупности). Пусть на самом деле 
$\xi_i' = \xi + \theta + \varepsilon_i$,
где $\varepsilon_i$ - случайная величина с абсолютно непрерывным
распределением и $\Pf(\varepsilon_i) > 0$.
Тогда гипотеза $H_0$ равносильна гипотезе $H_0'$:
$\forall i \, \Pf(\eta_i > 0) = \Pf(\eta_i < 0) = \frac 12$.
Судим о справедливости гипотезы $H_0$ по соотношению знаков
,,$+$'' и ,,$-$'' среди $z_i$. По сути имеется $n$ испытаний
Бернулли, где элементарными событиями являются
$A_i = \{ \eta_i > 0 \}$, $\overline{A_i} = \{ \eta_i < 0 \}$.
Гипотеза $H_0'$ равносильна гипотезе $H_0''$: ,,в этой схеме
Бернулли $p = \frac 12$'' ($p = \Pf(A_i)$ --- параметр схемы
Бернулли). Строим биномиальный критерий для случайной величины
$$
\mu_n^+ = \sum_{i=1}^{n} \ph_i,
\qquad
\ph_i = \begin{cases}
1, & \eta_i > 0 \\
0, & \eta_i < 0
\end{cases}
$$
с основной гипотезой $H_0'''$: $p = \frac 12$.

\emph{Практический совет} на случай, когда среди $z_i$ встречаются
нули: если нулей много, то критерий неприменим (потому что в
этом критерии считается, что $\Pf(\eta_i = 0) = 0$). Если же
нулей мало, то просто выкидываем те испытания, в которых
$z_i = 0$.

% FIXME: добавить способ явного вычисления

\subsubsection{Полиномиальный критерий} \label{task42}
Обобщим наш критерий. Будем проверять гипотезу о том, что данная
выборка $x_1,\ldots,x_n$ имеет полиномиальное распределение с $s$
исходами и заданными вероятностями: $H_0: p_1 = p_1^0, \ldots, p_s
= p_s^0$, $H_1 = \lnot H_0$. Обозначим за $m_k$ количество исходов
типа $k$ и составим статистику a l\`{a} $\widehat X^2_1$.

{\bf Статистика Пирсона:}
$$\widehat X^2_{s-1} = \sum_{k=1}^s \frac{(m_k - np_k^0)^2}{np_k^0}$$

Далее мы покажем, что в условии $H_0$ и $n \to \infty$ $\widehat X^2_{s-1}$ имеет
асимптотическое распределение $\chi^2_{s-1}$ (теорема Пирсона).

{\bf Критерий Пирсона} \label{task43} 
($\chi^2$) о данном распределении в полиномиальной модели.
Пусть $g_{1-\alpha}$~--- $(1-\alpha)$-квантиль $\chi^2_{s-1}$. Если $\widehat X^2_{s-1} >
g_{1-\alpha}(s-1)$, то $H_0$ отклоняется, иначе принимается.
Асимптотическая ошибка первого рода, как будет следовать из теоремы, которую мы сейчас докажем,
будет равна $\alpha$.

\subsubsection{Теорема Пирсона}

Пусть $\xi_1,\ldots,\xi_n$ взаимно независимы и одинаково распределены: $\xi_i = (\xi_{i_1},\ldots,\xi_{i_s})^T$~--- случайные векторы, причем
$
\Pf(\xi_{i_1} = a_1, \ldots, \xi_{i_s} = a_s) = p_1^{a_1} \cdot \ldots \cdot p_s^{a_s}$ для всех $i$,
$p_i > 0$, $\sum_i p_i = 1$; $a_i \in \{ 0,1 \}$, $\sum_i a_i = 1$ ($\xi_i$~--- вектор из 0 и 1 с ровно одной единицей). Очевидно, что $\Mf \xi_{i_k} = p_k$, $\Df \xi_{i_k} = p_k(1-p_k)$,
$\Mf \xi_i = (p_1,\ldots,p_s)^T = \Vec{p}$. Элементы ковариационной матрицы
$\Sigma = \Sigma_{\xi} =  \Mf(\xi - \Mf\xi)(\xi - \Mf\xi)^T = ( \sigma_{jl} )$ имеют вид
$\sigma_{jl} = \Mf (\xi_{i_j} - p_j)(\xi_{i_l} - p_l) = - p_j p_l$ при $j \ne l$ (в общем
случае $\sigma_{jl} = p_j \delta_{jl} - p_j p_l$).
%$$\Sigma_xi = \Mf (\xi - \Mf \xi)(\xi - \Mf \xi)^t$$
Рассмотрим случайный вектор
$S_n = \xi_1 + \ldots + \xi_n = (\mu_1,\ldots,\mu_s)^T$, где $\mu_k = \sum_{i-1}^n \xi_{i_k}$~-- сумма независимых случайных величин. $\Mf \mu_k = np_k$, $\Df \mu_k = np_k(1-p_k)$,
$\Mf S_n = (np_1,\ldots,np_s)^t = n\Vec{p}$; $\Sigma_{S_n} = n \Sigma_\xi$ (докажите это!).

Распределение $S_n$ называется \emph{полиномиальным} распределением. Найдём распределение статистики

$$\widehat X^2_{s-1} = \sum_{k=1}^s \frac{(\mu_k - np_k)^2}{np_k},\quad \Mf \widehat X^2_{s-1} = s-1%, \quad \Df \widehat X^2_{s-1} = ...$$
$$

\begin{theorem}[Пирсон]
$\widehat X^2_{s-1} \rad \chi^2_{s-1}$
\end{theorem}

\begin{proof}
Перейдём к $S_n^* = \frac{S_n - n \Vec{p}}{\sqrt n}, \Mf S_n^* = 0, \Sigma_{S_n^*} = \Sigma_\xi = \Sigma$.
Доказательство будет основано на одном из вариантов многомерной ЦПТ, а именно
\fbox{ \strut $S_n^* \rad Z \sim \Norm(0,\Sigma)$ }
[Севастьянов, гл.~11, \S~46, теорема 7]

% FIXME: доказать сходимость через характеристические функции

%\begin{proof}
Положим $\Delta_k = \dfrac{\mu_k - np_k}{\sqrt{np_k}}$, $k = 1,
\ldots, s$; $\Delta = (\Delta_1,\ldots,\Delta_s)^T$.
$\sum\limits_{k=1}^s \sqrt{p_k} \Delta_k = \dfrac{(n - n)}
{\sqrt{n}} = 0$, поэтому $s$-мерное распределение $\Delta$~---
вырожденное.

$S_n^*$ = $B\Delta$, $B = \diag (\sqrt{p_1},\ldots,\sqrt{p_s})$.
$\Delta = B^{-1} S_n^*$, поэтому $\Mf \Delta = 0$. Ковариационная матрица
$\Sigma_{\Delta} = \Mf (\Delta\Delta^T) = B^{-1} \Mf(S_n^* S_n^{*T}) B^{-1} = B^{-1} \Sigma B^{-1}$.
$(\Sigma_{\Delta})_{ij} = \delta_i^j - \sqrt{p_i p_j}$ (на главной диагонали $1 - p_1, \dots,
1 - p_k$, вне неё: $- \sqrt{p_k p_j}$).

Положим $\eta = C \Delta$, где $C$~--- ортогональная матрица с заданной первой строкой: $c_{1k} = \sqrt{p_k}$.
$\eta = (\eta_1,\ldots,\eta_s)^T$, $\Mf \eta = 0$; $\eta_1 = \sum_{k=1}^s c_{1k} \Delta_k = 0$.
Докажем, что $(\Sigma_\eta)_{ij} = \delta_i^j$, если $i,j \geqslant 2$, и $0$ иначе. При $j, l \geqslant 2$
имеем:

\begin{multline*}
\Mf (\eta_j \eta_l) =
\Mf \left(\sum_k c_{jk} \Delta_k \cdot \sum_k c_{lk} \Delta_k \right) =
\Mf \left(\sum_{s,t} c_{js} c_{lt} \Delta_s \Delta_t \right) =
\sum_{s = 1}^m c_{js} c_{ls} \Mf \Delta_s^2 + \sum_{s \neq t} c_{js} c_{lt}
\Mf (\Delta_s \Delta_t) =\\=
 \sum_{s=1}^m c_{js} c_{ls} (1 - p_s) -
 \sum_{s \neq t} c_{js} c_{lt} \sqrt{p_s p_t}  = \sum_s c_{js} c_{ls}
 - \left( \sum_s c_{js} \sqrt{p_s} \right) \left( \sum_t c_{lt} \sqrt{p_t} \right) =
  \sum_s c_{js} c_{ls} = \delta_{jl}
\end{multline*}

Последнее равенство верно в силу ортогональности строк матрицы $C$.
$$\widehat X^2_{s-1} = \sum_{k=1}^s \Delta_k^2 = \sum_{k=1}^s \eta_k^2 = \eta_2^2 + \ldots + \eta_s^2\quad(\eta_1 = 0)$$
Мы доказали, что при $i > 2$ $\eta_i$ не коррелируют. Если докажем, что $\eta_i \rad \Norm(0,1)$ (и к
тому же они независимы, а не просто не коррелируют --- тут-то нам и потребуется
многомерная ЦПТ!), то получим $\chi^2_{s-1}$. При $j \geqslant 2$
$$\eta_j = \sum_{k=1}^s c_{jk} \Delta_k = \sum_{i=1}^n \sum_{k=1}^s \frac{c_{jk}}{\sqrt{np_k}}(\xi_{i_k} - p_k) = \sum_{i=1}^n \eta_{ij}.$$
При фиксированном $j$ и разных $i$ $\eta_{ij}$ независимы и одинаково распределены, $\Mf \eta_{ij} = 0,\quad \Df \eta_j = 1$.
$$\eta_j = \sum_i \eta_{ij}, \quad \Mf \eta_{ij} = 0, \quad \Df \eta_{ij} = \frac1n\quad \text{докажите это!}$$
По одномерной ЦПТ  $\eta_j \rad \Norm(0,1)$ для всех $j \geqslant 2$.

Памятуя, что $\eta = (CB^{-1})S_n^*$, а $S_n^*$ сходится по распределению к $\Norm(0, \Sigma)$ (по формуле
в рамочке), получаем, что $\eta \rad \Norm(0, E')$, где $E'$ --- единичная матрица без верхней
левой единицы ($\eta_1 = 0$), причём компоненты его некоррелированы, а потому независимы (это
свойство многомерного нормального распределения). Поэтому
${\widehat X}^2_{s-1} = \eta_2^2 + \ldots + \eta_s^2 \rad \chi^2_{s-1}$, что и требовалось.
\end{proof}

\subsubsection{Критерий \texorpdfstring{$\chi^2$}{хи-квадрат}} 
\label{task44}
Пусть $x_1,\ldots,x_n$~--- повторная выборка, $\xi_i$ одинаково распределены с ф.р. $\mathcal{F}(x)$.\\
$H_0: \mathcal{F}(x) = F_0(x), \quad H_1 = \lnot H_0.$

Разобьем $\mathbb{R}$ на $s$ интервалов $\Delta_k = (t_{k-1},t_k]$ произвольным образом. Тогда наша задача сведется к проверке гипотезы о данном
распределении вероятностей успеха в полиномиальной модели, где $m_k$~--- число $x_i$, попавших в $\Delta_k$, $p_k^0$~--- вероятностная мера $\Delta_k$ (при условии $H_0$), и $\widehat X^2_{s-1}$, определенное ранее, будет иметь асимптотическое распределение $\chi^2_{s-1}$

{\bf Замечания}. Задачи, решаемые с помощью $\chi^2$:
\begin{itemize}
\item $F_0(x)$ может быть задана с точностью до $r$ параметров; тогда статистика Пирсона, в которой $p_k^0$ вычислены с подстановкой вместо неизвестных параметров статистик для них, будет иметь асимптотическое распределение $\chi^2_{s-1-r}$.
\item Критерий $\chi^2$ можно использовать для проверки гипотезы однородности. Пусть есть две полиномиальные схемы с $n$ и $l$ испытаниями, с результатами испытаний
$(m_1,\ldots,m_s)$, вероятностями $(p_1,\ldots,p_s)$ и $(m'_1,\ldots,m'_s)$ и $(p'_1,\ldots,p'_s)$ соответственно.

$H_0: p'_i = p_i, \quad H_1 = \lnot H_0$.
При условии $H_0$ выборки объединяются и оцениваются общие значения $p_i = p'_i$: $\hat p_i = \frac{m_i + m'_i}{n+l}$ (по методу МП). Тогда
$$
\widehat X^2_{s-1} = \sum_{k=1}^s \frac{(m_k - n \hat p_k)^2}{n \hat p_k} + \sum_{k=1}^s \frac{(m'_k - l \hat p_k)^2}{l \hat p_k} \Rightarrow \chi^2_{s-1}
$$
(в общем случае, имея r полиномиальных схем, получим $\chi^2_{(s-1)(r-1)}$).

\item Пусть $\mu$ -- медиана распределения $\xi_i$. 
$H_0: \mu = 0$, $H_1: \mu \ne 0$.
При гипотезе $\mu_n^{+}$ из критерия знаков имеет биномиальное 
распределение с $p = 1/2$.
\end{itemize}

\subsection{Критерий Колмогорова}

Рассмотрим две гипотезы о функции распределения $F(x)$: $H_0\colon
F(x)=F_0(x)$ (нулевая гипотеза), где $F_0(x)$~--- заданная
непрерывная функция распределения; $H_1\colon F(x)\neq F_0(x)$
(альтернативная гипотеза).

Статистика Колмогорова позволяет сформулировать критерий, согласно
которому выбирается одна из этих двух гипотез. А именно:

{\bf Критерий Колмогорова.} {\it Если $\sqrt{n}D_n>y_*$, то $H_0$
отклоняем ($H_1$ принимаем), если же $\sqrt{n}D_n\le y_*$, то
$H_0$ принимаем ($H_1$ отклоняем). Здесь число $y_*$ называется
критическим значением и равно $y_*=y_{1-\al}$~--- квантиль уровня
$(1-\al)$ функции Колмогорова $K(y)$ (\те решение уравнения
$K(y)=1-\al$).}

\smallskip

На практике для заданного $\al$ квантиль $y_{1-\al}$ находится по
таблице квантилей функции Колмогорова.

Действительно, по теореме Колмогорова
$\Pf_{H_0}(\sqrt{n}D_n>y_*)=1-\Pf_{H_0}(\sqrt{n}D_n\le
y_{1-\al})\xrightarrow[n\to\infty]{}
1-K(y_{1-\al})=1-(1-\al)=\al,$ \те вероятность ошибки I рода
приближенно равна $\al$ (если $n$ достаточно велико).

\section{Регрессия} \label{task45}
Рассмотрим пару случайных величин $(\xi_1, \xi_2)$.
Задача \emph{регрессии} состоит в том, чтобы найти такую 
измеримую функцию $f(\xi_1)$, что
$\Mf (\xi_2 - f(\xi_1))^2$ минимально ($\Mf \xi_2^2 < \infty$).
Как мы знаем, что такой минимум достигается в 
$m(\xi_1) := \Mf(\xi_2 \mid \xi_1)$.
Функция $m(x) = \Mf(\xi_2 \mid \xi_1 = x)$ называется 
\emph{функцией регрессии} $\xi_2$ по $\xi_1$. 
График $y = m(x)$ называется \emph{линией регрессии}.

Регрессия является оценкой $\widehat{\xi_2}$ случайной величины $\xi_2$. 
Эта оценка обладает следующими свойствами 
(по \hyperref[properties_CondE]{свойствам УМО}):
\begin{nums}{-1}
\item $\Mf \widehat{\xi_2} = \Mf \xi_2$;
\item $\Mf (\xi_2 - \widehat{\xi_2})^2$ -- минимально.
\end{nums}
То есть регрессия предсказывает $\xi_2$ по $\xi_1$ с {\it наименьшей среднеквадратичной ошибкой}.
\subsection{Разложение дисперсии случайной величины по отношению к регрессии.}
Рассмотрим условную дисперсию, определенную как
$$
\Df(\xi_2 \mid \xi_1) = \Mf((\xi_2 - \Mf(\xi_2 \mid \xi_1))^2 \mid \xi_1)
$$
Тогда по свойствам УМО
$$
\Mf \Df(\xi_2 \mid \xi_1) = 
\Mf\{\Mf((\xi_2 - \Mf(\xi_2 \mid \xi_1))^2 \mid \xi_1)\} =
\Mf (\xi_2 - \Mf(\xi_2 \mid \xi_1))^2;
$$
\begin{multline*}
\Mf \Df (\xi_2 \mid \xi_1) + \Df \Mf (\xi_2 \mid \xi_1) =
\Ef\{\xi_2^2 - 2 \xi_2 \Ef(\xi_2 \mid \xi_1) + \Ef^2(\xi_2 \mid \xi_1) +
\Ef^2(\xi_2 \mid \xi_1) - 2 \Ef \xi_2 \Ef(\xi_2 \mid \xi_1) + \Ef^2 \xi_2\}
= \\ \Ef \xi_2^2 + \Ef^2 \xi_2 - 2
\underbrace{\Ef\{\Ef \xi_2 \Ef(\xi_2 \mid \xi_1)\}}_{
= \Ef \xi_2 \Ef\{\Ef(\xi_2 \mid \xi_1)\} = \Ef^2 \xi_2} +
2 \Ef\{\Ef^2(\xi_2 \mid \xi_1) - \xi_2 \Ef(\xi_2 \mid \xi_1)\} = \\
\Ef \xi_2^2 - \Ef^2 \xi_2 +
2 \Ef\{\Ef(\Ef(\xi_2 \mid \xi_1)(\Ef(\xi_2 \mid \xi_1) - \xi_2)) \mid \xi_1\}
= \Df \xi_2 +  2 \Ef\{\Ef(\xi_2 \mid \xi_1) (\Ef(\xi_2 \mid \xi_1) - 
\Ef(\xi_2 \mid \xi_1))\} = \Df \xi_2.
\end{multline*}

\begin{df}
$\Mf \Df (\xi_2 \mid \xi_1)$ -- \emph{остаточная регрессия}.
\end{df}

Разберём крайние случаи.
\begin{nums}{-2}
\item Если $\xi_1$ и $\xi_2$ -- независимы, то 
$\Mf(\xi_2 \mid \xi_1) = \Mf \xi_2$ п.н., поэтому 
$\Df \Mf (\xi_2 \mid \xi_1) = 0$, 
$\Mf \Df (\xi_2 \mid \xi_1) = \Df \xi_2$.
\item Если $\xi_1 = \xi_2$ п.н., то
$\Mf(\xi_2 \mid \xi_1) = \xi_2$ п.н., поэтому
$\Mf \Df (\xi_2 \mid \xi_1) = 0$,
$\Df \Mf (\xi_2 \mid \xi_1) = \Df \xi_2$.
\end{nums}

\subsection{Связь регрессии и корреляции} \label{task47}
Введём понятие \emph{корреляционного отношения}
$$
\eta^2 = \eta^2(\xi_2 \mid \xi_1) :=
1 - \frac{\Mf \Df(\xi_2 \mid \xi_1)}{\Df \xi_2}
$$

Свойства корреляционного отношения
\begin{nums}{-1}
\item $0 \leq \eta^2 \leq 1$;
\item $\eta^2 = 0$ тогда и только тогда, когда 
$\Mf \Df (\xi_2 \mid \xi_1) = \Df \xi_2$
($\xi_2$ и $\xi_1$ не коррелированы);
\item $\eta^2 = 1$ тогда и только тогда, когда 
$\Mf \Df (\xi_2 \mid \xi_1) = 0$;
\item Если зависимость -- линейная, то $\eta^2 = r^2$,
где $r$ -- \href{http://www.machinelearning.ru/wiki/index.php?title=%D0%9A%D0%BE%D1%8D%D1%84%D1%84%D0%B8%D1%86%D0%B8%D0%B5%D0%BD%D1%82_%D0%BA%D0%BE%D1%80%D1%80%D0%B5%D0%BB%D1%8F%D1%86%D0%B8%D0%B8_%D0%9F%D0%B8%D1%80%D1%81%D0%BE%D0%BD%D0%B0}{коэффициент корреляции Пирсона};
\item $\eta^2 \geq r^2$.
\end{nums}

\subsection{Регрессия для двумерной нормальной выборки}

Рассмотрим частный случай. Пусть существует 
совместная плотность $p(x, y)$ случайных величин $(\xi, \eta)$.
Тогда $p(x) = \int_{X} p(x, y) dy$.
Рассмотрим
$$
q(y \mid x) = \frac{p(x, y)}{p(x)}
$$
плотность условного распределения $\xi_2$ при условии $\xi_1 = x$.
$$
\int_{Y} y q(y \mid x) dy = \frac{\int_{Y} y p(x, y) dy}{
\int_{X} p(x, y) dy} = \Mf (\xi_2 \mid \xi_1=x).
$$
$m(\xi_1) = \Mf(\xi_2 \mid \xi_1)$ -- регрессия $\xi_2$ по $\xi_1$.
Линейная регрессия $m(x) = k x + b$.

\begin{ex}
$$
p(x, y) = \frac{1}{2 \pi \sqrt{(1 - r^2) \sigma_1 \sigma_2}}
\exp \left\{-\frac{1}{2(1-r^2)} \left(
\frac{(x-a_1)^2}{\sigma_1^2} - 2r \frac{(x-a_1)(y-a_2)}{
\sigma_1 \sigma_2} + \frac{(y-a_2)^2}{\sigma_2^2}\right)\right\}
$$

Если $r = 0$,то получаем
$$
p(x, y) = \frac{1}{2 \pi \sigma_1 \sigma_2}
\exp \left\{-\frac{1}{2} \left(
\frac{(x-a_1)^2}{\sigma_1^2} +
\frac{(y-a_2)^2}{\sigma_2^2}\right)\right\}
$$
Для $\xi_1$ 
$$
p(x) = \frac{1}{\sqrt{2 \pi \sigma_1}} e^{-(x-a_1)^2/2}
$$
Тогда для $a_1 = a_2 = 0$
$$
q(y \mid x) = \frac{p(x, y)}{p(x)} =
\frac{1}{\sqrt{2 \pi (1-r^2) \sigma_2}}
\exp \left\{-\frac{(y - r \dfrac{\sigma_2}{\sigma_1} x)^2}{
2 (1-r^2) \sigma_2^2}\right\}
$$

Можно заметить, что
$q(y \mid x)$ -- плотность $\Norm (r \sigma_2 x / \sigma_1,
(1-r^2) \sigma_2^2)$.
Тогда 
$$
\Mf(\xi_2 \mid \xi_1) = r \frac{\sigma_2}{\sigma_1} \xi_1, \,
\Mf \Df (\xi_2 \mid \xi_1) = (1-r^2) \sigma_2^2
$$
То есть для нормального двумерного распределения $(\xi_1, \xi_2)$
регрессия линейная. В общем случае
$$
\widehat{\xi_2} = r \frac{\sigma_2}{\sigma_1} (\xi_1 - a_1) + a_2.
$$
\end{ex}

\subsection{Эмпирическая регрессия}
Пусть даны выборочные значения $(x_1, y_1), \ldots, (x_n, y_n)$.
Это реализация пары случайных величин $(\xi_1, \xi_2)$.
Мы можем нанести эти точки на график
Этот график называется \emph{диаграммой рассеяния}.
Предположим, что $y = m(x) = k x + b$.
Будем искать $k$ и $b$ такие, чтобы
$$
\sum_{i=1}^n (y_i - (k x_i + b))^2
$$
было бы минимально.
Такой метод называется \emph{методом наименьших квадратов (МНК)}.
Дифференцируя и приравнивая производные нулю получим, что оптимальные
параметры
$$
k = \widehat{r} \frac{S_2}{S_1}, \, b = \bar{y} - k \bar{x},
$$
где
$$
\widehat{r} = \frac{\sum_{i=1}^n 
(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2
\sum_{i=1}^n (y_i - \bar{y})^2}.
$$
Но что, если линейного приближения недостаточно?
Можно тогда использовать выборочный коэффициент корреляции.
Тогда нам нужно проверить гипотезу $\nu^2 = r^2$.

В нормальном случае $\widehat{r}$ имеем асимптотическое распределение
$\Norm(r, (1-r^2)/n)$. При гипотезе
$$
\widehat{t}_{n-2} = \frac{\widehat{r} \sqrt{n-2}}{
\sqrt{1-\widehat{r}r^2}}.
$$
Тогда для заданного критического уровня $\alpha$ составляем
неравенство и проверяем его.

\pagebreak
\section{Вопросы к экзамену} \label{tasks}
\begin{enumerate}
 \item \hyperref[task1]{Эмпирическая функция распределения: её свойства как
 функции распределения и как оценки теоретической функции распределения.}
 \item \hyperref[task2]{Теорема Гливенко-Кантелли о сходимости эмпирической
 функции распределения.}
 \item \hyperref[task3]{Асимптотические свойства эмпирических моментов
 и функций от них.}
 \item \hyperref[task4]{Теорема Колмогорова с доказательством независимости
 распределения статистики Колмогорова от вида непрерывной
 функции распределения.}
 \item \hyperref[char]{Вариационный ряд выборки и порядковые статистики.
 Распределение порядковых статистик. Оценка выборочных квантилей.}
 \item \hyperref[task6]{Информация Фишера и её свойства.}
 \item \hyperref[task7]{Условные математические ожидания и 
 условные распределения относительно сигма-алгебр и случайных величин.
 Условная плотность распределения одной случайной величины относительно другой.}
 \item \hyperref[properties_CondE]{Свойства условных математических ожиданий.}
 \item \hyperref[task9]{Достаточные статистики. 
 Теорема Неймана-Фишера (критерий достаточности).}
 \item \hyperref[task10]{Сравнение точечных статистических оценок по их свойствам.
 Асимптотические свойства оценок. Примеры состоятельных и 
 асимптотически нормальных оценок.}
 \item \hyperref[task11]{Эффективные оценки в регулярном случае.
 Неравенство Крамера-Рао.}
 \item \hyperref[task12]{Многомерное обобщение неравенства Крамера-Рао.
 Информация Фишера в двумерном случае, пример двумерной нормальной выборки.}
 \item \hyperref[task13]{Улучшение оценок с помощью достаточных статистик.
 Теорема Колмогорова-Блекуэла-Рао.}
 \item \hyperref[task14]{Полные достаточные статистики и теорема о несмещённых
 оценках с минимальной дисперсией.}
 \item \hyperref[task15]{Свойства частоты как оценки вероятности <<удачи>>
 в схеме Бернулли. Сравнение с другими оценками.}
 \item \hyperref[task16]{Метод моментов оценивания параметров.
 Теорема о состоятельности оценок метода моментов.}
 \item \hyperref[task17]{Метод максимального правдоподобия.
 Свойства оценок максимального правдоподобия.
 Примеры оценок максимального правдоподобия.}
 \item \hyperref[task18]{Метод максимального правдоподобия.
 Теорема об асимптотической нормальности оценок максимального правдоподобия.}
 \item \hyperref[tasks19]{Байесовский метод.
 Теорема о байесовской оценке при квадратичной функции риска.
 Априорное и апостериорное распределение.
 Априорный и апостериорный риск.}
 \item \hyperref[task20]{Байесовские оценки параметров биномиального
 и нормального распределений.}
 \item \hyperref[task21]{Свойства байесовских оценок: байесовские оценки
 и достаточные статистики; минимаксные оценки как байесовские
 с постоянным риском.}
 \item \hyperref[task22]{Многомерное нормальное распределение:
 эквивалентные определения и основные характеристики.
 Свойства многомерного нормального распределения.}
 \item \hyperref[task23]{Лемма Фишера о независимости среднего
 арифметического и среднего квадратического для независимых одинаково
 нормально распределённых случайных величин.}
 \item \hyperref[task24]{Распределение хи-квадрат, Стьюдента и
 Фишера-Снедекора. Вывод формулы плотности распределения.}
 \item \hyperref[task25]{Следствие из леммы Фишера о распределениях
 хи-квадрат и Стьюдента как распределениях статистик для нормальных выборок.}
 \item \hyperref[task26]{Интервальные оценки и их характеристики.
 Два метода построения точных доверительных интервалов.}
 \item \hyperref[task27]{Построение точного доверительного интервала
 для параметра биномиального распределения.}
 \item \hyperref[task28]{Доверительные интервалы для параметров нормального
 распределения (для среднего и дисперсии).}
 \item \hyperref[task29]{Асимптотические доверительные интервалы.
 Построение асимптотического доверительного интервала на основе 
 асимптотической нормальности подходящей статистики.}
 \item \hyperref[task30]{Асимптотические доверительные интервалы
 для параметров биномиального распределения.}
 \item \hyperref[task31]{Теорема Неймана-Пирсона. Критерий отношения
 правдоподобий для проверки двух простых гипотез, как наиболее
 мощный и несмещённый критерий.}
 \item \hyperref[task32]{Равномерно наиболее мощный критерий.
 Теорема о существовании РНМ-критерия при условии свойства монотонности 
 отношения правдоподобий.}
 \item \hyperref[task33]{Критерий отношения правдоподобий для проверки 
 двух гипотез о среднем значении нормального распределения.}
 \item \hyperref[task34]{Проверка гипотез о параметрах нормального 
 распределения (о среднем и дисперсии).}
 \item \hyperref[task35]{Проверка гипотез о параметрах нормального
 распределения с помощью доверительных интервалов.}
 \item \hyperref[equality_expectancy]{Критерий Стьюдента равенства средних
 значений двух независимых нормальных выборок.}
 \item \hyperref[task37]{Критерий Фишера равенства дисперсий
 двух независимых нормальных выборок.}
 \item \hyperref[task38]{Оценка наименьшего размера выборки,
 необходимого для проверки двух гипотез о параметре биномиального
 распределения с заданными вероятностями ошибок первого и второго рода.}
 \item \hyperref[task39]{Дисперсионный анализ однофакторной модели для
 нормальных выборок.}
 \item \hyperref[task40]{Множественное сравнение параметров однофакторной
 модели с помощью доверительных интервалов.}
 \item \hyperref[task41]{Критерий проверки гипотез о значении 
 параметра биномиального распределения. Критерий знаков.}
 \item \hyperref[task42]{Полиномиальное распределение, как многомерное
 распределение. Статистика Пирсона для проверки гипотезы о значениях
 параметров полиномиального распределения. Теорема об асимптотическом
 хи-квадрат распределении статистики Пирсона.}
 \item \hyperref[task43]{Критерий хи-квадрат для проверки гипотезы
 о данном полиномиальном распределении.}
 \item \hyperref[task44]{Критерий Колмогорова для проверки гипотезы
 о данном непрерывном распределении.}
 \item \hyperref[task45]{Решение задачи регрессии. Регрессия одной
 случайной величины по другой случайной величине. Разложение дисперсии
 случайной величины по отношению к регрессии. Остаточная дисперсия.}
 \item \hyperref[task46]{Сравнение байесовских оценок и оценок максимального
 правдоподобия для параметра биномиального распределения.\footnote{
 Не знаю что лектор рассказывал по теме данного вопроса, 
 но приведённый пример показывает разницу между ОМП и байесовскими, 
 минимаксными оценками~--- \emph{примеч. В.\,Х.}}}
 \item \hyperref[task47]{Линейная регрессия. Роль коэффициента 
 корреляции при оценивании одной случайной величины по другой. 
 Регрессия в случае двумерного нормального распределения. Статистическое 
 оценивание коэффициентов регрессии для нормальных выборок методом
 наименьших квадратов.}
\end{enumerate}

\end{document}

